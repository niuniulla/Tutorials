{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CNzvT2MRtqxp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from matplotlib import animation, rc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-JwqLzQtqx2"
      },
      "source": [
        "Q function represents the discounted future reward from a starting state.\n",
        "By adopting an optimal action policy, actions can be taken at each state to achieve the highest Q value at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_sPVNKutqx3"
      },
      "source": [
        "# Deterministic MDPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Intuitively, for RL, for a given state, we would like to choose a action to benefit all future states to eventually reach the winning point. Such benefit can be formulated as a cumulative decremental reward for a starting state at the instant $t$:\n",
        "\n",
        "$$ V^{\\pi}(s,t) =  \\sum_{i=0}^N \\gamma^i r_{t+i}$$\n",
        "\n",
        "The goal is to find an optimal policy $\\pi^*$ that maximizes the cumulative reward:\n",
        "\n",
        "$$ \\pi^*(s) = argmax_a V^{\\pi}(s)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKkyyH1stqx4"
      },
      "source": [
        "However, in reality, it is hard to have all reward at each state in advance to compute the value $V$.\n",
        "So we can consider the optimal policy will be the one that maximize the sum of the immadiate reward resulting an action and the value of all subsequent states after the action, which results in a iterative form of Q learning:\n",
        "\n",
        "$$Q(s,a) = r + \\gamma Max(Q(s',a'))$$\n",
        "\n",
        "where\n",
        " * $Q(s,a)$ is the Q value of the current state\n",
        " * $r$ is the reward by taking action $a$ from the current state\n",
        " * $Q(s',a')$ is the Q value of the next state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aE445Vstqx4"
      },
      "source": [
        "### Pseudo-code\n",
        "\n",
        "```\n",
        "initialize Q[numstates,numactions] arbitrarily\n",
        "observe initial state s\n",
        "repeat\n",
        "    select and carry out an action a\n",
        "    observe reward r and new state s'\n",
        "    Update Q table by\n",
        "        Q[s,a] = r + Î³max Q[s',a']\n",
        "    s = s'\n",
        "until terminated\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "wqZzmcaJtqx5",
        "outputId": "e8ad15cb-78d4-4229-e0f8-66a3b47de72b"
      },
      "outputs": [],
      "source": [
        "# S F F F       (S: starting point, safe)\n",
        "# F H F H       (F: frozen surface, safe)\n",
        "# F F F H       (H: hole, stuck forever)\n",
        "# H F F G       (G: goal, safe)\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, {'prob': 1})\n",
            "(1, 0.0, False, False, {'prob': 1.0})\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        }
      ],
      "source": [
        "out = env.reset()\n",
        "print(out)\n",
        "out= env.step(2)\n",
        "print(out)\n",
        "print(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success rate = 1.0%\n"
          ]
        }
      ],
      "source": [
        "def test(env, num, Q=None):\n",
        "    episodes = 100\n",
        "    nb_success = 0\n",
        "\n",
        "    # Evaluation\n",
        "    for _ in range(num):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        # Until the agent gets stuck or reaches the goal, keep training it\n",
        "        while not done:\n",
        "            # Choose the action with the highest value in the current state\n",
        "            if Q is not None:\n",
        "                action = np.argmax(Q[state])\n",
        "            else:\n",
        "                action = env.action_space.sample()\n",
        "\n",
        "            # Implement this action and move the agent in the desired direction\n",
        "            new_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            # Update our current state\n",
        "            state = new_state\n",
        "\n",
        "            # When we get a reward, it means we solved the game\n",
        "            nb_success += reward\n",
        "\n",
        "    # Let's check our success rate!\n",
        "    print (f\"Success rate = {nb_success/num*100}%\")\n",
        "\n",
        "test(env, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "FeBnkpe1tqx8"
      },
      "outputs": [],
      "source": [
        "#Initialize Q table of dimension (16 states X 4 actions)\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "# Set learning parameters\n",
        "gamma = 0.95\n",
        "alpha = 0.5 \n",
        "num_episodes = 1000\n",
        "done = False\n",
        "#create lists to contain total rewards and steps per episode\n",
        "#jList = []\n",
        "rList = []\n",
        "for i in range(num_episodes):\n",
        "    #Reset environment and get first new observation\n",
        "    s, _ = env.reset()\n",
        "    rAll = 0\n",
        "    done = False # fall in H\n",
        "    o = False # invalid action\n",
        "\n",
        "    #The Q-Table learning algorithm\n",
        "    while not done:\n",
        "        if np.max(Q[s]) > 0:\n",
        "            #Choose an action by greedily (with noise) picking from Q table\n",
        "            a = np.argmax(Q[s])\n",
        "        else:\n",
        "            # else select ramdomly\n",
        "            a = env.action_space.sample()\n",
        "\n",
        "        # apply action and get new state and reward from environment\n",
        "        s1, r, done ,_, _ = env.step(a)\n",
        "\n",
        "        #Update Q-Table with new knowledge\n",
        "        Q[s,a] = (r + gamma*np.max(Q[s1,:]))\n",
        "\n",
        "        rAll += r\n",
        "        s = s1\n",
        "\n",
        "    #jList.append(j)\n",
        "    rList.append(rAll)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1ITad4gFtqx9",
        "outputId": "77d66f39-42d4-4bf5-c4b6-e687e6079f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Q-Table Values\n",
            "[[0.         0.77378094 0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.81450625 0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.857375   0.        ]\n",
            " [0.         0.         0.9025     0.        ]\n",
            " [0.         0.95       0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         1.         0.        ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Final Q-Table Values\")\n",
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpBaw5Qptqx-"
      },
      "source": [
        "The Q values change at each run, which due to the randomness of the game (a wind blow may change randomly the state)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success rate = 100.0%\n"
          ]
        }
      ],
      "source": [
        "test(env, 100, Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBS2aMgGtqx-"
      },
      "source": [
        "# Nondeterministic MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HALtcoEtqx_"
      },
      "source": [
        "This modified equation is adapted to the randomness of the problem with which the Q value is approximated by the Q value of previous iteration.\n",
        "\n",
        "$$Q(s,a) = E[r] + \\gamma \\sum p(s'/ s,a ) Max(Q(s',a'))$$\n",
        "$$Q_n(s,a) = (1-\\alpha_n)Q_{n-1}(s,a) + \\alpha_n [r + \\gamma Max(Q_{n-1}(s',a'))]$$\n",
        "\n",
        "with $$\\alpha_n = \\frac{1}{1+Visits_n(s,a)}$$\n",
        "\n",
        "where\n",
        " * $Q(s,a)$ is the Q value of the current state\n",
        " * $r$ is the reward by taking action $a$ from the current state\n",
        " * $Q(s',a')$ is the Q value of the next state\n",
        " * $\\alpha$ is the learning rate\n",
        " * $Visit$ is the iteration number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOyV30gFtqx_"
      },
      "source": [
        "### Pseudo-code\n",
        "```\n",
        "initialize Q[numstates,numactions] arbitrarily\n",
        "observe initial state s\n",
        "repeat\n",
        "    select and carry out an action a\n",
        "    observe reward r and new state s'\n",
        "    Update Q table by\n",
        "        Q[s,a] = Q[s,a] + Î±(r + Î³maxa' Q[s',a'] - Q[s,a])\n",
        "    s = s'\n",
        "until terminated\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": true,
        "id": "s7goL7_LtqyA"
      },
      "outputs": [],
      "source": [
        "# Even this example doesn't make sense here, it is served only as a benchmark to test our method.\n",
        "\n",
        "#Initialize table with all zeros\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "visits = np.ones([env.observation_space.n,env.action_space.n])\n",
        "# Set learning parameters\n",
        "gamma = 0.95\n",
        "num_episodes = 1000\n",
        "#create lists to contain total rewards and steps per episode\n",
        "#jList = []\n",
        "rList = []\n",
        "for i in range(num_episodes):\n",
        "    #Reset environment and get first new observation\n",
        "    s, _ = env.reset()\n",
        "    rAll = 0\n",
        "    done = False # fall in H\n",
        "    #The Q-Table learning algorithm\n",
        "    while not done:\n",
        "        if np.max(Q[s]) > 0:\n",
        "            #Choose an action by greedily (with noise) picking from Q table\n",
        "            a = np.argmax(Q[s])\n",
        "        else:\n",
        "            # else select ramdomly\n",
        "            a = env.action_space.sample()\n",
        "\n",
        "        #Get new state and reward from environment\n",
        "        s1, r, done, _, _ = env.step(a)\n",
        "\n",
        "        #Update Q-Table with new knowledge\n",
        "        alpha = 1.0 / visits[s, a]\n",
        "        Q[s,a] = (1-alpha)*Q[s,a] + alpha*(r+gamma*np.max(Q[s1,:]))\n",
        "\n",
        "        rAll += r\n",
        "        s = s1\n",
        "        visits[s, a] += 1\n",
        "\n",
        "    #jList.append(j)\n",
        "    rList.append(rAll)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "9ggIDsyUtqyA",
        "outputId": "215fbe9a-1551-4a33-ceeb-1e6561e90a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Q-Table Values\n",
            "[[0.08163602 0.2304565  0.13905771 0.08301543]\n",
            " [0.14684164 0.         0.09963049 0.06880982]\n",
            " [0.08723636 0.2974211  0.0874676  0.13386757]\n",
            " [0.28255005 0.         0.06508916 0.13674933]\n",
            " [0.09162589 0.24258579 0.         0.15509241]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.31307485 0.         0.15611931]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.38566212 0.         0.81983764 0.2292585 ]\n",
            " [0.59323588 0.86298699 0.22910044 0.        ]\n",
            " [0.81983764 0.47251511 0.         0.17276561]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.18021446 0.90840735 0.81983764]\n",
            " [0.86298699 0.34540082 0.95913705 0.77884576]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Final Q-Table Values\")\n",
        "print(Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success rate = 100.0%\n"
          ]
        }
      ],
      "source": [
        "test(env, 100, Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Epsilon-greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Initialize table with all zeros\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "visits = np.ones([env.observation_space.n,env.action_space.n])\n",
        "# Set learning parameters\n",
        "gamma = 0.95\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.001\n",
        "num_episodes = 1000\n",
        "#create lists to contain total rewards and steps per episode\n",
        "#jList = []\n",
        "rList = []\n",
        "for i in range(num_episodes):\n",
        "    #Reset environment and get first new observation\n",
        "    s, _ = env.reset()\n",
        "    rAll = 0\n",
        "    done = False # fall in H\n",
        "    #The Q-Table learning algorithm\n",
        "    while not done:\n",
        "        rand = np.random.random()\n",
        "        if rand < epsilon or not np.max(Q[s]) > 0:\n",
        "            # else select ramdomly\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            #Choose an action by greedily (with noise) picking from Q table\n",
        "            a = np.argmax(Q[s])\n",
        "            \n",
        "        #Get new state and reward from environment\n",
        "        s1, r, done, _, _ = env.step(a)\n",
        "\n",
        "        #Update Q-Table with new knowledge\n",
        "        alpha = 1.0 / visits[s, a]\n",
        "        Q[s,a] = (1-alpha)*Q[s,a] + alpha*(r+gamma*np.max(Q[s1,:]))\n",
        "\n",
        "        rAll += r\n",
        "        s = s1\n",
        "        visits[s, a] += 1\n",
        "        epsilon = max(epsilon - epsilon_decay, 0.0)\n",
        "\n",
        "    #jList.append(j)\n",
        "    rList.append(rAll)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Q-Table Values\n",
            "[[0.00000000e+00 0.00000000e+00 5.65623724e-01 6.34954599e-05]\n",
            " [2.16744974e-04 0.00000000e+00 5.95512426e-01 1.58412821e-03]\n",
            " [3.79013274e-03 7.49672585e-01 5.54764911e-04 9.98919211e-03]\n",
            " [2.10226703e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 7.89156560e-01 0.00000000e+00 8.91085995e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 9.02500000e-01 1.62901250e-01 0.00000000e+00]\n",
            " [8.57375000e-01 7.33135213e-01 0.00000000e+00 1.53682958e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 9.50000000e-01 0.00000000e+00]\n",
            " [9.02500000e-01 3.26562500e-01 1.00000000e+00 6.90755631e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Final Q-Table Values\")\n",
        "print(Q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success rate = 100.0%\n"
          ]
        }
      ],
      "source": [
        "test(env, 100, Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# docs:\n",
        " * https://www.cs.swarthmore.edu/~meeden/cs63/f11/ml-ch13.pdf\n",
        " * http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_rl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
