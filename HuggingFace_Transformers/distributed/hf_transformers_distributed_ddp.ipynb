{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Data Parallel (DDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How DDP works\n",
    "\n",
    "For DDP, the steps are:\n",
    "\n",
    "  1. create several pipeline for training with their model and data\n",
    "  2. do forward pass for each pipeline\n",
    "  3. compute loss for each pipeline\n",
    "  4. communicate between each pipeline the gradients\n",
    "  5. update each model\n",
    "\n",
    "\n",
    "### 2. Some Notions\n",
    "\n",
    "There are some notions to understand DDP:\n",
    "\n",
    "  * group: all pieplies in all gpus for a task\n",
    "  * world_size: parallel size, normally, the number of the gpus\n",
    "  * node: a machine / node, can have several gpus\n",
    "  * rank(global rank): training pipeline number\n",
    "  * local_rank: the training advancement within a node\n",
    "\n",
    "eg.\n",
    "\n",
    "    2 machines with 2 gpus each\n",
    "    node = 2\n",
    "    wold_size = 4\n",
    "    each process use 2 gpus\n",
    "\n",
    "                    Node 0                                  Node 1\n",
    "     ++++++++++++++++++++++++++++++++++++++  +++++++++++++++++++++++++++++++++++++\n",
    "     +   Train.py      +   Train.py      +   +   Train.py      +  Train.py       +\n",
    "     + Global Rank 0   + Gloabal Rank 1  +   + Global Rank 2   + Global Rank 3   +\n",
    "     + Local Rank 0    + Local Rank 1    +   + Local Rank 0    + Local Rank 1    +\n",
    "     + ------  ------  + ------- ------- +   + ------- ------- + ------- ------- +\n",
    "     + |     | |     | + |     | |     | +   + |     | |     | + |     | |     | +\n",
    "     + | GPU | | GPU | + | GPU | | GPU | +   + | GPU | | GPU | + | GPU | | GPU | +\n",
    "     + |  0  | |  1  | + |  2  | |  3  | +   + |  0  | |  1  | + |  2  | |  3  | +\n",
    "     + |     | |     | + |     | |     | +   + |     | |     | + |     | |     | +\n",
    "     + ------- ------- + ------- ------- +   + ------- ------- + ------- ------- +\n",
    "     +++++++++++++++++++++++++++++++++++++   +++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "\n",
    "### 3. communication\n",
    "\n",
    "The communication between each node and gpus allows the exchange of information and coordinate the training. There are 2 majors types of communication:\n",
    "\n",
    "  * point to point: from one pipeline to another\n",
    "\n",
    "  * broadcast: see the schema for understanding.\n",
    "\n",
    "    - Scater: from one pipeline to all other pipeline in the group\n",
    "    \n",
    "                                    [t0, t1, t2, t3]                                \n",
    "                                      ++++++++++                                        \n",
    "                                      | Rank 0 |                                    \n",
    "                                      ++++++++++                                    \n",
    "                                          |                                      \n",
    "                   --------------  ------   ------- ----------------                 \n",
    "                  |                |               |                |              \n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++  \n",
    "             | Rank 0 |       | Rank 1 |       | Rank 2 |      | Rank 3 |\n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++\n",
    "              [t0,]             [t1,]             [t2,]           [t3,]                   \n",
    "\n",
    "\n",
    "    - Gather: it is the reverse operation of scatter\n",
    "    \n",
    "              [t0,]             [t1,]             [t2,]           [t3,]   \n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++  \n",
    "             | Rank 0 |       | Rank 1 |       | Rank 2 |      | Rank 3 |\n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++ \n",
    "                  |                |                |                |\n",
    "                  ----------------  ------   ------- -----------------\n",
    "                                           |\n",
    "                                       ++++++++++\n",
    "                                       | Rank 0 |\n",
    "                                       ++++++++++\n",
    "                                     [t0, t1, t2, t3]\n",
    "\n",
    "    - Reduce: different from Gather, he collected data were merged into one data through an operation\n",
    "\n",
    "              [t0,]             [t1,]             [t2,]           [t3,]   \n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++  \n",
    "             | Rank 0 |       | Rank 1 |       | Rank 2 |      | Rank 3 |\n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++ \n",
    "                  |                |                |                |\n",
    "                  ----------------  ------   ------- -----------------\n",
    "                                           |\n",
    "                                       ++++++++++\n",
    "                                       | Rank 0 |\n",
    "                                       ++++++++++\n",
    "                                     [t0 + t1 + t2 + t3]\n",
    "\n",
    "    - All Reduce: the reduced data were sent to all pipelines\n",
    "  \n",
    "                [t0,]             [t1,]             [t2,]           [t3,]   \n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++  \n",
    "             | Rank 0 |       | Rank 1 |       | Rank 2 |      | Rank 3 |\n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++ \n",
    "                  |                |                |                |\n",
    "                  ----------------  ---------------- -----------------\n",
    "                  |                |                |                |\n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++  \n",
    "             | Rank 0 |       | Rank 1 |       | Rank 2 |      | Rank 3 |\n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++ \n",
    "            [t0+t1+t2+t3]    [t0+t1+t2+t3]    [t0+t1+t2+t3]   [t0+t1+t2+t3]\n",
    "\n",
    "    - Broadcast: send data from one pipeline to all other in the group\n",
    "\n",
    "                                        [t0, ]                                \n",
    "                                      ++++++++++                                        \n",
    "                                      | Rank 0 |                                    \n",
    "                                      ++++++++++                                    \n",
    "                                          |                                      \n",
    "                   --------------  ------   ------- ----------------                 \n",
    "                  |                |               |                |              \n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++  \n",
    "             | Rank 0 |       | Rank 1 |       | Rank 2 |      | Rank 3 |\n",
    "             ++++++++++       ++++++++++       ++++++++++      ++++++++++\n",
    "              [t0,]             [t0,]             [t0,]           [t0,]   \n",
    "\n",
    "    - All Gather:\n",
    "\n",
    "               [t0,]             [t1,]            [t2,]           [t3,]   \n",
    "             ++++++++++       ++++++++++       ++++++++++       ++++++++++  \n",
    "             | Rank 0 |       | Rank 1 |       | Rank 2 |       | Rank 3 |\n",
    "             ++++++++++       ++++++++++       ++++++++++       ++++++++++ \n",
    "                  |                |                |                |\n",
    "                  ----------------  ---------------- -----------------\n",
    "                  |                |                |                |\n",
    "             ++++++++++       ++++++++++       ++++++++++       ++++++++++  \n",
    "             | Rank 0 |       | Rank 1 |       | Rank 2 |       | Rank 3 |\n",
    "             ++++++++++       ++++++++++       ++++++++++       ++++++++++ \n",
    "            [t0,t1,t2,t3]    [t0,t1,t2,t3]    [t0,t1,t2,t3]   [t0,t1,t2,t3]\n",
    "\n",
    "\n",
    "### 4. Run\n",
    "\n",
    "We can't run multi-gpu of pytorch in jupyter, so we should use a script and do :\n",
    "```\n",
    "torchrun --multi-gpu=2 ddp_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Using torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the same bert classification model as example. \n",
    "We should do some modifications to use DDP. For this, see the modification section and follow the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 09:51:57.476265: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-17 09:51:57.476319: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-17 09:51:57.479244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-17 09:51:57.493016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 09:51:59.593705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 4084\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = load_dataset(\"davidberg/sentiment-reviews\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "\n",
    "    label2id = {\"positive\": 0, \"negative\": 1}\n",
    "\n",
    "    def __init__(self, _data):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = {\"review\":[], \"division\":[]}\n",
    "        for i in range(len(_data[\"train\"][\"review\"])):\n",
    "            \n",
    "            if _data[\"train\"][i][\"division\"] in dataset.label2id.keys():\n",
    "                self.data[\"review\"].append(_data[\"train\"][i][\"review\"])\n",
    "                self.data[\"division\"].append(_data[\"train\"][i][\"division\"])\n",
    "            # else:\n",
    "            #     print(_data[\"train\"][i][\"review\"], _data[\"train\"][i][\"division\"])\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.data[\"review\"][index], dataset.label2id.get(self.data[\"division\"][index])\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548\n"
     ]
    }
   ],
   "source": [
    "# construct dataset\n",
    "\n",
    "ds = dataset(data)\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "trainset, validset = random_split(ds, lengths=[0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function collate\n",
    "\n",
    "import torch\n",
    "\n",
    "def collate_fct(batch):\n",
    "\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for item in batch:\n",
    "\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "\n",
    "    toks = tokenizer(texts, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    toks[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=2, shuffle=True, collate_fn=collate_fct)\n",
    "validloader = DataLoader(validset, batch_size=32, shuffle=False, collate_fn=collate_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckp)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. define eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    acc_num = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for batch in validloader:\n",
    "\n",
    "            batch = {k:v.cuda() for k, v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "\n",
    "            count += len(batch[\"labels\"])\n",
    "\n",
    "            acc_num += (batch[\"labels\"].int() == pred.int()).sum()\n",
    "\n",
    "    return acc_num /count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "def train(epoch=3, log_step=100):\n",
    "\n",
    "    gStep = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in trainloader:\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k:v.cuda() for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(**batch)\n",
    "\n",
    "            output.loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if gStep % log_step  == 0:\n",
    "\n",
    "                print(f\"epoch {e+1} / {epoch}: global step: {gStep}, loss: {output.loss.mean().item()}\")\n",
    "\n",
    "            gStep += 1\n",
    "\n",
    "        acc = evaluate()\n",
    "\n",
    "        print(f\"epoch: {e} : acc: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step to do distributed training are:\n",
    "\n",
    " step1. export the above code (without distributed training) to a .py file. To do so, use the \"Export\" functionality of VS Code to export a python script format.\n",
    "\n",
    " step2. remove texts and codes not related to the context. Mostly the titles, the dafault prints.\n",
    "\n",
    " step3. following below to modify the code to adapt it to distributed training.\n",
    "\n",
    "    A. before anything, import the distributed module and initialize the backend\n",
    "    B. for the loaders, remove shuffle and add distributed sampler.\n",
    "    C. send model to the main GPU (rank 0). This should be done for all data including model and batch data in the eval function and the training loop.\n",
    "\n",
    "    Until here, main steps using DDP are done and trhe training can be run without errors. However, the reporting is still messing (eg. loss shown several times, accuracy not correct.)\n",
    "    So some extra steps are needed to coordinate the training progress among the gpus.\n",
    "\n",
    "    D. reduce the loss from all GPUs. By doing this, if we run the training, we can see that the loss from all GPUs report the same value.\n",
    "\n",
    "    E. report once for all GPUs. Since all loss are the same now, we need only to report the value once. To do so, we report only for the rank 0.\n",
    "\n",
    "    F. compute final accuracy. Different from loss which can be averaged, accuracy sould be recomputed using the sum of correct predictions over the total number. \n",
    "\n",
    "    G. Fix data leak. So far, when we train, we will find that the accuracy seems to overfit. This is due to the data leak when the dataset was randomly split while training. To fix this, we fix the split using a random seed.\n",
    "\n",
    "    H. Do shuffle. In B when we add the distributed sample, we disabled the shuffle, this can also cause overfit. So, it is better to reactivate the shuffle.\n",
    "\n",
    " Step4. Launch the script using : \n",
    " ```bash\n",
    " $ torchrun --nproc_per_node=2 ddp_train_torch.py\n",
    " ```\n",
    "\n",
    "\n",
    "Notes: There are some formattings are done to render the script more lisible and the default prints in the notebook were removed in the script.\n",
    "\n",
    "Remarks: There still is a problem about the data scatter when using distributed training. This is due to the fact that DDP will automatically pad data to balance the data on each GPU (eg. there are total of 49 data, DDP will pad the data so that each process will have 25 data). This can cause problem when computing accuracy in the eval function. However, this is not tacled here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Using Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to do any modification to the code to use DPP. Follow the steps to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "ckp = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "# load data\n",
    "data = load_dataset(\"davidberg/sentiment-reviews\")\n",
    "\n",
    "split_data = data[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "\n",
    "label2id = {\"positive\": 0, \"negative\": 1}\n",
    "\n",
    "# process data\n",
    "\n",
    "def process(samples):\n",
    "\n",
    "    _data = {\"review\":[], \"division\":[]}\n",
    "    for i in range(len(samples[\"review\"])):\n",
    "        if samples[\"division\"][i] in label2id.keys():\n",
    "            _data[\"review\"].append(samples[\"review\"][i])\n",
    "            _data[\"division\"].append(samples[\"division\"][i])\n",
    "\n",
    "    toks = tokenizer(_data[\"review\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    toks[\"labels\"] = [label2id.get(d) for d in _data[\"division\"]]\n",
    "\n",
    "    return toks\n",
    "\n",
    "tokenized_data = split_data.map(process, batched=True, remove_columns=split_data[\"train\"].column_names)\n",
    "\n",
    "# load model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckp)\n",
    "\n",
    "# metric\n",
    "acc_fct = evaluate.load(\"accuracy\")\n",
    "f1_fct = evaluate.load(\"f1\")\n",
    "\n",
    "def metric(pred):\n",
    "\n",
    "    preds, refs = pred\n",
    "\n",
    "    preds = preds.argmax(axis=-1)\n",
    "\n",
    "    acc = acc_fct(predictions=preds, references=refs)\n",
    "    f1 = f1_fct(predictions=preds, references=refs)\n",
    "    acc.update(f1)\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=32,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=True),\n",
    "    compute_metrics=metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just follow the steps 1&2 for the torch case.\n",
    "\n",
    "For step3, we don't need to modify anything for the training. However, in this case, we may see overfit due to data split which causes data leak.\n",
    "\n",
    "So we can do:\n",
    "\n",
    "    A. fix split. We split data using a random seed.\n",
    "\n",
    "And finally to run, we do: \n",
    "\n",
    "```bash\n",
    "$ torchrun --nproc_per_node=2 ddp_train_transformers.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
