{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP divides the data and sends each portion to the GPUs. It then centralizes and merges the results. \n",
    "This approach is valid only when each device can run the model independently, since it will make a copy of model to each device.\n",
    "\n",
    "The steps for DP:\n",
    "\n",
    "  1. load Model and batch data to GPU0\n",
    "  2. devide the batch data on GPU0 to other devices\n",
    "  3. copy the model on GPU0 to other devices\n",
    "  4. foward pass on all devices\n",
    "  5. get all outputs to GPU0, compute loss\n",
    "  6. copy loss to all devices and do backward pass to compute gradients\n",
    "  7. get gradients to GPU0\n",
    "  8. update model on GPU0\n",
    "\n",
    "In pratice, we use pytorch - nn.DataParralle\n",
    "\n",
    "\n",
    "Warning: I had a lot of problems using this approach:\n",
    " - it seems that the wrapped model  had hard time to release memory after running.\n",
    " - The copy of data between gpus is very obscure.\n",
    "\n",
    "This approach is not recommanded. It is better to use distributed data parallel approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Example on Classification Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow 7 steps for training:\n",
    "\n",
    " 1) load dataset\n",
    " 2) data loader\n",
    " 3) load model\n",
    " 4) define optimizer\n",
    " 5) define eval function\n",
    " 6) training loop\n",
    " 7) train\n",
    "\n",
    "Compared to training without DP, there are 3 modifications:\n",
    "\n",
    "    A) wrap model: in step 3, after loading the model, wrap the model for parallel computation\n",
    "    B) Scatter data: in step 6 in the for training loop, remove the ops of sending data to gpu0.\n",
    "    C) Loss mean: since for DP, the loss is a tensor of losses retured from all gpus. To do the backward of loss, we have to compute the mean of all losses and then do the backward op."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my gpus are not connected, so I have to disable this option\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_data = \"davidberg/sentiment-reviews\"\n",
    "ckp = \"google-bert/bert-base-uncased\"\n",
    "# ckp = \"michellejieli/emotion_text_classifier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 4084\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = load_dataset(ckp_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class dataset(Dataset):\n",
    "\n",
    "    label2id = {\"positive\": 0, \"negative\": 1}\n",
    "\n",
    "    def __init__(self, _data):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = {\"review\":[], \"division\":[]}\n",
    "        for i in range(len(_data[\"train\"][\"review\"])):\n",
    "            \n",
    "            if _data[\"train\"][i][\"division\"] in dataset.label2id.keys():\n",
    "                self.data[\"review\"].append(_data[\"train\"][i][\"review\"])\n",
    "                self.data[\"division\"].append(_data[\"train\"][i][\"division\"])\n",
    "            # else:\n",
    "            #     print(_data[\"train\"][i][\"review\"], _data[\"train\"][i][\"division\"])\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.data[\"review\"][index], dataset.label2id.get(self.data[\"division\"][index])\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data[\"review\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548\n"
     ]
    }
   ],
   "source": [
    "# construct dataset\n",
    "\n",
    "ds = dataset(data)\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "trainset, validset = random_split(ds, lengths=[0.9, 0.1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function collate\n",
    "\n",
    "def collate_fct(batch):\n",
    "\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for item in batch:\n",
    "\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "\n",
    "    toks = tokenizer(texts, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    toks[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate_fct)\n",
    "validloader = DataLoader(validset, batch_size=32, shuffle=False, collate_fn=collate_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckp)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "#  A. wrap model #\n",
    "##################\n",
    "\n",
    "# wrap the model for data parallel computation\n",
    "\n",
    "# device_ids = None to use all available GPUs\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=None)\n",
    "# model = torch.nn.parallel.DataParallel(model, device_ids=None)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show devices\n",
    "\n",
    "model.device_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this shows that the bert model was wraped as \"DadaParallel\" object\n",
    "# to retrieve the bert model use: model = model.module\n",
    "\n",
    "model.module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. define eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    acc_num = 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for _batch in validloader:\n",
    "            \n",
    "            # if torch.cuda.is_available():\n",
    "            #     _batch = {k:v.cuda() for k, v in _batch.items()}\n",
    "\n",
    "            output = model(**_batch)\n",
    "\n",
    "            pred = torch.argmax(output.logits, dim=-1)\n",
    "\n",
    "            count += len(_batch[\"labels\"])\n",
    "\n",
    "            acc_num += (_batch[\"labels\"].int() == pred.int().cpu()).sum()\n",
    "\n",
    "    return acc_num /count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "def train(epoch=1, log_step=200):\n",
    "\n",
    "    gStep = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in trainloader:\n",
    "            \n",
    "            ###################\n",
    "            # B. Scatter data #\n",
    "            ###################\n",
    "\n",
    "            # on multiple GPU training, don't send the data to gpu\n",
    "            # The DataParallel module will divide the data and copy them from cpu to each gpu\n",
    "            # If we send the original data to gpu first, the module will copy garbage to gpus\n",
    "\n",
    "            # if torch.cuda.is_available():\n",
    "            #     batch = {k:v.to(\"cuda:0\") for k, v in batch.items()}\n",
    "            if batch[\"labels\"].size()[0] != 32:\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(**batch)\n",
    "\n",
    "            ################\n",
    "            # C. Loss mean #\n",
    "            ################\n",
    "\n",
    "            # As mentioned in the DP steps, it is said that the loss was calculated on GPU_data[\"train\"][i][\"division\"]\n",
    "            # However here, the loss was calculated on each GPU and copied to the GPU0.\n",
    "            # Therefore, the the loss value on GPU0 now is a vector instead of a scalar.\n",
    "            # So we have to convert the vector loss to scalar loss for the back-prop to work.\n",
    "            # So instead of doing the line (1)\n",
    "            # output.loss.backward() # (1) \n",
    "            # we do the following line:\n",
    "\n",
    "            output.loss.mean().backward()\n",
    "\n",
    "            # if uncomment the following line, we will see the loss is a vector of n values\n",
    "            # where n is the number of devices (use model.device_ids)\n",
    "            # print(\"old loss: \", batch[\"labels\"].size(), output.loss)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if gStep % log_step == 0:\n",
    "\n",
    "                print(f\"epoch {e+1} / {epoch}: global step: {gStep}, loss: {output.loss.mean().item()}\")\n",
    "\n",
    "            gStep += 1\n",
    "\n",
    "        acc = evaluate()\n",
    "\n",
    "        print(f\"epoch: {e+1} : acc: {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 1: global step: 0, loss: 0.12998534739017487\n",
      "epoch 1 / 1: global step: 20, loss: 0.13787509500980377\n",
      "epoch 1 / 1: global step: 40, loss: 0.11812254041433334\n",
      "epoch 1 / 1: global step: 60, loss: 0.0784941017627716\n",
      "epoch 1 / 1: global step: 80, loss: 0.06834685802459717\n",
      "epoch: 1 : acc: 0.8757061958312988\n"
     ]
    }
   ],
   "source": [
    "train(log_step=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one gpu - batch 32- 4.4G - 41.1s\n",
    "- 2 gpus - batch 32 - 45.3s\n",
    "\n",
    "It is not obvious that DP decreases the training time in this case. But if we increase the batch size, we could see a bigger difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######\n",
    "TODO\n",
    "pytorch parallel not working\n",
    "For this to work, we have to do changes in the source code - not recommanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Example on Classification Using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 17:20:06.854126: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-22 17:20:06.854183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-22 17:20:06.856844: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-22 17:20:06.868897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 17:20:08.629601: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec70ba221e9b453484d97486133d3e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3267 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adb57efc6c3415ebb76053c8b526c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "ckp_data = \"davidberg/sentiment-reviews\"\n",
    "ckp = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "# load data\n",
    "data = load_dataset(ckp_data)\n",
    "\n",
    "split_data = data[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "\n",
    "label2id = {\"positive\": 0, \"negative\": 1}\n",
    "\n",
    "# process data\n",
    "def process(samples):\n",
    "\n",
    "    _data = {\"review\":[], \"division\":[]}\n",
    "    for i in range(len(samples[\"review\"])):\n",
    "        if samples[\"division\"][i] in label2id.keys():\n",
    "            _data[\"review\"].append(samples[\"review\"][i])\n",
    "            _data[\"division\"].append(samples[\"division\"][i])\n",
    "\n",
    "    toks = tokenizer(_data[\"review\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    toks[\"labels\"] = [label2id.get(d) for d in _data[\"division\"]]\n",
    "\n",
    "    return toks\n",
    "\n",
    "tokenized_data = split_data.map(process, batched=True, remove_columns=split_data[\"train\"].column_names)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckp)\n",
    "\n",
    "# metric\n",
    "acc_fct = evaluate.load(\"accuracy\")\n",
    "f1_fct = evaluate.load(\"f1\")\n",
    "\n",
    "def metric(pred):\n",
    "\n",
    "    preds, refs = pred\n",
    "\n",
    "    preds = preds.argmax(axis=-1)\n",
    "\n",
    "    acc = acc_fct.compute(predictions=preds, references=refs)\n",
    "    f1 = f1_fct.compute(predictions=preds, references=refs)\n",
    "    acc.update(f1)\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../tmp/checkpoints\",\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=32,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=True),\n",
    "    compute_metrics=metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=2,\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "batch_eval_metrics=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=True,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "dispatch_batches=None,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_steps=None,\n",
       "eval_strategy=epoch,\n",
       "evaluation_strategy=None,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=32,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=True,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "include_num_input_tokens_seen=False,\n",
       "include_tokens_per_second=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=0,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=./checkpoints/runs/Jun14_16-42-09_Epok,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=10,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_kwargs={},\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=f1,\n",
       "mp_parameters=,\n",
       "neftune_noise_alpha=None,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3.0,\n",
       "optim=adamw_torch,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=./checkpoints,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=32,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['tensorboard'],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=./checkpoints,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_safetensors=True,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "split_batches=None,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_cpu=False,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we see: _n_gpu=2\n",
    "# when this argument is greater than 1, the trainer will use DP automatically.\n",
    "# See source code for details\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got error due to pytorch DP bug\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * wee see that DP doesn't really increase the training speed, and can decrease it.\n",
    " * This is a synchroneous process, whose processing time depends on the slowest process.\n",
    " * The main GPU needs more space for synchronization\n",
    " * it is used for single machine with multiple gpus.\n",
    "\n",
    "This approach is not recommanded. It is better to use distributed data parallel approach.\n",
    "\n",
    "However, it can be used to do inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my gpus are not connected, so I have to disable this option\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and data\n",
    "\n",
    "# imports\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "ckp_data = \"davidberg/sentiment-reviews\"\n",
    "ckp = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "# load data\n",
    "data = load_dataset(ckp_data)\n",
    "\n",
    "# dataset\n",
    "class dataset(Dataset):\n",
    "\n",
    "    label2id = {\"positive\": 0, \"negative\": 1}\n",
    "\n",
    "    def __init__(self, _data):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = {\"review\":[], \"division\":[]}\n",
    "        for i in range(len(_data[\"train\"][\"review\"])):\n",
    "            \n",
    "            if _data[\"train\"][i][\"division\"] in dataset.label2id.keys():\n",
    "                self.data[\"review\"].append(_data[\"train\"][i][\"review\"])\n",
    "                self.data[\"division\"].append(_data[\"train\"][i][\"division\"])\n",
    "            # else:\n",
    "            #     print(_data[\"train\"][i][\"review\"], _data[\"train\"][i][\"division\"])\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.data[\"review\"][index], dataset.label2id.get(self.data[\"division\"][index])\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data[\"review\"])\n",
    "\n",
    "ds = dataset(data)\n",
    "\n",
    "# split dataset\n",
    "trainset, validset = random_split(ds, lengths=[0.9, 0.1])\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "\n",
    "# function collate\n",
    "def collate_fct(batch):\n",
    "\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for item in batch:\n",
    "\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "\n",
    "    toks = tokenizer(texts, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    toks[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "    return toks\n",
    "\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckp)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# wrap model\n",
    "parallel_model = torch.nn.DataParallel(model, device_ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=False, collate_fn=collate_fct)\n",
    "validloader = DataLoader(validset, batch_size=32, shuffle=False, collate_fn=collate_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 s, sys: 1.74 s, total: 28 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mode = \"single\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    for batch in trainloader: # we use trainloader which has more data\n",
    "\n",
    "        if mode == \"single\":  \n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k:v.cuda() for k, v in batch.items()} \n",
    "            output = parallel_model.module(**batch)\n",
    "        \n",
    "        # here we copy the model to gpus for each batch\n",
    "        if mode == \"multiple\":\n",
    "            output = parallel_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# another way to optimize\n",
    "\n",
    "# however, we can't use model.generate to get results.\n",
    "# We can only use parallel_model.module(**input) to get logits and eventually the label\n",
    "\n",
    "# we copy the model only once \n",
    "replicas = parallel_model.replicate(parallel_model.module, parallel_model.device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    for batch in trainloader:\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k:v.cuda() for k, v in batch.items()} \n",
    "\n",
    "            inputs, module_kwargs = parallel_model.scatter(inputs=None, kwargs=batch, device_ids=parallel_model.device_ids)\n",
    "\n",
    "            test = [print(i, len(kwargs[\"labels\"]), kwargs[\"labels\"].get_device()) for i, kwargs in enumerate(module_kwargs)]\n",
    "            #print(inputs, module_kwargs[0][\"labels\"].get_device())\n",
    "\n",
    "            outputs = parallel_model.parallel_apply(replicas, inputs, module_kwargs)\n",
    "            outputs = parallel_model.gather(outputs, parallel_model.output_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared the single and multi GPU inference. This is a not a rigorous timming method, but to illustrate the effect. \n",
    "This shows that bigger the batch size, the more time it save using multi-GPUs.\n",
    "\n",
    "| method        | 32      | 64      |128      |\n",
    "| --------      | ------- |------- |------- |\n",
    "| single GPU    |  18.3   |17.9    |17.4   |\n",
    "| Multiple GPUs | 16.2     |16.4     |10.5    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra:\n",
    "\n",
    " * exaplaination of NCCL: https://medium.com/polo-club-of-data-science/how-to-measure-inter-gpu-connection-speed-single-node-3d122acb93f8\n",
    "\n",
    " * git repo for NCCL: https://github.com/nvidia/nccl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
