{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introdction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerate doesn't provide distributed functionalities, but is an API to facilitate distributed training. It supports many distributed training libs such as DDP, FSDP, Deepspeed.\n",
    "\n",
    "Transformers uses accelerate to help its training process.\n",
    "\n",
    "Officially, it consists 4 steps to use accelerate according to its doc:\n",
    "\n",
    " step1. import accelerate:\n",
    " ```python\n",
    " from accelerate import Accelerator\n",
    " ```\n",
    "\n",
    " step2. instanciate accelerate object\n",
    " ```python\n",
    " accelerator = Accelerator()\n",
    " ```\n",
    "\n",
    " step3. wrap training objets\n",
    " ```python\n",
    " model, optimizer, dataloader, scheduler = accelerator.prepare(model, \n",
    "                                                               optimizer, \n",
    "                                                               dataloader, \n",
    "                                                               scheduler)\n",
    " ```\n",
    "\n",
    " step4. back prop\n",
    "\n",
    " ```python\n",
    " accelerator.backward(loss)\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. How to realize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As example, we take the DDP code as the starting point and modify the code to use accelerate. This can help understand how the distributed concept was realized in accelerate.\n",
    "\n",
    "The steps to follow:\n",
    "\n",
    " step1. we take the code from DDP as initial code example: ddp_train_torch.py.\n",
    "\n",
    " step2. refactor the code into functions (this is optional):\n",
    " \n",
    " * get_dataloader: return tokenized data\n",
    " * get_model: return the model and the corresponding optimizer\n",
    "\n",
    " step3. add main function: this helps structure the script and make it more maintainable.\n",
    " \n",
    " step4. do modifications to use accelerate\n",
    "\n",
    "    A. import accelerator\n",
    "    B. Initialize accelerator\n",
    "    C. wrap objs\n",
    "    D. accelerator backward\n",
    "\n",
    "The above 4 steps are the accelerate basic steps. But this won't work yet, we should do other changes. Mostly, we have to get rid of all DDP stuffs.\n",
    "\n",
    "    E. remove ddp sampler in dataloader\n",
    "    F. remove the ddp wrap\n",
    "    G. remove to gpu: there are 2 places where the to operation should be removed: in the train loop and in the metric function.\n",
    "\n",
    "Until here, the script can run without error. But we will have the problem explained in DDP with the padded batch data. So the accuracy is not correct due to the incorrect data length (this length is the padded length, but not the real length).\n",
    "\n",
    "    H. gather preds\n",
    "    I. change report\n",
    "\n",
    " step5. All modifications are marked and exaplned in the file: accelerate_torch.py. To run do:\n",
    "\n",
    " ```bash\n",
    " $ torchrun --nproc_per_node=2 accelerate_torch.py\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerate provide also a command to run the script. To do so, use terminal:\n",
    "\n",
    "```bash\n",
    "$ accelerate launch your_script.py\n",
    "```\n",
    "\n",
    "If we run accelerate without parameters, the script will be run using default parameters. Otherwise, we can configure the parameters.\n",
    "\n",
    "To configure the accelerate launcher, we do:\n",
    "\n",
    "```bash\n",
    "$ accelerate config\n",
    "```\n",
    "\n",
    "Upon enter, Some prompt questions will show in terminal, use keyboard to select (arrows), and to valid (enter) the options.\n",
    "\n",
    "Arguments used in this context are shown below (depending on the selection, this may change): \n",
    "\n",
    "    * Please select a choice using the arrow or number keys, and selecting with enter\n",
    "        ➔ This machine     \n",
    "            AWS (Amazon SageMaker)  \n",
    "    * Which type of machine are you using?                          \n",
    "        Please select a choice using the arrow or number keys, and selecting with enter\n",
    "        ➔  No distributed training\n",
    "            multi-CPU\n",
    "            multi-XPU\n",
    "            multi-GPU\n",
    "            multi-NPU\n",
    "            multi-MLU\n",
    "            TPU\n",
    "    * How many different machines will you use (use more than 1 for multi-node training)? [1]:\n",
    "    * Should distributed operations be checked while running for errors? This can avoid timeout issues but will be slower. [yes/NO]: \n",
    "    * Do you wish to optimize your script with torch dynamo?[yes/NO]:                        \n",
    "    * Do you want to use DeepSpeed? [yes/NO]:      \n",
    "    * Do you want to use FullyShardedDataParallel? [yes/NO]:   \n",
    "    * Do you want to use Megatron-LM ? [yes/NO]:                                       \n",
    "    * How many GPU(s) should be used for distributed training? [1]:2\n",
    "    * What GPU(s) (by id) should be used for training on this machine as a comma-seperated list? [all]:\n",
    "    * Would you like to enable numa efficiency? (Currently only supported on NVIDIA hardware). [yes/NO]:\n",
    "    * Do you wish to use FP16 or BF16 (mixed precision)?\n",
    "    * Please select a choice using the arrow or number keys, and selecting with enter\n",
    "        ➔  no\n",
    "        fp16\n",
    "        bf16\n",
    "        fp8\n",
    "\n",
    "At the end, a message is shown: \n",
    "```bash\n",
    "accelerate configuration saved at ~/.cache/huggingface/accelerate/default_config.yaml\n",
    "```\n",
    "\n",
    "We can open this file (for VSCode, use ctrl+click on the file name) and see the content of all options:\n",
    "\n",
    "```yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "distributed_type: MULTI_GPU\n",
    "downcast_bf16: 'no'\n",
    "enable_cpu_affinity: false\n",
    "gpu_ids: all\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: 'no'\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false\n",
    "```\n",
    "\n",
    "We can now run the program using: \n",
    "```bash\n",
    "$ accelerate launch accelerate_torch.py \n",
    "```\n",
    "\n",
    "To modify the config, there are some options:\n",
    "\n",
    "     * redo step 1) to regenerate the config file\n",
    "     * modify directly the config file\n",
    "     * copy the config file and modify it, we then can use it using the argument --config_file\" to pass the new config file to the accelerator program\n",
    "\n",
    "To show all accelerate launcher arguments, we can use:\n",
    "```bash\n",
    "$ accelerate launch --help \n",
    "```\n",
    "\n",
    "For the trainer of Transformers, nothing needed to be changed. We can run it directly using:\n",
    "\n",
    "```bash\n",
    "$ accelerate launch ddp_train_transformers.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. mixed precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed precision training allows to minimize memory usage or accelerate the training speed.\n",
    "It uses FP32 and FP16/BF16 to train a model as shown in the schema below.\n",
    "    \n",
    "      fp16 -> forward -> fp16  \n",
    "        |                  |  \n",
    "      fp32 <- backward <- fp32\n",
    "\n",
    "However, can it really decrease the memory usage and speed up training.\n",
    "\n",
    "For the first question, let's take an aexample of a model with N parameters. \n",
    "According to the schema above, we can get the total mem (line total) for the mixed precision is more than 16*N bytes, while for full precision is 16*N bytes. \n",
    "So we can see that the mixed precision don't decrease the mem so far. This is due to the extra copy of the model and some overhead for gradients conversion for the mixed precision.\n",
    "\n",
    "Hovever, if we count the activation (suppose there are A activations), the mem usage will be (16+)*N + 2*A for mixed precision and 16*N + 4*A for full precision.\n",
    "This means the more the activation, the more advantageous to use mixed precision in terms of memory usage.\n",
    "\n",
    "|   \t              | mixed (MB)         | fp32 (MB)   |\n",
    "|---\t              |---\t           |---\t       |\n",
    "| model             | (4 + 2) * N        | 4 *N        |\n",
    "| optimizer         | 8  * N             | 8  * N      |\n",
    "| gradients         | (2+)  * N          | 4  * N      |\n",
    "| total  \t        | (16+)  * N         | 16  * N     |\n",
    "| activation        | 2 * A              | 4 * A       |\n",
    "| total + activation  | (16+)* N  + 2*A  | 16* N + 4*A |\n",
    "\n",
    "To use half precision for accelerate, there are 3 ways:\n",
    "\n",
    " - instanciate accelerator with mixed precision argument:\n",
    "\n",
    "    ```python\n",
    "    accelerator = Accelerator(mixed_precision=\"bf16\")\n",
    "    ```\n",
    " - Use config command: \n",
    " accelerator config -> choose bf16\n",
    " - user launcher with mixed precision argument: \n",
    " accelerator launch --mixed_precision bf16 train.py\n",
    "\n",
    "However, for trainer of Tranformers, we can only use the last 2 methods to run the training with mixed precision.\n",
    "\n",
    "conclusion:\n",
    " - the running time decreased\n",
    " - the momery usage decreases when the batch size is significant\n",
    "\n",
    "In code, see the script accelerate_torch_mixed.py. The modifications are:\n",
    "\n",
    " * IV.A. set mixed precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add gradient accumulation in accelerate, there are 3 steps:\n",
    "\n",
    " 1. specify the accumulation steps while instanciate the accelerate:\n",
    "\n",
    "    ```python\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=xx)\n",
    "    ```\n",
    "\n",
    " 2. add accumulation context in the training loop:\n",
    "\n",
    "    ```python\n",
    "    for batch in trainloader:\n",
    "       with accelerator.accumulate(model): # <- ADD CONTEXT\n",
    "           _optimizer.zero_grad()          # <- indent all lines from here\n",
    "           output=_model(**batch)          # ...\n",
    "           ...                             # ...\n",
    "           gStep += 1                      # <- until here\n",
    "    ```\n",
    "\n",
    " 3. So far, the global step is updated with the batch, which doesn't reflect the training loop.  Instead, the step should be updated at each update of the gradients. accelerate provide a variable to indicate when to update the step. We then can use this as:\n",
    "\n",
    "      ```python\n",
    "      if accelerate.sync_gradients: # <- ADD CONDITION\n",
    "         gStep += 1\n",
    "         if gStep % _log_step  == 0:\n",
    "             ...\n",
    "         ...\n",
    "      ```\n",
    "\n",
    "In code, see the script accelerate_torch_mixed.py. The modifications are:\n",
    "\n",
    " * V.A. set gradient accumulation\n",
    " * V.B. accumulate context \n",
    " * V.C. update accumulation step\n",
    "\n",
    "\n",
    "For the trainer of Transformers, we just have to add the accumulation steps in the training arguments and run the training as before:\n",
    "\n",
    "```python\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    ...\n",
    "    gradient_accumulation_steps=32,\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerate supports all most common logging libs: Tensorboard, Wandb, CometML, Aim, MLFlow, Neptune, Visdom.\n",
    "\n",
    "The steps are:\n",
    "\n",
    " 1. specify the log lib and project_dir while instantiante the accelerator:\n",
    "\n",
    "    ```python\n",
    "    accelerator = Accelerator(log_with=\"tensorboard\", project_dir=\"checkpoints\")\n",
    "    ```\n",
    "\n",
    " 2. initiate tracker\n",
    "\n",
    "    ```python\n",
    "    accelerator.init_trackers(project_name=\"runs\")\n",
    "    ```\n",
    "\n",
    "  3. end trancking after training finishs\n",
    "\n",
    "      ```python\n",
    "      accelerator.end_training()\n",
    "      ```\n",
    "\n",
    " 4. add log variables, we use log of accelerator, which takes a dict and an int as inputs\n",
    "\n",
    "      ```python\n",
    "      accelerator.log({\"loss\": loss.item()}, Step) \n",
    "      ```\n",
    "\n",
    " 5. open tensorboard: see tensorboard doc for details. In VS Code, we can open directly the tensorboard from its control panel (ctrl+shift+p) and choose the dir to open.\n",
    "\n",
    "\n",
    "In code, see the script accelerate_torch_mixed.py. The modifications are:\n",
    "\n",
    " * VI.A. set logging \n",
    " * VI.B. initiate tracker\n",
    " * VI.C. end tracking\n",
    " * VI.D. log variables\n",
    "\n",
    "For the trainer of TRansformers, add logging options in the training arguments as needed, the trainer will take care of the rest correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commons files used are:\n",
    " \n",
    " * weight files (eg. .bin and .safetensors)\n",
    " * config files (eg. config.json): to describe model structure\n",
    " * misc: generation_config.json, adapter_model.safetensors\n",
    "\n",
    "For saving, if on signle machine, we can simply use model.save_pretrained(dir). However, if distributed, the above mothod will reture error. In this case, we can use:\n",
    "\n",
    "```python\n",
    "accelerator.save_model(model, accelerator.project_dir + f\"/step_{step}\")\n",
    "```\n",
    "\n",
    "But, there are 2 problems with method:\n",
    "\n",
    " * it doesn't save the config file, only the weights.\n",
    " * if there are adapters (peft models), it will save the whole model but not the adapters.\n",
    "\n",
    " So another method is to use:\n",
    "\n",
    "```python\n",
    "accelerator.unwrap_model(_model).save_pretrained(...)\n",
    "```\n",
    "\n",
    "\n",
    "In code, see the script accelerate_torch_mixed.py. The modifications are:\n",
    "\n",
    " * VII.A. save\n",
    "\n",
    "\n",
    "For the trainer of Tranformers, no extra things needed to be done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. resume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of resuming training consists of:\n",
    "\n",
    " * save checkpoint regularly\n",
    " * in case of resume, load all resources such as weights, optimizer, learning rate, random state...\n",
    " * skip epochs and steps already done.\n",
    "\n",
    "For accelerate, the steps to follow are:\n",
    "\n",
    " 1. checkpoint:\n",
    "\n",
    "    ```python\n",
    "    accelerator.save_state()\n",
    "    ```\n",
    "    \n",
    "After saving, there will be some files such as:\n",
    "  - model weights: model.safetensors\n",
    "  - optimizer: optimizer.bin\n",
    "  - random state files: random_states_0.pkl, random_states_1.pkl\n",
    "  - adapter files if any: adapter_config.json, adapter_model.safetensors\n",
    "  - ...\n",
    "\n",
    " 2. load\n",
    "\n",
    "    ```python\n",
    "    accelerator.load_state()\n",
    "    ```\n",
    "\n",
    " 3. compute epochs and steps to skip\n",
    "\n",
    " 4. skip epochs and steps\n",
    "\n",
    "    ```python\n",
    "    accelerator.skip_first_batches(trainloader, resume_step)\n",
    "    ```\n",
    "\n",
    "In code, see the script accelerate_torch_mixed.py. The modifications are:\n",
    "\n",
    " * VIII.A. save for resume\n",
    " * VIII.B. add resume option\n",
    " * VIII.C. resume training\n",
    "\n",
    "\n",
    "For the trainer in Tranformers, there are 3 ways to resume the training:\n",
    " \n",
    " * We can load the trained model and train it as done for the finetuning.\n",
    " * In the training arguments, add the resume option:\n",
    "\n",
    "      ```python\n",
    "      args = TrainingArguments(\n",
    "          ...\n",
    "          resume_from_checkpoint=True\n",
    "      )\n",
    "      ```\n",
    "    This restart the saving, thus overwrites the previously saved training.\n",
    "\n",
    " * In the train function of the trainer, add resume option:\n",
    "\n",
    "    ```python\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "    ```\n",
    "   This will continue the training until the epoch number is reached. So, after epochs training and we want to continue the training further, we should increase the epochs numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Introduction\n",
    "\n",
    "The doc of deepspeed is https://huggingface.co/docs/accelerate/main/en/usage_guides/deepspeed#deepspeed-resources. It explained everything of deepspeed, which implments the paper of Zero Redundancy Optimizer (ZeRO).\n",
    "\n",
    "Deepspeed allows load, train or inference of large models on GPUS with limited capacities. It includes:\n",
    " \n",
    " * ZeRO1: Optimizer partitioning\n",
    " * ZeRO2: ZeRO1 + gradients partitioning\n",
    " * ZeRO3: ZeRO2 + parameters partitioning\n",
    " * ZeRO-Offload: resources to CPU\n",
    " * ZeRO++: optimize communication of ZeRO3\n",
    " * DeepSpeed Ulysses: data partitioning\n",
    "\n",
    "### B. Usage\n",
    "\n",
    "Before using it, we need to pip install it.\n",
    "\n",
    "In practice, we could use accelerate config to activate deepspeed and accelerate launch to run the training with default arguments. However, since we would like to adapt the arguments with our use case, the easier way is to launch the accelerate with a customized config file. \n",
    "The steps to do so are:\n",
    "\n",
    " * generate accelerate config file by enabling deepspeed, use:\n",
    " \n",
    "    ```bash\n",
    "    $ accelerate config\n",
    "    ```\n",
    "\n",
    "   All other related options can be set to NO or None and the for now we use stage 2 optimization.\n",
    "\n",
    " * For not to crush the original config file, we copy the default config file in another location (here, the current folder), since we will modify this file to run the deepspeed in different modes.\n",
    "   To copy to the current folder, we can do:\n",
    "\n",
    "   ```bash\n",
    "   $ cp /home/Qingyi/.cache/huggingface/accelerate/default_config.yaml  ./\n",
    "   ```\n",
    "\n",
    "#### B.1. default config\n",
    "\n",
    "So far, we can run the script with ZeRO2 already. The corresponding settings in the config file are:\n",
    "\n",
    "```yaml\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 1\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  zero_stage: 2\n",
    "```\n",
    "Where \"zero_stage\" sets the deepspeed to ZeRO2.\n",
    "\n",
    "Currently, we can either use the config file generated by accelerate, or we can use the copied config file. For the latter, we have to use an argument:\n",
    "\n",
    "  ```bash\n",
    "  $ accelerate launch --config_file default_config.yaml accelerate_torch_mixed.py.py\n",
    "  ```\n",
    "\n",
    "#### B.2. custom config\n",
    "\n",
    "In the previous steps when generate accelerate config file, while asked providing a config file (for deepspeed, don't be confused with accelerate config file), we entered none. In the following steps, we will use a deepspeed config file.\n",
    "The content of this file can be obtained from the deepspeed doc page (the link at the top of this section) and be adapted to a use case.\n",
    "The steps to be followed are:\n",
    "\n",
    " 1. comment the chendren fileds in the section \"deepspeed_condig\" shown previously in the accelerate config file. Also comment the line \"mixed_precision: bf16\".\n",
    " 2. add a new entry in this section: \n",
    "\n",
    "    ```yaml\n",
    "    deepspeed_config:\n",
    "      deepspeed_config_file: zero2_config.json\n",
    "    ```\n",
    " 3. create \"zero2_config.json\" file. Copy the example of \"ZeRO Stage-2 DeepSpeed Config File Example\" section in the doc to a file. Be careful that the config file of deepspeed is json, whereas the config file of accelerate is yaml. We should adpat the content to our training case (see the json file for the changes.).\n",
    "\n",
    "\n",
    " For ZeRO3, we can also use a config file as for ZeRO2.\n",
    "\n",
    "### C. Traniner\n",
    "\n",
    "For trainer of transformers, there is almost nothing extra to do to run it using deepspeed.\n",
    "However, we may encounter errors. This mostly caused by the differences of the trainer arguments and the deepspeed arguments, such as mixed precision set for deepspeed but not in the trainer arguments, gradient accumulation steps has different number for the two.\n",
    "\n",
    "To run the example, do:\n",
    "\n",
    "```bash\n",
    "$ accelerate launch --config_file default_config.yaml ddp_train_transformers.py\n",
    "```\n",
    "\n",
    "\n",
    "### D. Error Handling\n",
    "\n",
    "Deepspeed gives very obscure error messages, here is a list of possible errors, but most of them are caused by argument differences:\n",
    "\n",
    " 1. when getting the error message \"deepspeed.ops.op_builder.builder.MissingCUDAException: CUDA_HOME does not exist, unable to compile CUDA op(s)\", this usually means that the cuda driver or its path is missing. There are several things can be done:\n",
    "    - In linux, check the if the cuda driver can be accessed using (or which nvss which supposed to reture the installation path):\n",
    "        ```bash\n",
    "        $ nvcc --version\n",
    "        ```\n",
    "    - If you get error message as \"command not found\", check if the cuda driver was actually installed by looking for its installation folder (this path can vary depending the os, in my case using gentoo, look in opt instead of usr):\n",
    "        ```bash\n",
    "        $ ls /usr/local\n",
    "        ```\n",
    "    - If the cuda installation folder existes, this means that cuda driver is installed but its path is not registered in the system. So we just need to setup the correcponding environment variable.\n",
    "        ```bash\n",
    "        $ export CUDA_HOME=/the/path/to/cuda\n",
    "        ```\n",
    "    - if the cuda driver not installed, you have to install it first.\n",
    "\n",
    "    For conda, refer to its doc for installation.\n",
    "\n",
    "  2. There is only one mixed precision argument. Since we set \"bf16\" to enabled in the deepspeed config file, the same option in the accelerate config file should be commentted. Otherwise, it will issu error.\n",
    "\n",
    "  3. The accumulation steps should be set to 1 in the training code.\n",
    "\n",
    "  4. if the model was trained using 16bit and we use the deepspeed config file, the \"mixed_precision\" option should be commented out in the accelerate config file.\n",
    "\n",
    "  5. For ZeRO3, if the model is trained using 16 bits and loading and saving are involved, the option \"zero3_save_16bit_model: true\" should be added to the deepspeed section.\n",
    "\n",
    "  6. For ZeRO3, make sure to use no_grad() instead of inference_mode() in the evaluation function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
