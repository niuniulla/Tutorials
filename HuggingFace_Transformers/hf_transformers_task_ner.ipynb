{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER (Named Entity Recognition)\n",
    "\n",
    "## 1. Ner example\n",
    "\n",
    "It allow to abstrct designated objects in a text.\n",
    "\n",
    "eg. Paul is at the the parc.\n",
    "\n",
    "\n",
    "| type    |  object |\n",
    "----------|:-------:|\n",
    "| place   |  parc   |\n",
    "| subject |  Paul   |\n",
    "\n",
    "\n",
    "## 2. labeling system\n",
    "\n",
    "    - IOB\n",
    "    - IOBES\n",
    "\n",
    "\n",
    "## 3. evaluation\n",
    "\n",
    "    - metrics: precision recall, F1\n",
    "\n",
    "eg.\n",
    "\n",
    "\n",
    "|truth:| The | old | lady | whirled | round | and | snatched | her  | skirts | out | of   | danger |\n",
    "|------|:---:|----:|-----:|--------:|------:|----:|---------:|-----:|-------:|----:|-----:|-----:|\n",
    "|gold: | <font color=\"blue\">b-A</font> | <font color=\"blue\">i-A</font> | <font color=\"blue\">i-A</font>  |  <font color=\"blue\">e-A</font>    | o   | o   | b-DSE    |i-DSE | e-DSE  | o   | <font color=\"yellow\">b-T </font>   | <font color=\"yellow\">e-T</font>    |\n",
    "|pred: | o   | <font color=\"pink\">b-A</font> |<font color=\"pink\">e-A</font>  |   o      | b-DSE | o   | <font color=\"red\">b-DSE</font>   |<font color=\"red\">i-DSE</font> | <font color=\"red\">e-DSE</font>  | o   | b-T  | i-T    |\n",
    "\n",
    "\n",
    "The truth has 3 objects marked as blue, white and yellow. The prediction identified 2 objects marked as pink and red, where red is the correct one.\n",
    "\n",
    "So the corresponding confusion matrix is as follow where the columns are the ground truth and the lines are the predictions: \n",
    "\n",
    "|   | P  | N  | \n",
    "|---|---|---|\n",
    "| P | 1  | 1 |\n",
    "| N | 2  | - |\n",
    "\n",
    "$$Precision = \\frac{1}{1+1}$$\n",
    "$$Recall = \\frac{1}{1+2}$$\n",
    "$$F1 = 2 * \\frac{P * R}{P + R} = 2 * \\frac{1/2 * 1/3}{1/2 + 1/3}$$\n",
    "\n",
    "Where precision measure the rate of correct identifications over the true entities, recall measures the rate of correct identifications over all predictions. And f1 is the averaged rate of ^recision and recall.\n",
    "\n",
    "Inspired from : https://huggingface.co/learn/nlp-course/chapter7/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not installed, uncomment and run this\n",
    "\n",
    "# !python -m pip install seqeval --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 10:54:51.768523: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-29 10:54:51.768625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-29 10:54:51.770865: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-29 10:54:51.785118: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-29 10:54:53.914764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/ashleyliu31/tech_product_names_ner\n",
    "\n",
    "data = load_dataset(\"ashleyliu31/tech_product_names_ner\", cache_dir='./ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split data since the original dataset don't provide test set\n",
    "# Only dataset can be split, not the datadict\n",
    "\n",
    "data = data[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': Value(dtype='int64', id=None),\n",
       " 'sentence': Value(dtype='string', id=None),\n",
       " 'word_labels': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the features of the dataset\n",
    "\n",
    "data[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 485, 'sentence': \"The Acer TravelMate TMP-249-0's subpar screen quality diminishes its overall user experience.\", 'word_labels': 'O, B-pn, I-pn, I-pn, O, O, O, O, O, O, O, O, O, O'}\n",
      "93\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "# if we look closer to the data, we realize that the format of the data are all in string\n",
    "# we have to cut them apart and do some clean-ups (remove punctuations)\n",
    "\n",
    "ind = 0\n",
    "print(data[\"train\"][ind])\n",
    "print(len(data[\"train\"][ind][\"sentence\"]))\n",
    "print(len(data[\"train\"][ind][\"word_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but one thing for sure is that the labels consist of 3 symbols that we need later\n",
    "# so we define them here\n",
    "\n",
    "label_list = {'O':0, 'B-pn':1, 'I-pn':2}\n",
    "label_list_inv = {0:'O', 1:'B-pn', 2:'I-pn'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 44, 'sentence': 'The HP Pavilion 15-af006ax is praised for its sleek design and robust performance.', 'word_labels': 'O, B-pn, I-pn, I-pn, O, O, O, O, O, O, O, O, O, O'}\n",
      "['The', 'HP', 'Pavilion', '15-af006ax', 'is', 'praised', 'for', 'its', 'sleek', 'design', 'and', 'robust', 'performance.']\n",
      "{'input_ids': [101, 1996, 6522, 10531, 2321, 1011, 21358, 8889, 2575, 8528, 2003, 5868, 2005, 2049, 21185, 2640, 1998, 15873, 2836, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['O', 'B-pn', 'I-pn', 'I-pn', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "21\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# After some tweaks, I found a way to treat the data.\n",
    "# For both sentence and labels:\n",
    "# - remove punctuations (except - and . for model names)\n",
    "# - split the string by space\n",
    "\n",
    "ind = 169 # pick a sample\n",
    "\n",
    "# remove , and . at the end of the sentence (so we replace \". \" instead of \".\")\n",
    "# split the sentence into list\n",
    "sen = data[\"train\"][ind][\"sentence\"].replace(\",\", \" \").replace(\". \", \" \").split()\n",
    "\n",
    "# we tokenize the sentence\n",
    "# the option \"is_split_into_words\" works on list instead of string\n",
    "tok = tokenizer(sen, is_split_into_words=True)\n",
    "\n",
    "# It can be seen that the tokenized sentence has different length as the labels\n",
    "# This is because the way the tokenizer separate the sentence, which is not solely based\n",
    "# on word level, but the label was based on word.                                  \n",
    "\n",
    "print(data[\"train\"][ind])\n",
    "print(sen)\n",
    "print(tok)\n",
    "print(data[\"train\"][ind][\"word_labels\"].split(\", \"))\n",
    "print(len(tok[\"input_ids\"]))\n",
    "print(len(data[\"train\"][ind][\"word_labels\"].split(\", \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The tokenizer provides information about the tokens such as the word it was tokenized.\n",
    "# Where None is the special token marks the begining and the end of the sentence\n",
    "\n",
    "# we should take consideration of this info when we re-construct the label\n",
    "tok.tokens() # show the tokens\n",
    "tok.word_ids() # show the ids of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by putting all above mentioned processing together, we define the function \n",
    "# to preprocess the data\n",
    "\n",
    "def process(examples) :\n",
    "\n",
    "    # convert strings to list\n",
    "    sens = [ex.replace(\",\", \" \").replace(\". \", \" \").split() for ex in examples[\"sentence\"]]\n",
    "    lab_raw = [ex.split(\", \") for ex in examples[\"word_labels\"]]\n",
    "\n",
    "    tokenized = tokenizer(sens, is_split_into_words=True, max_length=128, truncation=True)\n",
    "    \n",
    "    labels = []\n",
    "    for i, lab in enumerate(lab_raw):\n",
    "        \n",
    "        lab = [label_list[x] for x in lab] # convert word label to num label\n",
    "\n",
    "        label = [-100 if l is None else lab[int(l)] for l in tokenized.word_ids(batch_index=i)] # match label length to token length\n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "        assert len(label) == len(tokenized[\"input_ids\"][i])\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e849a8035f54548b7adff0065afd672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65589bfa8951488aa50a829a2c0433b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess the data\n",
    "\n",
    "tokenized_data = data.map(process, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 485, 'sentence': \"The Acer TravelMate TMP-249-0's subpar screen quality diminishes its overall user experience.\", 'word_labels': 'O, B-pn, I-pn, I-pn, O, O, O, O, O, O, O, O, O, O', 'input_ids': [101, 1996, 9078, 2099, 3604, 8585, 1056, 8737, 1011, 23628, 1011, 1014, 1005, 1055, 4942, 19362, 3898, 3737, 11737, 5498, 4095, 2229, 2049, 3452, 5310, 3325, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "# show preprocessed data\n",
    "print(tokenized_data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "# By default, the output dim is 2.\n",
    "# However, we need a 3-class classifier.\n",
    "# So to change the output dim, we use the option \"num_labels\" to specify the wanted dim.\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can check the output dim is indeed 3\n",
    "\n",
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n",
       "Produces labelling scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
       "    references: List of List of reference labels (Ground truth (correct) target values)\n",
       "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
       "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
       "        default: None\n",
       "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
       "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
       "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
       "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
       "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
       "\n",
       "Returns:\n",
       "    'scores': dict. Summary of the scores for overall and per type\n",
       "        Overall:\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
       "        Per type:\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> seqeval = evaluate.load(\"seqeval\")\n",
       "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
       "    >>> print(results[\"overall_f1\"])\n",
       "    0.5\n",
       "    >>> print(results[\"PER\"][\"f1\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define evaluation module\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_metric(preds):\n",
    "\n",
    "    pred, label = preds\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "    pred_word_labels = []\n",
    "    label_word_labels = []\n",
    "\n",
    "    for p, l in zip(pred, label) :\n",
    "        if len(p) != len(l):\n",
    "            print(p)\n",
    "            print(l)\n",
    "\n",
    "        pred_word_labels.append([label_list_inv[i] for i, j in zip(p, l) if j != -100])\n",
    "\n",
    "        label_word_labels.append([label_list_inv[i] for i in l if i != -100 ])\n",
    "\n",
    "    result = seqeval.compute(predictions=pred_word_labels, references=label_word_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "    \n",
    "    return {'f1': result[\"overall_f1\"]} # we have to reture a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=128,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"f1\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=10,\n",
    "        use_cpu=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 04:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0745091438293457,\n",
       " 'eval_f1': 0.036885245901639344,\n",
       " 'eval_runtime': 25.3014,\n",
       " 'eval_samples_per_second': 46.084,\n",
       " 'eval_steps_per_second': 0.395}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 14:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.037632</td>\n",
       "      <td>0.926089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.022258</td>\n",
       "      <td>0.954174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.014311</td>\n",
       "      <td>0.981845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=219, training_loss=0.057121608559399434, metrics={'train_runtime': 846.9158, 'train_samples_per_second': 16.514, 'train_steps_per_second': 0.259, 'total_flos': 243321535569708.0, 'train_loss': 0.057121608559399434, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.014311173930764198,\n",
       " 'eval_f1': 0.9818445896877269,\n",
       " 'eval_runtime': 20.1221,\n",
       " 'eval_samples_per_second': 57.946,\n",
       " 'eval_steps_per_second': 0.497,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_data[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use pipeline to evaluate the model\n",
    "pipe_ner = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-pn\",\n",
      "    \"2\": \"I-pn\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-pn\": 1,\n",
      "    \"I-pn\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we have to customized the label-id conversion to output meaningful result\n",
    "# otherwise, the results will only show default labels\n",
    "\n",
    "model.config.label2id = label_list\n",
    "model.config.id2label = label_list_inv\n",
    "print(model.config) # show the configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'pn',\n",
       "  'score': 0.97650355,\n",
       "  'word': 'laser',\n",
       "  'start': 13,\n",
       "  'end': 18},\n",
       " {'entity_group': 'pn',\n",
       "  'score': 0.87703407,\n",
       "  'word': '##jet',\n",
       "  'start': 18,\n",
       "  'end': 21},\n",
       " {'entity_group': 'pn',\n",
       "  'score': 0.9707254,\n",
       "  'word': 'laser',\n",
       "  'start': 42,\n",
       "  'end': 47},\n",
       " {'entity_group': 'pn',\n",
       "  'score': 0.64945084,\n",
       "  'word': '##writer',\n",
       "  'start': 47,\n",
       "  'end': 53}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_ner(\"The first HP LaserJet and the first Apple LaserWriter used the same print engine, the Canon CX engine\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
