{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will:\n",
    "\n",
    "    I. present the use of tokenizer\n",
    "    II. show how to use the tokenizer tool of transformers\n",
    "    III. compare the fast and slow tokenizer\n",
    "    IV. Other notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Use of tokenizer\n",
    "\n",
    "The tokenizer was used to :\n",
    "\n",
    "* tokenize texts: convert text to units (word, grams, syllables, prefix... or mixed of them)\n",
    "\n",
    "* construct vocabulary: based on the tokens, construct a vocabulary in order to numerize the units\n",
    "\n",
    "* numerize the text units\n",
    "\n",
    "* padding / truncation: pad the short text and truncate the long ones. This ensures the text length don't exceed the max\n",
    "  length of model and also the batch has same length data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Usage\n",
    "\n",
    "How to use:\n",
    "\n",
    "1) import\n",
    "\n",
    "2) load: from_pretrained\n",
    "\n",
    "3) save: save_pretrained\n",
    "\n",
    "4) tokenize: tokenize\n",
    "\n",
    "5) inspect vocabulary: vocab\n",
    "\n",
    "6) converting: convert_tokens_to_ids / convert_ids_to_tokens\n",
    "\n",
    "7) padding / truncate\n",
    "\n",
    "8) outputs: input_ids, token_type_ids, attention_mask..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) import\n",
    "###########\n",
    "\n",
    "# if we don't know which tokenizer to use, we can just use AutoTokenizer\n",
    "\n",
    "# this will load automatically the corresponding tokenizer\n",
    "\n",
    "# Sometimes, we have to import the correct tokenizer by ourself: this will be explained later\n",
    "\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) load\n",
    "#########\n",
    "\n",
    "# By using the hf repository for the first time, it will download directly from the HF site.\n",
    "\n",
    "# By default, the downloaded files are in: ~/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/\n",
    "\n",
    "# If we load again this model, it will load from the local cache folder, not from the HF site.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tmp/my_tokenizer/tokenizer_config.json',\n",
       " './tmp/my_tokenizer/special_tokens_map.json',\n",
       " './tmp/my_tokenizer/vocab.txt',\n",
       " './tmp/my_tokenizer/added_tokens.json',\n",
       " './tmp/my_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) save\n",
    "#########\n",
    "\n",
    "# We can save it anywhare we want by providing a directory\n",
    "\n",
    "tokenizer.save_pretrained(\"./tmp/my_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and reload it later from the local directory\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tmp/my_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'can', 'save', 'any', 'crypt', '##oc', '##ur', '##ren', '##cy', 'we', 'want', '.']\n"
     ]
    }
   ],
   "source": [
    "# 4) tokenize\n",
    "#############\n",
    "\n",
    "text = \"We can save any cryptocurrency we want.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': 20407,\n",
       " 'disguised': 17330,\n",
       " 'nasa': 9274,\n",
       " 'decorate': 29460,\n",
       " '##土': 30327,\n",
       " 'shamrock': 28782,\n",
       " '##posed': 19155,\n",
       " 'cesare': 26708,\n",
       " 'basha': 26074,\n",
       " 'ernest': 8471,\n",
       " 'isa': 18061,\n",
       " 'occult': 27906,\n",
       " '##り': 30212,\n",
       " '##γ': 29721,\n",
       " 'dub': 12931,\n",
       " 'inevitable': 13418,\n",
       " 'tow': 15805,\n",
       " 'voltage': 10004,\n",
       " 'francisco': 3799,\n",
       " 'limits': 6537,\n",
       " 'yiddish': 20112,\n",
       " 'lastly': 22267,\n",
       " 'yielding': 21336,\n",
       " 'savoy': 16394,\n",
       " 'cupboard': 25337,\n",
       " 'kyle': 7648,\n",
       " '##rmed': 29540,\n",
       " 'fallen': 5357,\n",
       " 'parentheses': 27393,\n",
       " 'blue': 2630,\n",
       " '[unused26]': 27,\n",
       " 'jaw': 5730,\n",
       " 'favoured': 16822,\n",
       " 'dwellings': 16707,\n",
       " 'write': 4339,\n",
       " 'beginning': 2927,\n",
       " '##mount': 20048,\n",
       " '[unused236]': 241,\n",
       " '×': 1095,\n",
       " 'rectangular': 10806,\n",
       " 'abandonment': 22290,\n",
       " 'bates': 11205,\n",
       " 'tome': 21269,\n",
       " 'yearning': 29479,\n",
       " '##trick': 22881,\n",
       " 'battalion': 4123,\n",
       " 'elevator': 7764,\n",
       " '##oco': 24163,\n",
       " 'racers': 25791,\n",
       " '##llen': 12179,\n",
       " 'carr': 12385,\n",
       " 'suspicions': 17817,\n",
       " 'ibm': 9980,\n",
       " '##pipe': 24548,\n",
       " 'fluctuations': 28892,\n",
       " '##ae': 6679,\n",
       " '##heads': 13038,\n",
       " '##sia': 8464,\n",
       " 'clue': 9789,\n",
       " 'shaggy': 25741,\n",
       " '[unused102]': 107,\n",
       " '##dev': 24844,\n",
       " '[unused716]': 721,\n",
       " 'commissioned': 4837,\n",
       " 'brighton': 10309,\n",
       " 'bauer': 17838,\n",
       " 'graphs': 19287,\n",
       " 'secondly': 16378,\n",
       " 'worthless': 22692,\n",
       " 'pope': 4831,\n",
       " 'restoration': 6418,\n",
       " '[unused9]': 10,\n",
       " '##fied': 10451,\n",
       " '##マ': 30249,\n",
       " '##old': 11614,\n",
       " 'shovel': 24596,\n",
       " 'revelation': 11449,\n",
       " 'epa': 19044,\n",
       " '##rod': 14127,\n",
       " 'prodigy': 28334,\n",
       " 'carousel': 27628,\n",
       " '1690': 24765,\n",
       " 'signifies': 27353,\n",
       " 'assassinated': 16370,\n",
       " '英': 1941,\n",
       " 'egg': 8288,\n",
       " 'speculated': 15520,\n",
       " 'movements': 5750,\n",
       " 'olive': 9724,\n",
       " 'defect': 21262,\n",
       " 'trade': 3119,\n",
       " '[unused172]': 177,\n",
       " 'reduced': 4359,\n",
       " '##rus': 7946,\n",
       " 'theatrical': 8900,\n",
       " 'magician': 16669,\n",
       " 'rejoin': 25261,\n",
       " '##izer': 17629,\n",
       " 'ata': 29533,\n",
       " 'manson': 21440,\n",
       " 'hilary': 22744,\n",
       " 'alexis': 13573,\n",
       " '[unused399]': 404,\n",
       " '##cious': 18436,\n",
       " 'disappeared': 5419,\n",
       " 'reeves': 17891,\n",
       " '[unused0]': 1,\n",
       " 'misty': 15167,\n",
       " 'cemetery': 4528,\n",
       " 'registration': 8819,\n",
       " 'jerusalem': 6744,\n",
       " 'fashioned': 13405,\n",
       " 'unknown': 4242,\n",
       " 'breakaway': 26321,\n",
       " 'incense': 28647,\n",
       " 'abandoned': 4704,\n",
       " '##伊': 30288,\n",
       " 'neighbors': 10638,\n",
       " 'foreign': 3097,\n",
       " 'manly': 19385,\n",
       " 'elgin': 23792,\n",
       " 'narrated': 17356,\n",
       " 'drawers': 22497,\n",
       " 'vale': 10380,\n",
       " '[unused785]': 790,\n",
       " 'vaguely': 15221,\n",
       " '##ₙ': 19110,\n",
       " 'belt': 5583,\n",
       " 'administrators': 15631,\n",
       " '##ega': 29107,\n",
       " 'pastures': 24813,\n",
       " '##has': 14949,\n",
       " 'reflex': 22259,\n",
       " 'crawford': 10554,\n",
       " '都': 1961,\n",
       " '##mas': 9335,\n",
       " 'scrambled': 13501,\n",
       " '##frey': 22586,\n",
       " 'shipping': 7829,\n",
       " 'baseman': 18038,\n",
       " 'killed': 2730,\n",
       " '##itic': 18291,\n",
       " 'impact': 4254,\n",
       " 'reunite': 25372,\n",
       " 'projections': 21796,\n",
       " 'freaking': 13847,\n",
       " 'humiliated': 26608,\n",
       " 'iucn': 20333,\n",
       " 'taboo': 27505,\n",
       " 'appear': 3711,\n",
       " 'infused': 29592,\n",
       " 'illusions': 24883,\n",
       " '##notes': 20564,\n",
       " 'gross': 7977,\n",
       " '1758': 16832,\n",
       " '##masters': 27751,\n",
       " 'ended': 3092,\n",
       " 'evans': 6473,\n",
       " 'presumed': 14609,\n",
       " '##ノ': 30243,\n",
       " 'burundi': 28836,\n",
       " '##yra': 19563,\n",
       " 'ruben': 19469,\n",
       " 'notes': 3964,\n",
       " 'naturalist': 19176,\n",
       " 'runway': 9271,\n",
       " 'seas': 11915,\n",
       " 'synthesizers': 22211,\n",
       " 'directs': 23303,\n",
       " 'resign': 12897,\n",
       " 'booths': 27612,\n",
       " 'aragon': 16146,\n",
       " 'struggle': 5998,\n",
       " 'abigail': 15983,\n",
       " '##て': 30191,\n",
       " '51st': 26017,\n",
       " 'besieged': 17923,\n",
       " 'relegation': 9591,\n",
       " 'veterans': 8244,\n",
       " 'alpine': 10348,\n",
       " 'opinions': 10740,\n",
       " 'schmidt': 12940,\n",
       " '##vos': 19862,\n",
       " 'gillian': 22358,\n",
       " 'چ': 1303,\n",
       " 'campbell': 6063,\n",
       " 'birch': 16421,\n",
       " 'esa': 28776,\n",
       " 'strikes': 9326,\n",
       " 'dudley': 12648,\n",
       " 'legislation': 6094,\n",
       " 'probation': 19703,\n",
       " 'wood': 3536,\n",
       " '##eta': 12928,\n",
       " 'dialogues': 22580,\n",
       " 'lok': 13660,\n",
       " 'drones': 24633,\n",
       " 'perimeter': 13443,\n",
       " 'vampires': 6144,\n",
       " 'pausing': 20490,\n",
       " 'roi': 25223,\n",
       " 'retracted': 28214,\n",
       " 'animator': 25132,\n",
       " 'suffers': 17567,\n",
       " 'assess': 14358,\n",
       " '[unused400]': 405,\n",
       " '[SEP]': 102,\n",
       " 'avon': 16131,\n",
       " 'demo': 9703,\n",
       " 'strengths': 20828,\n",
       " 'lodges': 26767,\n",
       " 'wins': 5222,\n",
       " 'bite': 6805,\n",
       " 'anglican': 9437,\n",
       " 'ton': 10228,\n",
       " '##sque': 17729,\n",
       " 'plunge': 25912,\n",
       " '##sight': 25807,\n",
       " 'vibration': 17880,\n",
       " '[unused356]': 361,\n",
       " 'analysts': 18288,\n",
       " 'jia': 25871,\n",
       " 'himself': 2370,\n",
       " 'raid': 8118,\n",
       " 'runoff': 19550,\n",
       " '##″': 30067,\n",
       " 'selected': 3479,\n",
       " 'loads': 15665,\n",
       " 'appropriately': 23263,\n",
       " 'flaw': 28450,\n",
       " 'stint': 12116,\n",
       " 'water': 2300,\n",
       " 'lust': 11516,\n",
       " 'splitting': 14541,\n",
       " 'torino': 24737,\n",
       " 'computed': 24806,\n",
       " 'ء': 1269,\n",
       " 'rc': 22110,\n",
       " '##jar': 16084,\n",
       " '##anov': 25417,\n",
       " 'casualties': 8664,\n",
       " 'cartridge': 15110,\n",
       " '153': 16710,\n",
       " 'cuisine': 12846,\n",
       " 'seeking': 6224,\n",
       " 'realization': 12393,\n",
       " 'tibet': 13319,\n",
       " 'proportional': 14267,\n",
       " 'jon': 6285,\n",
       " 'discoveries': 15636,\n",
       " '##lewood': 26580,\n",
       " 'laurie': 16450,\n",
       " 'assisting': 13951,\n",
       " '##д': 29742,\n",
       " 'hackney': 28425,\n",
       " 'navajo': 22440,\n",
       " 'fixed': 4964,\n",
       " 'jillian': 27286,\n",
       " 'perspectives': 15251,\n",
       " 'upstairs': 8721,\n",
       " '##ade': 9648,\n",
       " 'ո': 1232,\n",
       " 'rep': 16360,\n",
       " '[unused363]': 368,\n",
       " '##uman': 19042,\n",
       " '00pm': 27995,\n",
       " 'calendar': 8094,\n",
       " '##ulation': 9513,\n",
       " 'burton': 9658,\n",
       " 'unreliable': 23579,\n",
       " '266': 25162,\n",
       " 'maize': 21154,\n",
       " 'projectile': 25921,\n",
       " '##lusion': 24117,\n",
       " 'talbot': 14838,\n",
       " 'salle': 18005,\n",
       " '##art': 8445,\n",
       " '##quay': 25575,\n",
       " 'dorchester': 27252,\n",
       " 'combination': 5257,\n",
       " '##wheel': 22920,\n",
       " 'cbc': 13581,\n",
       " 'bypass': 11826,\n",
       " '##ay': 4710,\n",
       " 'beetles': 14538,\n",
       " 'cantata': 23629,\n",
       " 'biographical': 16747,\n",
       " 'blackpool': 17444,\n",
       " 'lukas': 23739,\n",
       " '##row': 10524,\n",
       " 'toxin': 29090,\n",
       " 'advertisement': 15147,\n",
       " '##able': 3085,\n",
       " 'froze': 10619,\n",
       " 'despised': 26626,\n",
       " 'touring': 6828,\n",
       " 'abs': 14689,\n",
       " 'register': 4236,\n",
       " '☉': 1622,\n",
       " '##街': 30472,\n",
       " 'paved': 12308,\n",
       " 'ர': 1391,\n",
       " 'imprisoned': 8580,\n",
       " 'filing': 15242,\n",
       " 'soaking': 22721,\n",
       " 'atari': 18978,\n",
       " 'que': 10861,\n",
       " '##sett': 21678,\n",
       " 'townsend': 16139,\n",
       " '720': 22857,\n",
       " '[unused622]': 627,\n",
       " 'ye': 6300,\n",
       " 'elbows': 13690,\n",
       " 'bam': 25307,\n",
       " 'households': 3911,\n",
       " 'substantial': 6937,\n",
       " 'dart': 14957,\n",
       " 'forge': 15681,\n",
       " 'ය': 1404,\n",
       " 'giovanni': 9136,\n",
       " 'goo': 27571,\n",
       " 'palazzo': 18482,\n",
       " 'described': 2649,\n",
       " '##wig': 16279,\n",
       " 'critics': 4401,\n",
       " '7': 1021,\n",
       " 'boulder': 13264,\n",
       " '##say': 24322,\n",
       " 'punta': 27377,\n",
       " 'polling': 17888,\n",
       " 'businesses': 5661,\n",
       " 'baroque': 10456,\n",
       " '##sius': 24721,\n",
       " '##bala': 25060,\n",
       " 'lil': 13451,\n",
       " 'carlo': 9758,\n",
       " 'friend': 2767,\n",
       " 'tb': 26419,\n",
       " 'primetime': 18474,\n",
       " 'vogue': 17734,\n",
       " 'yoko': 28758,\n",
       " 'wrought': 18481,\n",
       " '##sc': 11020,\n",
       " 'announced': 2623,\n",
       " 'encounters': 11340,\n",
       " 'wrestlemania': 28063,\n",
       " 'plants': 4264,\n",
       " 'unwilling': 15175,\n",
       " 'pussy': 22418,\n",
       " '##տ': 29783,\n",
       " 'corrosion': 24625,\n",
       " '##年': 30366,\n",
       " 'es': 9686,\n",
       " 'buddhism': 11388,\n",
       " 'a2': 22441,\n",
       " '##―': 30053,\n",
       " 'manchester': 5087,\n",
       " 'rein': 27788,\n",
       " '##ola': 6030,\n",
       " 'democrat': 7672,\n",
       " 'postponed': 14475,\n",
       " 'specimen': 11375,\n",
       " 'ceramics': 17314,\n",
       " 'argus': 25294,\n",
       " 'collegiate': 9234,\n",
       " 'chalk': 16833,\n",
       " 'slabs': 28761,\n",
       " 'goldsmith': 22389,\n",
       " 'wwe': 11700,\n",
       " 'wen': 19181,\n",
       " 'saskatchewan': 10068,\n",
       " '##rand': 13033,\n",
       " '##igo': 14031,\n",
       " 'drills': 28308,\n",
       " 'doomed': 20076,\n",
       " 'rupert': 14641,\n",
       " '##tyle': 27983,\n",
       " '41': 4601,\n",
       " 'hot': 2980,\n",
       " 'specialised': 17009,\n",
       " 'monks': 9978,\n",
       " 'viable': 14874,\n",
       " '##hil': 19466,\n",
       " 'sizes': 10826,\n",
       " 'kong': 4290,\n",
       " 'mature': 9677,\n",
       " 'parallel': 5903,\n",
       " 'guns': 4409,\n",
       " '178': 19289,\n",
       " 'car': 2482,\n",
       " '##lices': 29146,\n",
       " '##real': 22852,\n",
       " 'prisoners': 5895,\n",
       " 'ahead': 3805,\n",
       " '##rga': 28921,\n",
       " '[unused553]': 558,\n",
       " 'creeping': 18266,\n",
       " 'nominee': 9773,\n",
       " 'bros': 10243,\n",
       " '##zawa': 19708,\n",
       " 'packaged': 21972,\n",
       " 'musicians': 5389,\n",
       " 'fresh': 4840,\n",
       " 'pull': 4139,\n",
       " 'score': 3556,\n",
       " 'kirk': 11332,\n",
       " '[unused341]': 346,\n",
       " 'euroleague': 26093,\n",
       " 'clad': 13681,\n",
       " 'annals': 16945,\n",
       " 'berries': 22681,\n",
       " '[unused316]': 321,\n",
       " '##bana': 19445,\n",
       " 'allegations': 9989,\n",
       " '##kins': 14322,\n",
       " 'अ': 1311,\n",
       " 'arguably': 15835,\n",
       " '##hai': 10932,\n",
       " 'horses': 5194,\n",
       " 'tae': 22297,\n",
       " 'vanishing': 24866,\n",
       " '―': 1518,\n",
       " 'concentration': 6693,\n",
       " 'avoidance': 24685,\n",
       " '401': 22649,\n",
       " 'packard': 24100,\n",
       " 'throughout': 2802,\n",
       " 'ella': 11713,\n",
       " 'conner': 17639,\n",
       " 'reassure': 24647,\n",
       " 'strong': 2844,\n",
       " '##行': 30471,\n",
       " 'mole': 16709,\n",
       " 'thoughtful': 16465,\n",
       " 'tuning': 17372,\n",
       " '[PAD]': 0,\n",
       " 'bowling': 9116,\n",
       " 'joe': 3533,\n",
       " '##hers': 22328,\n",
       " 'eligibility': 11395,\n",
       " 'lands': 4915,\n",
       " 'shrine': 9571,\n",
       " 'minogue': 27736,\n",
       " 'comparing': 13599,\n",
       " '##བ': 29967,\n",
       " 'ribbons': 22688,\n",
       " 'tobin': 28096,\n",
       " 'lucie': 28831,\n",
       " 'mcmahon': 17741,\n",
       " 'submit': 12040,\n",
       " 'leaves': 3727,\n",
       " 'goodbye': 9119,\n",
       " 'winced': 15574,\n",
       " 'agenda': 11376,\n",
       " 'yer': 20416,\n",
       " 'madhya': 20841,\n",
       " 'pablo': 11623,\n",
       " 'daly': 18509,\n",
       " 'ˡ': 1151,\n",
       " 'stylistic': 24828,\n",
       " 'daze': 28918,\n",
       " 'brahms': 28419,\n",
       " 'consecration': 24730,\n",
       " 'put': 2404,\n",
       " 'location': 3295,\n",
       " 'count': 4175,\n",
       " 'duval': 23929,\n",
       " 'mini': 7163,\n",
       " '##pel': 11880,\n",
       " 'computers': 7588,\n",
       " 'fights': 9590,\n",
       " 'viktor': 13489,\n",
       " 'deposits': 10042,\n",
       " '[unused919]': 924,\n",
       " 'հ': 1228,\n",
       " '##nan': 7229,\n",
       " 'erupted': 12591,\n",
       " 'theorist': 24241,\n",
       " 'sigismund': 26748,\n",
       " 'newell': 28052,\n",
       " '=': 1027,\n",
       " '##inas': 15227,\n",
       " 'astronomer': 15211,\n",
       " '##vic': 7903,\n",
       " '##ifice': 23664,\n",
       " '##lston': 21540,\n",
       " '##し': 30183,\n",
       " 'then': 2059,\n",
       " 'carlisle': 13575,\n",
       " 'mccain': 19186,\n",
       " '##di': 4305,\n",
       " 'unauthorized': 24641,\n",
       " '##ж': 29743,\n",
       " '坂': 1803,\n",
       " 'prominently': 14500,\n",
       " 'fated': 27442,\n",
       " 'ou': 15068,\n",
       " 'cords': 24551,\n",
       " 'punching': 19477,\n",
       " 'reflective': 21346,\n",
       " '##ler': 3917,\n",
       " 'pressured': 25227,\n",
       " 'brushing': 12766,\n",
       " 'tugging': 17100,\n",
       " 'snack': 19782,\n",
       " 'celtics': 23279,\n",
       " 'helped': 3271,\n",
       " 'electrification': 23679,\n",
       " 'pointless': 23100,\n",
       " 'play': 2377,\n",
       " 'bath': 7198,\n",
       " 'hellenic': 23122,\n",
       " '##ani': 7088,\n",
       " '##zo': 6844,\n",
       " 'retained': 6025,\n",
       " 'bracelet': 19688,\n",
       " 'bourbon': 15477,\n",
       " 'fixtures': 17407,\n",
       " 'adding': 5815,\n",
       " '1830': 9500,\n",
       " 'flying': 3909,\n",
       " 'character': 2839,\n",
       " 'intend': 13566,\n",
       " '1679': 27924,\n",
       " '₇': 1554,\n",
       " 'iii': 3523,\n",
       " '##ワ': 30262,\n",
       " 'principle': 6958,\n",
       " 'blackberry': 25935,\n",
       " 'parameter': 16381,\n",
       " 'falls': 4212,\n",
       " 'invention': 11028,\n",
       " '##sworth': 12255,\n",
       " 'arbitrary': 15275,\n",
       " 'bikes': 18105,\n",
       " 'ci': 25022,\n",
       " 'hope': 3246,\n",
       " 'egyptians': 23437,\n",
       " '1913': 5124,\n",
       " 'academia': 16926,\n",
       " '##jia': 26541,\n",
       " '##ad': 4215,\n",
       " 'intimacy': 20893,\n",
       " 'armoured': 11104,\n",
       " 'objection': 22224,\n",
       " 'inequality': 16440,\n",
       " 'duration': 9367,\n",
       " 'planning': 4041,\n",
       " 'superstar': 18795,\n",
       " 'proponents': 20401,\n",
       " '谷': 1951,\n",
       " 'overview': 19184,\n",
       " 'bipolar': 29398,\n",
       " 'radically': 25796,\n",
       " 'oak': 6116,\n",
       " 'vow': 19076,\n",
       " 'thanksgiving': 15060,\n",
       " 'amazement': 21606,\n",
       " 'team': 2136,\n",
       " '##ʑ': 29703,\n",
       " 'adele': 17623,\n",
       " 'stamped': 20834,\n",
       " '[unused749]': 754,\n",
       " 'housekeeper': 22583,\n",
       " 'transmitter': 11659,\n",
       " 'frequently': 4703,\n",
       " '27th': 15045,\n",
       " 'niccolo': 27033,\n",
       " 'soyuz': 29412,\n",
       " 'doping': 23799,\n",
       " '[unused455]': 460,\n",
       " 'muted': 22124,\n",
       " '）': 1988,\n",
       " '¶': 1086,\n",
       " '##cup': 15569,\n",
       " 'lied': 9828,\n",
       " 'trough': 23389,\n",
       " 'units': 3197,\n",
       " 'norwich': 12634,\n",
       " 'memories': 5758,\n",
       " 'america': 2637,\n",
       " 'bremen': 16314,\n",
       " '1922': 4798,\n",
       " 'risks': 10831,\n",
       " 'want': 2215,\n",
       " 'addressing': 12786,\n",
       " 'ʉ': 1131,\n",
       " '##は': 30198,\n",
       " '[unused345]': 350,\n",
       " '[unused614]': 619,\n",
       " 'overshadowed': 28604,\n",
       " 'reyes': 12576,\n",
       " 'lanka': 7252,\n",
       " 'discovers': 9418,\n",
       " 'favorite': 5440,\n",
       " 'raven': 10000,\n",
       " 'sounds': 4165,\n",
       " 'lip': 5423,\n",
       " 'mythological': 21637,\n",
       " '900': 7706,\n",
       " '##virus': 23350,\n",
       " '##nton': 15104,\n",
       " '##vating': 26477,\n",
       " '##ლ': 29983,\n",
       " 'folks': 12455,\n",
       " '##weight': 11179,\n",
       " '##written': 15773,\n",
       " 'battleship': 17224,\n",
       " '##xy': 18037,\n",
       " 'stephan': 15963,\n",
       " '##ever': 22507,\n",
       " 'guitarists': 28523,\n",
       " 'brest': 22451,\n",
       " 'macau': 16878,\n",
       " 'residues': 22644,\n",
       " 'affection': 12242,\n",
       " 'majority': 3484,\n",
       " 'libretto': 17621,\n",
       " 'cat': 4937,\n",
       " 'cuckoo': 29010,\n",
       " 'alley': 8975,\n",
       " 'rejects': 19164,\n",
       " '##thesis': 25078,\n",
       " 'exceptionally': 17077,\n",
       " '306': 24622,\n",
       " 'envy': 21103,\n",
       " '##grounds': 28951,\n",
       " 'frog': 10729,\n",
       " 'clinic': 9349,\n",
       " 'burgundy': 18383,\n",
       " 'greenfield': 26713,\n",
       " '##彳': 30371,\n",
       " 'holt': 12621,\n",
       " 'prospective': 17464,\n",
       " 'nr': 17212,\n",
       " 'mendes': 27916,\n",
       " 'ᅡ': 1470,\n",
       " 'shield': 6099,\n",
       " '1667': 27643,\n",
       " 'fiction': 4349,\n",
       " 'wept': 24966,\n",
       " 'dancer': 8033,\n",
       " 'nicole': 9851,\n",
       " 'highest': 3284,\n",
       " '##pped': 11469,\n",
       " 'unbelievable': 23653,\n",
       " '志': 1851,\n",
       " 'catalyst': 16771,\n",
       " '##athic': 20972,\n",
       " 'pale': 5122,\n",
       " 'reads': 9631,\n",
       " 'warning': 5432,\n",
       " 'structured': 14336,\n",
       " '##not': 17048,\n",
       " 'shiva': 12535,\n",
       " 'flashes': 16121,\n",
       " '##ace': 10732,\n",
       " 'feeding': 8521,\n",
       " 'haji': 28174,\n",
       " 'rugby': 4043,\n",
       " 'settlement': 4093,\n",
       " 'depths': 11143,\n",
       " 'cites': 17248,\n",
       " 'vamps': 24064,\n",
       " 'parry': 20803,\n",
       " 'whitish': 16800,\n",
       " 'medicare': 27615,\n",
       " 'oman': 16640,\n",
       " 'textile': 12437,\n",
       " 'mal': 15451,\n",
       " '[unused988]': 993,\n",
       " '##im': 5714,\n",
       " 'sixteen': 7032,\n",
       " 'daring': 15236,\n",
       " 'palatine': 22202,\n",
       " '国': 1799,\n",
       " 'dumont': 25830,\n",
       " 'inevitably': 21268,\n",
       " '106': 10114,\n",
       " '##pio': 22071,\n",
       " 'martian': 20795,\n",
       " 'chained': 22075,\n",
       " 'gala': 16122,\n",
       " 'fungi': 15289,\n",
       " 'conspicuous': 19194,\n",
       " '##ᅯ': 30015,\n",
       " 'award': 2400,\n",
       " 'anders': 15387,\n",
       " 'cube': 14291,\n",
       " 'unidentified': 20293,\n",
       " 'gates': 6733,\n",
       " '##yana': 16811,\n",
       " 'reg': 19723,\n",
       " 'guild': 9054,\n",
       " 'instructional': 23219,\n",
       " 'comeback': 12845,\n",
       " 'horned': 26808,\n",
       " '##জ': 29894,\n",
       " 'fruit': 5909,\n",
       " 'famously': 18172,\n",
       " 'usd': 13751,\n",
       " '##olin': 18861,\n",
       " 'mccall': 25790,\n",
       " '##path': 15069,\n",
       " 'flora': 10088,\n",
       " '##gnan': 28207,\n",
       " '##小': 30355,\n",
       " 'lahore': 15036,\n",
       " 'info': 18558,\n",
       " '251': 22582,\n",
       " '##ust': 19966,\n",
       " '##rance': 21621,\n",
       " 'slang': 21435,\n",
       " '##ک': 29841,\n",
       " '[unused173]': 178,\n",
       " 'lithuania': 9838,\n",
       " 'oclc': 12258,\n",
       " 'groundbreaking': 23222,\n",
       " 'mitchell': 6395,\n",
       " 'leonid': 27316,\n",
       " 'carlton': 12989,\n",
       " 'forum': 7057,\n",
       " 'т': 1197,\n",
       " 'permit': 9146,\n",
       " 'innovations': 15463,\n",
       " 'timely': 23259,\n",
       " '[unused138]': 143,\n",
       " 'capita': 8353,\n",
       " 'sterile': 25403,\n",
       " 'philosopher': 9667,\n",
       " 'karachi': 15381,\n",
       " 'ruiz': 18773,\n",
       " '´': 1084,\n",
       " 'onions': 24444,\n",
       " 'foil': 17910,\n",
       " 'cavendish': 23570,\n",
       " '##utation': 26117,\n",
       " 'trousers': 15292,\n",
       " '£3': 28182,\n",
       " 'charlotte': 5904,\n",
       " 'packaging': 14793,\n",
       " 'eddy': 16645,\n",
       " 'surrounds': 20626,\n",
       " 'decker': 20946,\n",
       " '[unused599]': 604,\n",
       " 'hearings': 19153,\n",
       " 'professorship': 22661,\n",
       " '850': 15678,\n",
       " '##es': 2229,\n",
       " 'metropolitan': 4956,\n",
       " 'fines': 21892,\n",
       " 'refurbishment': 24478,\n",
       " 'segregated': 24382,\n",
       " 'delaying': 29391,\n",
       " '#': 1001,\n",
       " 'poles': 10567,\n",
       " 'bray': 19743,\n",
       " '1892': 6527,\n",
       " 'holidays': 11938,\n",
       " 'bread': 7852,\n",
       " '##dha': 17516,\n",
       " '1864': 6717,\n",
       " 'motioned': 13054,\n",
       " 'indies': 9429,\n",
       " '##ht': 11039,\n",
       " 'estuary': 18056,\n",
       " '1773': 19916,\n",
       " 'visual': 5107,\n",
       " 'exams': 13869,\n",
       " 'unemployed': 18787,\n",
       " 'agree': 5993,\n",
       " 'doesn': 2987,\n",
       " 'explosive': 11355,\n",
       " 'ₕ': 1564,\n",
       " 'carrie': 13223,\n",
       " 'farmland': 16439,\n",
       " 'infancy': 22813,\n",
       " 'declined': 6430,\n",
       " '##lham': 25968,\n",
       " 'spawned': 18379,\n",
       " 'hd': 10751,\n",
       " 'helps': 7126,\n",
       " '##ש': 29812,\n",
       " 'returning': 4192,\n",
       " 'corridor': 7120,\n",
       " 'gazed': 11114,\n",
       " 'downstream': 13248,\n",
       " 'willingly': 18110,\n",
       " 'mahal': 27913,\n",
       " 'peered': 10757,\n",
       " 'definitely': 5791,\n",
       " '##kha': 15256,\n",
       " 'steven': 7112,\n",
       " 'guarantees': 21586,\n",
       " 'richmond': 6713,\n",
       " 'subgenus': 25356,\n",
       " 'outdoors': 19350,\n",
       " 'samurai': 16352,\n",
       " 'tata': 23236,\n",
       " '68': 6273,\n",
       " 'derived': 5173,\n",
       " 'princely': 22771,\n",
       " 'barclay': 23724,\n",
       " 'steamboat': 24897,\n",
       " 'molecular': 8382,\n",
       " 'tasmania': 12343,\n",
       " 'atm': 27218,\n",
       " 'suppliers': 20141,\n",
       " 'trenton': 17148,\n",
       " 'aqua': 28319,\n",
       " 'enrico': 21982,\n",
       " 'limb': 15291,\n",
       " 'notions': 21951,\n",
       " '##nail': 25464,\n",
       " 'bb': 22861,\n",
       " 'poems': 5878,\n",
       " 'thinking': 3241,\n",
       " '##ena': 8189,\n",
       " 'accommodation': 11366,\n",
       " 'tags': 22073,\n",
       " '##楊': 30409,\n",
       " 'andrea': 8657,\n",
       " 'anthem': 11971,\n",
       " 'remembers': 17749,\n",
       " 'governments': 6867,\n",
       " 'cecilia': 18459,\n",
       " 'د': 1278,\n",
       " '[unused75]': 76,\n",
       " 'geelong': 18664,\n",
       " 'wraps': 19735,\n",
       " 'devastation': 25594,\n",
       " 'lu': 11320,\n",
       " 'purchase': 5309,\n",
       " 'ghz': 29066,\n",
       " 'schooner': 21567,\n",
       " '[unused257]': 262,\n",
       " 'suicidal': 26094,\n",
       " 'diagonal': 19754,\n",
       " '##ji': 4478,\n",
       " 'bleak': 21657,\n",
       " 'engineer': 3992,\n",
       " 'predict': 16014,\n",
       " '1998': 2687,\n",
       " 'dated': 6052,\n",
       " '##patient': 24343,\n",
       " '1703': 28366,\n",
       " 'wounded': 5303,\n",
       " '##kian': 28545,\n",
       " 'tactics': 9887,\n",
       " '##李': 30403,\n",
       " 'intense': 6387,\n",
       " 'rooney': 24246,\n",
       " '##e': 2063,\n",
       " 'patio': 19404,\n",
       " 'parades': 26635,\n",
       " 'multiplayer': 17762,\n",
       " 'cinematographer': 19245,\n",
       " '##rup': 21531,\n",
       " '##এ': 29887,\n",
       " 'sony': 8412,\n",
       " '##‘': 30055,\n",
       " '310': 17196,\n",
       " 'we': 2057,\n",
       " '##fera': 27709,\n",
       " 'slate': 12796,\n",
       " 'quebec': 5447,\n",
       " 'idol': 10282,\n",
       " 'horticultural': 26235,\n",
       " 'indira': 28232,\n",
       " '##田': 30437,\n",
       " 'cloak': 11965,\n",
       " 'luther': 9678,\n",
       " '##mind': 23356,\n",
       " '##bbled': 12820,\n",
       " '1807': 13206,\n",
       " 'wilder': 18463,\n",
       " 'anna': 4698,\n",
       " 'arms': 2608,\n",
       " '##edd': 22367,\n",
       " 'demonstrations': 13616,\n",
       " 'doubts': 13579,\n",
       " 'unconscious': 9787,\n",
       " 'arises': 18653,\n",
       " '##iac': 20469,\n",
       " '##uj': 23049,\n",
       " 'although': 2348,\n",
       " 'pup': 26781,\n",
       " '##க': 29918,\n",
       " '##gm': 21693,\n",
       " 'boating': 23639,\n",
       " 'breeds': 15910,\n",
       " 'everett': 15160,\n",
       " 'inclusion': 10502,\n",
       " '##uses': 25581,\n",
       " 'pontifical': 22362,\n",
       " '##hood': 9021,\n",
       " 'seven': 2698,\n",
       " 'perch': 21836,\n",
       " 'garland': 17017,\n",
       " 'region': 2555,\n",
       " 'delivers': 18058,\n",
       " 'preaching': 17979,\n",
       " 'khalifa': 27925,\n",
       " 'antoinette': 28429,\n",
       " '##gpur': 25758,\n",
       " 'gathering': 7215,\n",
       " '##bered': 22408,\n",
       " '##‖': 30054,\n",
       " '[unused60]': 61,\n",
       " 'restaurants': 7884,\n",
       " 'conservation': 5680,\n",
       " 'transforming': 17903,\n",
       " 'someplace': 24956,\n",
       " 'picks': 11214,\n",
       " 'syrian': 9042,\n",
       " 'soils': 13622,\n",
       " 'dying': 5996,\n",
       " '⊕': 1612,\n",
       " 'velocity': 10146,\n",
       " '##lands': 8653,\n",
       " '##rling': 22036,\n",
       " 'danes': 27476,\n",
       " '##erie': 17378,\n",
       " 'landscapes': 12793,\n",
       " '1786': 17436,\n",
       " 'deposition': 19806,\n",
       " 'melville': 20154,\n",
       " 'thud': 20605,\n",
       " 'favors': 21191,\n",
       " 'arriving': 7194,\n",
       " 'abduction': 23415,\n",
       " 'shouted': 6626,\n",
       " '##ticus': 29587,\n",
       " 'wheelbase': 29141,\n",
       " '##hal': 8865,\n",
       " '##ィ': 30220,\n",
       " 'proof': 6947,\n",
       " 'huff': 21301,\n",
       " 'silvery': 21666,\n",
       " 'dukes': 16606,\n",
       " 'retrieval': 26384,\n",
       " 'attractive': 8702,\n",
       " '##য': 29907,\n",
       " 'н': 1192,\n",
       " 'receptions': 16466,\n",
       " 'lucille': 28016,\n",
       " '##national': 25434,\n",
       " 'hana': 26048,\n",
       " 'unrelated': 15142,\n",
       " '##zzle': 17644,\n",
       " '##lc': 15472,\n",
       " 'flipping': 18497,\n",
       " 'max': 4098,\n",
       " 'conquest': 9187,\n",
       " 'olsen': 18997,\n",
       " 'historians': 7862,\n",
       " 'shelly': 28360,\n",
       " '##ف': 29833,\n",
       " 'primal': 22289,\n",
       " 'opposition': 4559,\n",
       " 'cypriot': 18543,\n",
       " 'offspring': 13195,\n",
       " 'resigning': 24642,\n",
       " 'grow': 4982,\n",
       " 'mocking': 19545,\n",
       " '[unused592]': 597,\n",
       " 'alexia': 21683,\n",
       " 'habitat': 6552,\n",
       " 'п': 1194,\n",
       " 'primitive': 10968,\n",
       " 'susanna': 26681,\n",
       " 'halted': 12705,\n",
       " 'shivered': 13927,\n",
       " 'explosions': 18217,\n",
       " 'somme': 25158,\n",
       " 'ar': 12098,\n",
       " '[unused752]': 757,\n",
       " '##et': 3388,\n",
       " 'changing': 5278,\n",
       " '##さ': 30182,\n",
       " 'dismiss': 19776,\n",
       " 'lowered': 6668,\n",
       " 'harp': 14601,\n",
       " 'worldwide': 4969,\n",
       " '1950': 3925,\n",
       " 'tweed': 26922,\n",
       " 'regard': 7634,\n",
       " 'sandals': 24617,\n",
       " '##cule': 21225,\n",
       " 'dialects': 11976,\n",
       " '##asco': 28187,\n",
       " '##モ': 30253,\n",
       " 'concession': 16427,\n",
       " 'eugene': 8207,\n",
       " 'mayfield': 27224,\n",
       " 'opponents': 7892,\n",
       " '1733': 27230,\n",
       " 'january': 2254,\n",
       " 'militants': 17671,\n",
       " 'advisers': 24205,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) vocabulary\n",
    "# ## represents a substr\n",
    "\n",
    "tokenizer.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size \n",
    "\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2057, 2064, 3828, 2151, 19888, 10085, 3126, 7389, 5666, 2057, 2215, 1012]\n"
     ]
    }
   ],
   "source": [
    "# 6) converting\n",
    "###############\n",
    "\n",
    "## token -> ids\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'can', 'save', 'any', 'crypt', '##oc', '##ur', '##ren', '##cy', 'we', 'want', '.']\n"
     ]
    }
   ],
   "source": [
    "## ids -> tokens\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can save any cryptocurrency we want.\n"
     ]
    }
   ],
   "source": [
    "## tokens -> text\n",
    "\n",
    "text = tokenizer.convert_tokens_to_string(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2057, 2064, 3828, 2151, 19888, 10085, 3126, 7389, 5666, 2057, 2215, 1012]\n"
     ]
    }
   ],
   "source": [
    "## text -> ids\n",
    "\n",
    "# using encode can convert the text directly to ids:\n",
    "\n",
    "# ids = tokenizer.encode(text)\n",
    "\n",
    "# but it will by default add 2 special tokens at the start (CLS) and the end (SEP) of the text\n",
    "\n",
    "# and we will get result:\n",
    "\n",
    "# [101, 2057, 2064, 3828, 2009, 2151, 2860, 8167, 2063, 2057, 2215, 1012, 102]\n",
    "\n",
    "# If we don't want to add those special tokens, we use a parameter:\n",
    "\n",
    "ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 22:49:33.386959: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-18 22:49:33.387013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-18 22:49:33.389244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-18 22:49:33.402125: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-18 22:49:35.479987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can save any cryptocurrency we want.\n"
     ]
    }
   ],
   "source": [
    "## ids -> text\n",
    "\n",
    "# using decode can convert the ids directly to text\n",
    "#\n",
    "# text = tokenizer.decode(ids)\n",
    "#\n",
    "# now we can see the special tokens CLS and SEP:\n",
    "# '[CLS] we can save it anywhare we want. [SEP]'\n",
    "\n",
    "# If we don't want those special tokens, we use a parameter:\n",
    "\n",
    "text = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2057, 2064, 3828, 2151, 19888, 10085, 3126, 7389, 5666, 2057, 2215, 1012, 102, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 7) padding / truncate\n",
    "#######################\n",
    "\n",
    "## padding\n",
    "\n",
    "ids = tokenizer.encode(text, padding=\"max_length\", max_length=20)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2057, 2064, 3828, 102]\n"
     ]
    }
   ],
   "source": [
    "## truncate\n",
    "\n",
    "# the max_lenght will include the added special tokens\n",
    "\n",
    "ids = tokenizer.encode(text, max_length=5, truncation=True)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2064, 3828, 2151, 19888, 10085, 3126, 7389, 5666, 2057, 2215, 1012, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 8) outputs\n",
    "############\n",
    "\n",
    "## single text data \n",
    "\n",
    "# it outputs a dict of 3 fields:\n",
    "# - input_ids\n",
    "# - token_type_ids\n",
    "# - attention_mask\n",
    "\n",
    "# Depending on the text input, arguments, the output element can change\n",
    "# sometimes, we have to create this structure ourselves \n",
    "# some other times, we have to complete this sctruture by adding some fields\n",
    "\n",
    "toks = tokenizer.encode_plus(text, padding=\"max_length\", max_length=20)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2057, 2064, 3828, 2151, 19888, 10085, 3126, 7389, 5666, 2057, 2215, 1012, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# or simply we can just call the encode directly\n",
    "\n",
    "toks = tokenizer(text, padding=\"max_length\", max_length=20)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 2009, 27852, 1037, 4487, 6593, 1997, 1017, 3787, 1012, 102], [101, 1045, 2288, 1996, 2168, 3277, 1012, 102], [101, 1999, 2026, 2553, 1010, 2009, 2499, 2986, 2077, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "## batched data\n",
    "\n",
    "# The tokenizer can take a list of texts and outputs a list for each dict field\n",
    "\n",
    "texts = [\"it outputs a dict of 3 elements.\",\n",
    "        \"I got the same issue.\",\n",
    "        \"In my case, it worked fine before.\"\n",
    "        ]\n",
    "\n",
    "toks = tokenizer(texts)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## single data vs batched data\n",
    "\n",
    "# batched data is faster to tokenizer\n",
    "# so we should tokenize the data by batch whenever possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 302 ms, sys: 3.78 ms, total: 306 ms\n",
      "Wall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(2000):\n",
    "    tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 444 ms, sys: 221 ms, total: 664 ms\n",
      "Wall time: 86.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "toks = tokenizer([text] * 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Fast vs. Slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by default, the fast tokenizer is loaded\n",
    "\n",
    "# The loaded tokenizer is called \"BertTokenizerFast\", and the argument \"is_fast\" is true.\n",
    "\n",
    "# fast tokenizer is based on RUST implementation, which is supposed to be faster than the slow version which is \n",
    "# based on python\n",
    "\n",
    "tokenizer_fast = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "tokenizer_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load the slow tokenizer, we should use the argument \"use_fast\" set to false.\n",
    "\n",
    "# The loaded tokenizer is called \"BertTokenizer\", and the argument \"is_fast\" is false.\n",
    "\n",
    "tokenizer_slow= AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", use_fast=False)\n",
    "tokenizer_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 293 ms, sys: 6.53 ms, total: 299 ms\n",
      "Wall time: 293 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time the fast tokenizer on single data\n",
    "for i in range(2000):\n",
    "    tokenizer_fast(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 4.21 ms, total: 1.4 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time the slow tokenizer on single data\n",
    "for i in range(2000):\n",
    "    tokenizer_slow(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 548 ms, sys: 652 ms, total: 1.2 s\n",
      "Wall time: 232 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time the fast tokenizer on batched data\n",
    "toks = tokenizer_fast([text] * 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 3.49 ms, total: 1.27 s\n",
      "Wall time: 1.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# time the slow tokenizer on batched data\n",
    "toks = tokenizer_slow([text] * 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in summary (approximate time in ms for 2000 data):\n",
    "\n",
    "|   \t| single (ms) | batched (ms)|\n",
    "|---\t|---\t |---\t   |\n",
    "| fast  |  286 \t |   83.8  |\n",
    "| slow\t|  1240  |   1120  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Some Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### offset mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2057, 2064, 3828, 2151, 19888, 10085, 3126, 7389, 5666, 2057, 2215, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 6), (7, 11), (12, 15), (16, 21), (21, 23), (23, 25), (25, 28), (28, 30), (31, 33), (34, 38), (38, 39), (0, 0)]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Only fast tokenizer can return this extra information\n",
    "\n",
    "# by setting the argument \"return_offsets_mapping\" to true, we will get an extra element in the output\n",
    "# called \"offset_mapping\".\n",
    "\n",
    "# \"offset_mapping\" corresponds a list of tuples with the first value is the start position of the corresponding token \n",
    "# and the second the end.\n",
    "\n",
    "# Here the position is the position of the letter the text.\n",
    "\n",
    "toks = tokenizer_fast(text, return_offsets_mapping=True)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 6, 7, None]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we look at the word_ids we get some repeated indices,\n",
    "# this is because the tokenization of the text is not solely based on word.\n",
    "\n",
    "# The tokenized text is ['we', 'can', 'save', 'any', 'crypt', '##oc', '##ur', '##ren', '##cy', 'we', 'want', '.']\n",
    "# obtained before using tokenizer.tokenize(text)\n",
    "\n",
    "# The word \"cryptocurrency\" is the 4th word in the text.\n",
    "# It is tokenized into 4 tokens, which is why the number 4 was repeated 4 times in the word_ids.\n",
    "\n",
    "# Letter:  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 0\n",
    "# Word:      W e   c a n   s a v  e     a  n  y     c  r  y  p  t  o  c  u  r  r  e  n  c  y     w  e     w  a  n  t  .\n",
    "# token:     we     can     save          any          crypt       ##oc  ##ur  ##ren    ##cy      we         want     .\n",
    "# word_ids:  0       1        2            3            4           4     4      4        4        5          6       7          0         \n",
    "# map: (0,0)(0,2)  (3,6)  (7,11)       (12,15)      (16,21)       (21,23)(23,25)(25,28)(28,30) (31, 33)  (34,38)  (38,39)(0,0)   \n",
    "\n",
    "toks.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Loading internlm/internlm-chat-7b requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For some exotic tokenizer, we have to add the argument trust_remote_code=True\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# otherwise, it will report error as below.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minternlm/internlm-chat-7b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:852\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m tokenizer_auto_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    845\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m TOKENIZER_MAPPING \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    846\u001b[0m     config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m     )\n\u001b[1;32m    851\u001b[0m )\n\u001b[0;32m--> 852\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_fast \u001b[38;5;129;01mand\u001b[39;00m tokenizer_auto_map[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/dynamic_module_utils.py:639\u001b[0m, in \u001b[0;36mresolve_trust_remote_code\u001b[0;34m(trust_remote_code, model_name, has_local_code, has_remote_code)\u001b[0m\n\u001b[1;32m    636\u001b[0m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires you to execute the configuration file in that\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[0;31mValueError\u001b[0m: Loading internlm/internlm-chat-7b requires you to execute the configuration file in that repo on your local machine. Make sure you have read the code there to avoid malicious use, then set the option `trust_remote_code=True` to remove this error."
     ]
    }
   ],
   "source": [
    "# For some exotic tokenizer, we have to add the argument trust_remote_code=True\n",
    "# otherwise, it will report error as below.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm-chat-7b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
