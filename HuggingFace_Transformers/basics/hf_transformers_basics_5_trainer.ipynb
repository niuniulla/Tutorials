{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, it will:\n",
    "\n",
    "    I. Present Trainer tools\n",
    "    II. How to use\n",
    "    III. Apply to training\n",
    "    IV. display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is a API to help train HF models (not limited but with restrictions*) on several types of devices.\n",
    "It incompasses all training components such as training loop, evaluation, logging to facilitate training.\n",
    "It also support backends such as DeepSpeed, Distributed training etc.\n",
    "\n",
    "* There are some requirements to use Trainer:\n",
    "  - outputs format\n",
    "  - if there are labels, the model should provide a loss value\n",
    "\n",
    "The doc of Trainer is: https://huggingface.co/docs/transformers/main_classes/trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Update Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the Training section from 'hf_transformers_basics_evaluate.ipynb\" and replace the manually defined training loop function by the Trainer of transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this.\n",
    "# Should be run the first\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "    num_rows: 3548\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) load dataset\n",
    "#################\n",
    "\n",
    "# not changed\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ckp_data = \"davidberg/sentiment-reviews\"\n",
    "\n",
    "data = load_dataset(ckp_data, split=\"train\")\n",
    "\n",
    "data = data.filter(lambda column: \"neutral\" not in column[\"division\"]) # filter out neutral\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 3193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) split data\n",
    "###############\n",
    "\n",
    "# not changed\n",
    "\n",
    "split_data = data.train_test_split(test_size=0.1)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111a382266ef447e8add4c43602d1837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da138be5ad343a582dc0656d5f409d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) tokenizer\n",
    "##############\n",
    "\n",
    "# not changed\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "label2id = {\"negative\":0, \"positive\": 1}\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def process(batch):\n",
    "\n",
    "    toks = tokenizer(batch[\"review\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    toks[\"labels\"] = torch.tensor([label2id.get(item) for item in batch[\"division\"]])\n",
    "\n",
    "    return toks\n",
    "\n",
    "tokenized_data = split_data.map(process, batched=True, remove_columns=data.column_names)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) dataloader\n",
    "###############\n",
    "\n",
    "# removed, will be handled by Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5) load model\n",
    "###############\n",
    "\n",
    "# we don't need to send model to gpu here\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) define optimizer\n",
    "#####################\n",
    "\n",
    "# removed, this will be handled by Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 21:15:43.825240: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 21:15:43.825308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 21:15:43.827583: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-19 21:15:43.841032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 21:15:45.966705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 7) evaluation\n",
    "###############\n",
    "\n",
    "# We don't need to do the loop ourselves\n",
    "\n",
    "import evaluate\n",
    "\n",
    "acc_fct = evaluate.load(\"accuracy\")\n",
    "f1_fct = evaluate.load(\"f1\")\n",
    "\n",
    "def eval_metrics(outputs):\n",
    "\n",
    "    preds, refs = outputs\n",
    "    # if using torch.argmax: preds = torch.argmax(preds, dim=-1)\n",
    "    # if using tensor.argmax: preds = preds.argmax(axis=-1)\n",
    "    preds = preds.argmax(axis=-1)\n",
    "    acc = acc_fct.compute(predictions=preds, references=refs)\n",
    "    f1 = f1_fct.compute(predictions=preds, references=refs)\n",
    "    \n",
    "    acc.update(f1) # combine the metrics together\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Train\n",
    "##########\n",
    "\n",
    "# Change completely\n",
    "#\n",
    "# There are 2 parts for training:\n",
    "#   * define train arguments\n",
    "#   * define trainer\n",
    "\n",
    "## training argument\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./tmp/checkpoints\",\n",
    "    per_device_train_batch_size = 64,\n",
    "    per_device_eval_batch_size = 64,\n",
    "    logging_steps=50,                       # to report training loss\n",
    "    eval_strategy = \"epoch\",                # do evaluation every epoch\n",
    "    save_strategy = \"epoch\",                # save every epoch to checkpoints\n",
    "    save_total_limit = 3,\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    metric_for_best_model = \"f1\",\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainer\n",
    "\n",
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = tokenized_data[\"train\"],\n",
    "    eval_dataset = tokenized_data[\"test\"],\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics = eval_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 01:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.211154</td>\n",
       "      <td>0.921127</td>\n",
       "      <td>0.951724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>0.213811</td>\n",
       "      <td>0.932394</td>\n",
       "      <td>0.959732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>0.175402</td>\n",
       "      <td>0.940845</td>\n",
       "      <td>0.964225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.19963932355244954, metrics={'train_runtime': 72.6406, 'train_samples_per_second': 131.868, 'train_steps_per_second': 2.065, 'total_flos': 630085199823360.0, 'train_loss': 0.19963932355244954, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.17540201544761658,\n",
       " 'eval_accuracy': 0.9408450704225352,\n",
       " 'eval_f1': 0.9642248722316865,\n",
       " 'eval_runtime': 2.7628,\n",
       " 'eval_samples_per_second': 128.491,\n",
       " 'eval_steps_per_second': 2.172,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tensorboard:\n",
    "\n",
    " 1) open a terminal\n",
    " 2) activate the virtual environment\n",
    " 3) enter: tensorboard --logdir the/dir/to/the/runs\n",
    " 4) copy the local address and open it in a browser\n",
    "\n",
    "Or for VS code user, we can launch the tensorboard using the pallete (ctrl + shift + p) and enter \"tensorboard\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
