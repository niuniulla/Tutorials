{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, it will:\n",
    "\n",
    "    I. Summarize the transformers so far\n",
    "    II. Memory Usage\n",
    "    III. Optimization of memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The modules presented so far\n",
    "\n",
    " - **datasets**: download, construct, process data, can be online or offline dataset.\n",
    "\n",
    " - **tokenizer**: tokenize dataset. It produce a dict structure containing \"input_ids\", \"attention_mask\", \"labels\" and\n",
    "    maybe, other field depending on the problem.\n",
    "\n",
    " - **model**: to construct, download, modify models\n",
    "\n",
    " - **evaluate**: define, load metrics\n",
    "\n",
    " - **trainer**: put things together for training with ways to optimize training\n",
    " \n",
    " - **pipeline**: construct process to take input and output results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train steps\n",
    "\n",
    "So far, we can summarize the training and usage into 10 steps:\n",
    "\n",
    " 1) import modules: hugging face provides transformers and datasets for their own resources\n",
    " 2) load datasets: we can use datasets from hugging face or any other datasets. Depending\n",
    " on the data format and application, we should clean, structure, split or process it.\n",
    " 3) split dataset\n",
    " 4) tokenization: process + tokenize datasets\n",
    " 5) load model\n",
    " 6) construct metrics\n",
    " 7) training arguments: epochs, devices, loggings...\n",
    " 8) construct trainer: designate model, tokenizer, datasets, metrics, datacollator...\n",
    " 9) train + evaluate\n",
    " 10) inference: we can use pipeline from hugging face, but sometimes we should write our own pipeline.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. memory usage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run any inference or training, we should make sure that we have enough memory since the model can easily pass the gpu memory limit and render the run really slow or crash the machine.\n",
    "\n",
    "There is an article to exaplain the memory usage by running the model: https://arxiv.org/pdf/1910.02054v3.\n",
    "\n",
    "hugging face provide a tool to show those infos: https://huggingface.co/spaces/hf-accelerate/model-memory-usage.\n",
    "\n",
    "Besides, there are also other overhead memories depending on the batch size, input/output size, and some hidden states or other factors.\n",
    "\n",
    "But, PyTorch doesnâ€™t report the memory usage (torch.cuda.memory_allocated() could return a 0 allocation).\n",
    "\n",
    "On runtime, we could use nvidia-smi (or any other reporting tool such as \"nvtop\") to check the overall GPU memory usage.\n",
    "\n",
    "\n",
    "Below, we present a simple example to show the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take a bert model as example\n",
    "\n",
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained(\"google-bert/bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameter numbers:  335 MB\n",
      "model:  1340 MB\n",
      "gradients:  1340 MB\n",
      "optimizer:  4.02 GB\n",
      "total:  6.7 GB\n"
     ]
    }
   ],
   "source": [
    "# count the total number of the parameters in the model\n",
    "\n",
    "params = sum(param.numel() for param in model.parameters())\n",
    "\n",
    "print(\"model parameter numbers: \", round(params/1e6), \"MB\")\n",
    "\n",
    "# for adam training normal we should consider memories for (by using f32):\n",
    "#  - copy of the model parameter\n",
    "#    model parameters x 4 (bytes)\n",
    "\n",
    "print(\"model: \", round(params/1e6) * 4, \"MB\")\n",
    "\n",
    "#  - copy of the gradients\n",
    "#    model parameters x 4 (bytes)\n",
    "\n",
    "print(\"gradients: \", round(params/1e6) * 4, \"MB\")\n",
    "\n",
    "#  - optimizer states (copy of parameters, momentum and variance)\n",
    "#    model parameters x 12 (bytes)\n",
    "\n",
    "print(\"optimizer: \", round(params/1e9 * 12, 2), \"GB\")\n",
    "\n",
    "# so in total we need for model: \n",
    "print(\"total: \", round(params/1e9 * (4 + 4 + 12), 2), \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Memory Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the models becomes larger and larger, the memory demande for training increases as well.\n",
    "\n",
    "We would like to illustrate how to optimize certain parameters to decrease the memory usage to run large models and their effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To produce the below results, run the below cell with corresponding parameters changes. Restart the kernel before each run.\n",
    "\n",
    " - we use model : google-bert/bert-large-uncased, \n",
    "   * 336M\n",
    " - NVIDIA GeForce RTX 3090 \n",
    "   * CUDA Version: 12.3 \n",
    "   * 24GB\n",
    "\n",
    "| num |  Optimization \t                    |   Part\t    | Memory(GB) | time(s) | comment |\n",
    "|--- |---\t                                |---\t          |---\t       |---\t     |---\t     |\n",
    "| 0 |Bbaseline (BS=32, Length=128)\t      |   \t          |8.8   \t     |90   \t   |            |            \n",
    "| 1 |Gradient Accumulation (BS=1, GA=32) \t|Forward   \t    |5.7   \t     |747      |gradient_accumulation_steps=32|\n",
    "| 2 |Gradient Checkpoints (BS=1, GA=32)   |Forward   \t    |5.4   \t     |947      |gradient_checkpointing=True|\n",
    "| 3 |Adafactor Optimizer (BS=1, GA=32)   \t|Optimizer  \t  |3.0   \t     |907   \t |optim=\"adafactor\"|\n",
    "| 4 |Freeze Model (BS=1, GA=32)   \t      |Foward+Gradient|1.6   \t     |370   \t |set model params requires_grad to false|\n",
    "| 5 |Data Length (BS=1, GA=32, length=32) |Forward   \t    |1.6   \t     |360   \t |decrease max_length in tokenizer|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Other methods can also be used to be able to run large model on limited resources:\n",
    " - Reduce Batch Size: The first methode to be considered\n",
    " - Use a Simpler Model: Smaller architectures require less memory. Depending on the problem we may not always need larger models.\n",
    " - Distributed Training: on multiple GPUs.\n",
    " - Use Mixed Precision Training: use f8, f16 for training (explored later.)\n",
    " - Release unused tensors.\n",
    " - other finetuning techniques (explored later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 17:35:11.881082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-07 17:35:11.881146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-07 17:35:11.884144: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-07 17:35:11.897349: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-07 17:35:14.019421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5241e63aa446399bc6db3280b68685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79527dafaa54f20894539676daa51ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034cf149d9b04741bfea30a296265b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# code to preduce the table #\n",
    "#############################\n",
    "\n",
    "# The above table was obtained by changing the code according to the num of the comment.\n",
    "# The number of momory and time may vary a bit depending on the machine and the state of the machine.\n",
    "\n",
    "\n",
    "# This cell should be run first before any other transformers calls\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "\n",
    "\n",
    "# example used to get the above table results\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "ckp = \"google-bert/bert-large-uncased\"\n",
    "\n",
    "# load data\n",
    "data = load_dataset(\"sepidmnorozy/English_sentiment\")\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "\n",
    "# process dataset\n",
    "def process(samples):\n",
    "\n",
    "    toks = tokenizer(samples[\"text\"], truncation=True, max_length=128, padding=True) # (5) change max_length=32\n",
    "    toks[\"labels\"] = samples[\"label\"]\n",
    "\n",
    "    return toks\n",
    "\n",
    "tokenized_data = data.map(process, batched=True, remove_columns=data[\"train\"].column_names)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckp)\n",
    "\n",
    "# evaluate\n",
    "acc_fct = evaluate.load(\"accuracy\")\n",
    "f1_fct = evaluate.load(\"f1\")\n",
    "\n",
    "def metric(pred):\n",
    "\n",
    "    preds, refs = pred\n",
    "    preds = preds.argmax(axis=-1)\n",
    "\n",
    "    accuracy = acc_fct.compute(predictions=preds, references=refs)\n",
    "    f1 = f1_fct.compute(predictions=preds, references=refs)\n",
    "\n",
    "    accuracy.update(f1)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# train args\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoint\",\n",
    "    per_device_train_batch_size=32, # (1-5) change the batch size to 1\n",
    "    per_device_eval_batch_size=1,\n",
    "    # gradient_accumulation_steps=32, # (1) uncomment to use gradient accumulation\n",
    "    # gradient_checkpointing=True, # (2) uncomment to use gradient checkpoint\n",
    "    # optim=\"adafactor\", # (3) uncomment to use adafactor optimization\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# The model is composed of a BertModel (instanced as self.bert) and a linear output layer\n",
    "# the idea is to freeze the BertModel parameters and only update the output layer parameters\n",
    "\n",
    "# (4) uncomment below to freeze bert model parameters\n",
    "# for _, param in model.bert.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=metric\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
