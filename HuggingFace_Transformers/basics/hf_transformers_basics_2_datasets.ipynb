{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, it will:\n",
    "\n",
    "    I. present the basic usgae of datasets module\n",
    "    II. use the module in the process\n",
    "    III. Compare the torch and datasets ways\n",
    "    IV. Modify the training process from Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets is a dataset manipulation module developed by hugging face. The public database of HF is https://huggingface.co/datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "########\n",
    "\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. OnLine Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 4084\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "######\n",
    "\n",
    "# To load the data, you have to pass the online location of the dataset which \n",
    "# is composed of the name of the repository and the dataset name. \n",
    "#\n",
    "# There is no use to provide the url.\n",
    "#\n",
    "# By default, data is a DatasetDict.\n",
    "#\n",
    "# Depending ont he data structure to be downloaded, the content of this dict \n",
    "# will change.\n",
    "#\n",
    "# The following example has only on element in the dict since there is only one\n",
    "# set is available.\n",
    "\n",
    "# Otherwise, the dict may contain several elements such as train, valid, test. \n",
    "\n",
    "\n",
    "data =  load_dataset(\"davidberg/sentiment-reviews\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "    num_rows: 4084\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split\n",
    "#######\n",
    "\n",
    "# if we are only interested in one part of the dataset (eg. train), one can use\n",
    "# the 'split' argument.\n",
    "# And the data becomes Dataset but not a dict anymore.\n",
    "\n",
    "data =  load_dataset(\"davidberg/sentiment-reviews\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slice\n",
    "#######\n",
    "\n",
    "# One can also slicing the dataset to be loaded\n",
    "# the Slicing can be done by providing :\n",
    "#  - the start and the end indices.\n",
    "#  - the percentage of the data size\n",
    "\n",
    "data =  load_dataset(\"davidberg/sentiment-reviews\", split=\"train[:50]\")\n",
    "# data =  load_dataset(\"yezhengli9/opus_books_demo\", split=\"train[10:100]\")\n",
    "# data =  load_dataset(\"yezhengli9/opus_books_demo\", split=\"train[:10%]\")\n",
    "# data =  load_dataset(\"yezhengli9/opus_books_demo\", split=[\"train[:10%]\", \"train[90%:]\"]) # return a list\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subset\n",
    "########\n",
    "\n",
    "# Sometimes, there are several subsets in a dataset.\n",
    "# We can download a subset by providing the name of this subset\n",
    "# the following example has a subset named 'en-fr'\n",
    "\n",
    "data_sub = load_dataset(\"yezhengli9/opus_books_demo\", \"en-fr\")\n",
    "data_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Inspect dataset\n",
    "\n",
    "The inspection is to see and check data.\n",
    "The returned results are lists.\n",
    "So they can't be used directly in subsequent operations such as dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'},\n",
       " {'en': 'Alain-Fournier', 'fr': 'Alain-Fournier'},\n",
       " {'en': 'First Part', 'fr': 'PREMIÃˆRE PARTIE'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by Slicing\n",
    "############\n",
    "\n",
    "# if the data is a dict, by specifying the key (eg. train), we can get its content\n",
    "# otherwise, the data can be accessible directly.\n",
    "\n",
    "data_sub[\"train\"][:3][\"translation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colums names:  ['Unnamed: 0', 'review', 'polarity', 'division']\n",
      "features:  {'Unnamed: 0': Value(dtype='int64', id=None), 'review': Value(dtype='string', id=None), 'polarity': Value(dtype='float64', id=None), 'division': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "##########\n",
    "\n",
    "# we can get other attributes \n",
    "\n",
    "# all collumns\n",
    "print(\"colums names: \", data.column_names)\n",
    "print(\"features: \", data.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 8\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spliting data into different groups (eg. train, valid)\n",
    "# this returns a DatasetDict\n",
    "\n",
    "data.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Filter & Manipulation\n",
    "\n",
    "Those functions are to process the data for later usage, which is different from the inspection in the above section.\n",
    "The results of those functions are still dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select\n",
    "########\n",
    "\n",
    "# select the data, this return a Dataset, which is different from previous section\n",
    "# for inspection, which return a list or dict\n",
    "\n",
    "data.select([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d89d67becbf64a08b33d2c884d79478e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "########\n",
    "\n",
    "# For more complex filter, one can use filter method with a filter function composing of filter criteria\n",
    "#this returns a Dataset\n",
    "# The opration is not in place\n",
    "\n",
    "data_neu = data.filter(lambda column : \"neutral\" in column[\"division\"])\n",
    "data_neu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prefix: able play youtube alexa',\n",
       " 'prefix: able recognize indian accent really well drop function helpful call device talk person near device smart plug schedule work seamlessly con would sound kindloud but lack clarity mid frequency need tweeked optimum clarity rarely device doesnt respond call alexa',\n",
       " 'prefix: absolute smart device amazon connect external sub woofer sound amaze recons voice even close room like almost collection songs english hindi must quite moneys worth',\n",
       " 'prefix: absolutely amaze new member family control home voice connect home anywhere world',\n",
       " 'prefix: absolutely amaze previously sceptical invest money but arrive worth ityou absolutely buy wont regret cheer']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "#####\n",
    "\n",
    "# To apply processings to data, one can use map method.\n",
    "\n",
    "def add_prefix(sample) :\n",
    "\n",
    "    sample[\"review\"] = \"prefix: \" + sample[\"review\"]\n",
    "    return sample\n",
    "\n",
    "data_prefix = data.map(add_prefix)\n",
    "\n",
    "data_prefix[\"review\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Combined with Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more complex example using tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to to the preprocessing\n",
    "# https://huggingface.co/docs/transformers/tasks/translation\n",
    "\n",
    "# Prefix the input with a prompt so T5 knows this is a translation task. \n",
    "# Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "def preprocess(example) :\n",
    "\n",
    "    inputs = prefix + example[\"translation\"][source_lang]\n",
    "    targets = example[\"translation\"][target_lang]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471546f7a1614506ac2a8acc9ce6d4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply preprocess function to data\n",
    "\n",
    "preprocessed_data = data_sub.map(preprocess)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c341f4a8294d9c8fea58bad1c635ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use multple processes\n",
    "\n",
    "# To accelerate the process, we can specify the number of process to be used.\n",
    "# the more the process, the faster the process\n",
    "\n",
    "preprocessed_data = data_sub.map(preprocess, num_proc=4)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b9d22f330f466f8f5c8ef68222d920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## batched processing \n",
    "\n",
    "# if we use the function above for batched process, we will get an error of indice.\n",
    "# The error is caused by the fact that the function above doesn't support batched data,\n",
    "# eg: without batch, each element is data[i][\"translation\"]['en'], which returns the \n",
    "# ieme element in english.\n",
    "# however, data[i:i+batch_size][\"translation\"]['en'] triggers error.\n",
    "\n",
    "def preprocess_batched(examples):\n",
    "    \n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "preprocessed_data = data_sub.map(preprocess_batched, batched=True)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## batch + proc\n",
    "\n",
    "preprocessed_data = data_sub.map(preprocess_batched, batched=True, num_proc=4)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0da96881c3e4c2c983c34446a3b4de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove columns\n",
    "\n",
    "# we can remove the information we don't want\n",
    "# here we see that colums names:  ['id', 'translation'] are removed\n",
    "\n",
    "preprocessed_data = data_sub.map(preprocess_batched, batched=True, num_proc=4, remove_columns=data_sub[\"train\"].column_names)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b182dc56c05b43d2adbbdf194a14020b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## save to disc\n",
    "\n",
    "# after save, there are:\n",
    "#   - a folder called \"train\" with:\n",
    "#       * data-00000-of-00001.arrow\n",
    "#       * dataset_info.json\n",
    "#       * state.json\n",
    "#   - a file called \"dataset_dict.json\"\n",
    "\n",
    "preprocessed_data.save_to_disk(\"./tmp/processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load from disc\n",
    "\n",
    "preprocessed_data = load_from_disk(\"./tmp/processed_data\")\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other format\n",
    "##############\n",
    "\n",
    "# we can load one or several files\n",
    "# If there are several files, put all file names into a list and pass the list to data_files arg.\n",
    "\n",
    "data = load_dataset(\"csv\", data_files=\"./mydata.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is dedicated function to load csv file\n",
    "\n",
    "data = Dataset.from_csv(\"./mydata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all files in a folder\n",
    "\n",
    "data = load_dataset(\"csv\", data_dir=\"./myfilespath\", split=\"train\")\n",
    "\n",
    "# or\n",
    "# data = load_dataset(\"csv\", data_files=[\"./myfilespath/mydata1.csv\", \"./myfilespath/mydata2.csv\"], split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from panda data\n",
    "\n",
    "# import pandas as pd\n",
    "# data = pd.read_csv(\"./mydata.csv\")\n",
    "\n",
    "data = Dataset.from_pandas(data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'sentence1'}, {'text': 'sentence2'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the data is a list, we can't load them directly as dataset\n",
    "# if so, we get an error: AttributeError: 'str' object has no attribute 'get'\n",
    "\n",
    "data_list = [\"sentence1\", \"sentence2\"]\n",
    "\n",
    "# we should decorate the list as\n",
    "\n",
    "data_list = [{\"text\": line} for line in data_list]\n",
    "\n",
    "print(data_list)\n",
    "\n",
    "Dataset.from_list(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data For Training\n",
    "\n",
    "Let's put is all together and construct some useful data structure for training.\n",
    "There is not one way to prepare data for training. The choice depends on the data, its quality, format, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the torch way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1. read data into pairs\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "class TranslationDataset(Dataset) :\n",
    "\n",
    "    def __init__(self) :\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = load_dataset(\"opus_books\", 'en-fr', split='train')\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "\n",
    "        return self.data[index][\"translation\"]['en'], self.data[index][\"translation\"]['fr']\n",
    "    \n",
    "dataset = TranslationDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2. split data\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_set, valid_set = random_split(dataset, lengths=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3. preprocessing\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "def preprocess(batch) :\n",
    "\n",
    "    prefix = \"translate English to French: \"\n",
    "    inputs = [prefix + example[0] for example in batch]\n",
    "    targets = [example[1] for example in batch]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, padding=\"max_length\", max_length=500, truncation=True, return_tensors=\"pt\")\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4. construct dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=False, collate_fn=preprocess)\n",
    "valid_loader = DataLoader(train_set, batch_size=1, shuffle=False, collate_fn=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 8786,     3,    85,  ...,     0,     0,     0],\n",
       "        [  325,     3,    40,  ...,     0,     0,     0],\n",
       "        [  312,  1072,     9,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 1022, 11857,     9,  ...,     0,     0,     0],\n",
       "        [  312, 14879,     6,  ...,     0,     0,     0],\n",
       "        [ 1636,  3307,    71,  ...,     0,     0,     0]])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1. load data\n",
    "\n",
    "data = load_dataset(\"opus_books\", 'en-fr', split='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2. split data\n",
    "\n",
    "data_set =  data.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step3. preprocesse data\n",
    "\n",
    "# Here should use batched processing\n",
    "\n",
    "def preprocess_batched(examples):\n",
    "    \n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "preprocessed_data = data_set.map(preprocess_batched, batched=True, remove_columns=data_set[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step4. define tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step5. define model\n",
    "\n",
    "# The reason to provide model to collator is that this information can\n",
    "# inform the collator to shift or not the decoder input by an extra \n",
    "# token.\n",
    "# This is vitally important for translation tasks. But not necessary for\n",
    "# other tasks such as classifications.\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, AutoConfig\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step6. get collator\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step7. construct dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader_col = DataLoader(preprocessed_data[\"train\"], batch_size=10, shuffle=False, collate_fn=data_collator)\n",
    "valid_loader_col = DataLoader(preprocessed_data[\"test\"], batch_size=1, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[13959,  1566,    12,  2379,    10,   101,   646,    24,   294,    13,\n",
       "             8,   684,  2111, 17310,   203,   977,    11,  1522,  1852,   470,\n",
       "           281,   223,    12,    34,     5,     1,     0,     0],\n",
       "        [13959,  1566,    12,  2379,    10,  1853, 10842, 10327,  3316,     1,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [13959,  1566,    12,  2379,    10,   901,     9,    77,    18,   371,\n",
       "          1211,  8632,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [13959,  1566,    12,  2379,    10,   101,   130,   840,    16,     8,\n",
       "           740,    13,     8, 16808, 18867, 23081,    44,  2788,    15,    18,\n",
       "           188,  5497,    88,    31,     7,  1121,     5,     1],\n",
       "        [13959,  1566,    12,  2379,    10,   216,  4363,    44,    69,   234,\n",
       "            30,     3,     9,  1771,    13,  1671,     6,     3, 25312,    18,\n",
       "             5,     1,     0,     0,     0,     0,     0,     0],\n",
       "        [13959,  1566,    12,  2379,    10,    27,     1,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [13959,  1566,    12,  2379,    10,    37, 12832,    49,     1,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [13959,  1566,    12,  2379,    10,    27,   341,   497,     3,    31,\n",
       "          1211,   234,     6,    31,  2199,     8,   629,   150,  1200, 16952,\n",
       "            12,   178,     5,     1,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0]]), 'labels': tensor([[ 2228,     3,  4123, 10399,  2229,    90,  3277,  2873, 21707,   285,\n",
       "            29,   776,    46,     7,     3,    15,    17,   678,     3,    29,\n",
       "            22,    63, 21109, 18935,     7,   824,  1194,  6462,     5,     1],\n",
       "        [ 9132,   276, 18433,  9215,  5999, 14132,     1,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [  901,     9,    77,    18,   371,  1211,  8632,     1,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 2228,  7386,  2865,   110, 16414,     7,   146, 13579,     7,  1923,\n",
       "         14051,  1238,    20,  2788,    15,    18,   188,  5497,    88,     5,\n",
       "             1,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [  802,     3, 10269,   900,  4058,   678,    73, 15872,    20, 13123,\n",
       "             3, 25312,    18,   233,     1,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [    3, 20891,  4111, 20371, 22694,   329, 18988,     1,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [  312,  1907,  1212,     9,    83,  1496,     1,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1022,   916,     3,    85,  5794,   673,  4058,   678,  5506,   923,\n",
       "           238,    50,  4053,     3,    29,    15,   678,     3, 10198,    17,\n",
       "          4731,   303,     5,     1,  -100,  -100,  -100,  -100,  -100,  -100]]), 'decoder_input_ids': tensor([[    0,  2228,     3,  4123, 10399,  2229,    90,  3277,  2873, 21707,\n",
       "           285,    29,   776,    46,     7,     3,    15,    17,   678,     3,\n",
       "            29,    22,    63, 21109, 18935,     7,   824,  1194,  6462,     5],\n",
       "        [    0,  9132,   276, 18433,  9215,  5999, 14132,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,   901,     9,    77,    18,   371,  1211,  8632,     1,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,  2228,  7386,  2865,   110, 16414,     7,   146, 13579,     7,\n",
       "          1923, 14051,  1238,    20,  2788,    15,    18,   188,  5497,    88,\n",
       "             5,     1,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,   802,     3, 10269,   900,  4058,   678,    73, 15872,    20,\n",
       "         13123,     3, 25312,    18,   233,     1,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,     3, 20891,  4111, 20371, 22694,   329, 18988,     1,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,   312,  1907,  1212,     9,    83,  1496,     1,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [    0,  1022,   916,     3,    85,  5794,   673,  4058,   678,  5506,\n",
       "           923,   238,    50,  4053,     3,    29,    15,   678,     3, 10198,\n",
       "            17,  4731,   303,     5,     1,     0,     0,     0,     0,     0]])}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data are not the same as the torch way since the split of the data is random.\n",
    "next(iter(train_loader_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Update training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the Training section from 'hf_transformers_basics_model.ipynb\" and replace the data module of torch by the datasets module of transformers explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this.\n",
    "# Should be run the first\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9002503a1c4e44ee93ada57e1488bb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4084 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "    num_rows: 3548\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) load dataset\n",
    "#################\n",
    "\n",
    "# replace the class by the load_dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ckp_data = \"davidberg/sentiment-reviews\"\n",
    "\n",
    "data = load_dataset(ckp_data, split=\"train\")\n",
    "\n",
    "data = data.filter(lambda column: \"neutral\" not in column[\"division\"]) # filter out neutral\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 3193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) split data\n",
    "###############\n",
    "\n",
    "# replace the ramdom_split by the trans_test_split of datasets\n",
    "\n",
    "split_data = data.train_test_split(test_size=0.1)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce9d010b2ba4c75a052d2c9b7ece5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334e6fe8dc334eb1beaf0818fd541e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) tokenizer\n",
    "##############\n",
    "\n",
    "# we define a function for tokenization and use map to obtain the tokens\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "label2id = {\"negative\":0, \"positive\": 1}\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def process(batch):\n",
    "\n",
    "    toks = tokenizer(batch[\"review\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    toks[\"labels\"] = torch.tensor([label2id.get(item) for item in batch[\"division\"]])\n",
    "\n",
    "    return toks\n",
    "\n",
    "tokenized_data = split_data.map(process, batched=True, remove_columns=data.column_names)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 17:19:16.835486: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 17:19:16.835539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 17:19:16.837529: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-19 17:19:16.845762: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 17:19:18.851902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 4) dataloader\n",
    "###############\n",
    "\n",
    "# replace the user-defined collate function by the transformers' data collator\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "trainset, validset = tokenized_data[\"train\"], tokenized_data[\"test\"]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "validloader = DataLoader(validset, batch_size=32, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5) load model\n",
    "###############\n",
    "\n",
    "# not changed\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# sent to gpu\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) define optimizer\n",
    "#####################\n",
    "\n",
    "# not changed\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) evaluation\n",
    "###############\n",
    "\n",
    "# not changed\n",
    "\n",
    "def eval():\n",
    "\n",
    "    acc_count = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in validloader:\n",
    "\n",
    "        # if there is GPU, send the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        output = model(**batch)\n",
    "\n",
    "        pred = torch.argmax(output.logits, dim=-1)\n",
    "\n",
    "        # count correct labels\n",
    "        acc_count += (pred.int() == batch[\"labels\"].int()).sum()\n",
    "\n",
    "    return acc_count / len(validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Train\n",
    "##########\n",
    "\n",
    "# not changed\n",
    "\n",
    "def train(epoch=3, log_step=50):\n",
    "\n",
    "    gStep = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in trainloader:\n",
    "            \n",
    "            # if there is GPU, send the data to GPU\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(**batch)\n",
    "\n",
    "            output.loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if gStep % log_step == 0:\n",
    "\n",
    "                print(f\"{e+1} / {epoch} - global step: {gStep}, loss: {output.loss.item()}\")\n",
    "\n",
    "            gStep += 1\n",
    "\n",
    "        acc = eval()\n",
    "\n",
    "        print(f\"{e+1} / {epoch} - acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 3 - global step: 0, loss: 0.822261393070221\n",
      "1 / 3 - global step: 50, loss: 0.26876893639564514\n",
      "1 / 3 - acc: 0.9492957592010498\n",
      "2 / 3 - global step: 100, loss: 0.12767672538757324\n",
      "2 / 3 - global step: 150, loss: 0.04294269159436226\n",
      "2 / 3 - acc: 0.9605633616447449\n",
      "3 / 3 - global step: 200, loss: 0.08602031320333481\n",
      "3 / 3 - global step: 250, loss: 0.014622676186263561\n",
      "3 / 3 - acc: 0.9436619281768799\n"
     ]
    }
   ],
   "source": [
    "# not changed\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
