{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this botebook, it will:\n",
    "\n",
    "    I. present transformer models\n",
    "    II. explain the use of model\n",
    "        A. model download\n",
    "        B. model Inference\n",
    "        C. Model with Head\n",
    "    III. Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Transformer\n",
    "\n",
    "\n",
    "#### 1. Transformer Model structure\n",
    "\n",
    "This is the original transformer structure, which is composed of an encoder and a decoder.\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1-727x1024.png\" width=\"300\"/>\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/model_summary\n",
    "\n",
    "#### 2. Model Types\n",
    "\n",
    "Based on the principal components of the transformer models, there are 3 major types:\n",
    "\n",
    "* encoder-only model\n",
    "* decoder-only model\n",
    "* encoder-decoder model\n",
    "\n",
    "#### 3. Pretrained Models\n",
    "\n",
    "This is a summary of some pretrained models and their applications.\n",
    "\n",
    "| model type      | pretrained model                    | applications                                 |\n",
    "|---\t          |---\t                                |---\t                                       |\n",
    "| encoder-only    | ALBERT, BERT, DistilBERT, RoBERTa   | text classification, NER, Text comprehension |\n",
    "| decoder-only    | GPT, GPT-2, Bloom, LLaMA            | text generation                              | \n",
    "| encoder-decoder | BART, T5, Marian, mBART, GLM        | text summary, translation                    |\n",
    "\n",
    "#### 4. Model Head\n",
    "\n",
    "The model head is the last or several last fully connected layers. It is used to project the hidden states to the outputs based on the task.\n",
    "Transformers provide some predefined heads:\n",
    "* *Model: no head, return the hidden states\n",
    "* *ForCausalLM: decoder, return a sequence\n",
    "* *ForMaskedLM: \n",
    "* *ForSeq2SeqLM\n",
    "* *ForMultipleChoice\n",
    "* *ForQuestionAnswering\n",
    "* *ForSequenceClassification\n",
    "* *ForTokenClassification\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. How to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) import\n",
    "###########\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Model download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we could use \"AutoModel\" to download the base models. The base models contain usually the common structure of the network without task-specific output layers head (eg, last layer of feedforward or dense layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Loading\n",
    "############\n",
    "\n",
    "## load online using Transformers\n",
    "\n",
    "# download base model without head\n",
    "\n",
    "# the model is in: https://huggingface.co/google-bert/bert-base-uncased\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download using git\n",
    "\n",
    "!git lfs clone https://huggingface.co/google-bert/bert-base-uncased --include=\"*.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load offline\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) model config\n",
    "#################\n",
    "\n",
    "## show model config\n",
    "\n",
    "# The config contains common charactistiques of the model such as:\n",
    "#   * max_position_embeddings: the maximum dim of input tensor\n",
    "#   * hidden_size: the dim of the hidden layer\n",
    "#   * hidden_dropout_prob: dropout rate\n",
    "#   * ...\n",
    "\n",
    "# The config contains only the model related parameters\n",
    "# But it don't contains the customizable arguments\n",
    "\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Qingyi/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## config\n",
    "\n",
    "# there is also an associated config object to the model\n",
    "# it is loaded as:\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "{0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "{0: 'Negative', 1: 'Positive'}\n"
     ]
    }
   ],
   "source": [
    "## change model config\n",
    "\n",
    "# show attentions\n",
    "print(model.config.output_attentions)\n",
    "model.config.output_attentions = True\n",
    "print(model.config.output_attentions)\n",
    "\n",
    "# convert id to label\n",
    "print(model.config.id2label)\n",
    "model.config.id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "print(model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Negative', 1: 'Positive'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2234, 2182, 6575, 2005, 2019, 3437, 1012,  102]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) prepare some data\n",
    "######################\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "input_text = \"I came here searching for an answer.\"\n",
    "tokenized_data = tokenizer(input_text, return_tensors='pt')\n",
    "tokenized_data.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Model with no head\n",
    "#######################\n",
    "\n",
    "# This is a encoder-only model.\n",
    "\n",
    "# the last layer is Linear(in_features=768, out_features=768)\n",
    "\n",
    "model_nh = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "model_nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0414,  0.2263, -0.3680,  ..., -0.0351,  0.2934,  0.5573],\n",
       "         [ 0.6679,  0.0198, -0.5826,  ..., -0.0465,  0.5988,  0.4897],\n",
       "         [ 0.0205, -0.1866,  0.0394,  ...,  0.0380,  0.1209, -0.2524],\n",
       "         ...,\n",
       "         [ 0.6688, -0.5342,  0.1776,  ..., -0.1937,  0.1931,  0.0144],\n",
       "         [ 0.5437,  0.2672, -0.4816,  ..., -0.0911, -0.2954, -0.7393],\n",
       "         [ 0.7015,  0.1112,  0.1859,  ...,  0.3119, -0.4980, -0.4962]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9180, -0.4124, -0.8767,  0.7994,  0.6171, -0.1314,  0.8555,  0.2321,\n",
       "         -0.8629, -0.9999, -0.5016,  0.9781,  0.9815,  0.5616,  0.9504, -0.7621,\n",
       "         -0.3130, -0.6262,  0.1598, -0.5764,  0.7138,  0.9999, -0.0433,  0.3060,\n",
       "          0.3983,  0.9946, -0.6577,  0.9428,  0.9665,  0.7421, -0.6475,  0.0761,\n",
       "         -0.9916,  0.0264, -0.8849, -0.9890,  0.4022, -0.6485,  0.1285,  0.2126,\n",
       "         -0.9041,  0.1580,  1.0000, -0.1404,  0.4013, -0.1488, -1.0000,  0.2417,\n",
       "         -0.9001,  0.9400,  0.8911,  0.9445,  0.1067,  0.5272,  0.4684, -0.5014,\n",
       "         -0.0930,  0.0190, -0.1577, -0.4902, -0.6143,  0.3922, -0.8483, -0.8363,\n",
       "          0.9380,  0.8243, -0.0339, -0.2032,  0.0220, -0.2276,  0.8732,  0.1591,\n",
       "         -0.2310, -0.8445,  0.7110,  0.1510, -0.6858,  1.0000, -0.6897, -0.9828,\n",
       "          0.7484,  0.7503,  0.5812, -0.2819,  0.2771, -1.0000,  0.6091,  0.0784,\n",
       "         -0.9900,  0.0855,  0.5915, -0.2747,  0.2132,  0.6100, -0.3016, -0.4931,\n",
       "         -0.2935, -0.8641, -0.2291, -0.5081,  0.0864, -0.0739, -0.3583, -0.2904,\n",
       "          0.3562, -0.5216, -0.5203,  0.4861,  0.1030,  0.6840,  0.4439, -0.2712,\n",
       "          0.4521, -0.9585,  0.6755, -0.1613, -0.9888, -0.6031, -0.9900,  0.5915,\n",
       "         -0.3021, -0.1397,  0.9669, -0.1282,  0.2649,  0.0486, -0.9602, -1.0000,\n",
       "         -0.7574, -0.4713, -0.2150, -0.2626, -0.9828, -0.9730,  0.5225,  0.9536,\n",
       "          0.1563,  0.9997, -0.2136,  0.9485, -0.3719, -0.6730,  0.7834, -0.3762,\n",
       "          0.7243,  0.2306, -0.4817,  0.0601, -0.4628,  0.4640, -0.7264, -0.1854,\n",
       "         -0.7985, -0.9076, -0.2544,  0.9548, -0.6097, -0.9289, -0.0111, -0.1173,\n",
       "         -0.3378,  0.6740,  0.8108,  0.2503, -0.4830,  0.3745, -0.0689,  0.5639,\n",
       "         -0.8452, -0.2927,  0.3487, -0.3157, -0.8039, -0.9862, -0.2464,  0.3839,\n",
       "          0.9912,  0.7458,  0.1635,  0.8718, -0.2513,  0.7996, -0.9482,  0.9882,\n",
       "         -0.0555,  0.2064, -0.1890,  0.4028, -0.8865, -0.0885,  0.7983, -0.7286,\n",
       "         -0.8960, -0.0950, -0.4080, -0.2963, -0.7680,  0.5136, -0.1754, -0.2419,\n",
       "          0.0937,  0.9465,  0.9671,  0.7536,  0.1283,  0.6936, -0.8720, -0.5537,\n",
       "          0.1568,  0.1352,  0.0487,  0.9957, -0.4923,  0.0046, -0.9478, -0.9883,\n",
       "         -0.2782, -0.9521, -0.0494, -0.6243,  0.6812, -0.1614,  0.5789,  0.4936,\n",
       "         -0.9802, -0.8229,  0.4981, -0.3888,  0.3966, -0.2317,  0.9002,  0.9298,\n",
       "         -0.6205,  0.6034,  0.9435, -0.8472, -0.8253,  0.7836, -0.2471,  0.8767,\n",
       "         -0.5831,  0.9618,  0.9278,  0.8602, -0.9220, -0.5765, -0.8659, -0.6661,\n",
       "          0.0215, -0.1605,  0.8968,  0.6422,  0.3283,  0.6744, -0.5940,  0.9971,\n",
       "         -0.8975, -0.9586,  0.0576, -0.2522, -0.9914,  0.8209,  0.0980,  0.0083,\n",
       "         -0.3755, -0.6688, -0.9654,  0.8600,  0.1205,  0.9821, -0.4432, -0.9029,\n",
       "         -0.7366, -0.9284, -0.2439, -0.1511, -0.4176, -0.1414, -0.9587,  0.4663,\n",
       "          0.5625,  0.4952, -0.7710,  0.9984,  1.0000,  0.9798,  0.9004,  0.8526,\n",
       "         -0.9997, -0.5203,  1.0000, -0.9956, -1.0000, -0.9421, -0.6485,  0.3142,\n",
       "         -1.0000, -0.1545,  0.1454, -0.8927,  0.7181,  0.9828,  0.9933, -1.0000,\n",
       "          0.7353,  0.9568, -0.6300,  0.9767, -0.4932,  0.9749,  0.4575,  0.3063,\n",
       "         -0.0598,  0.3145, -0.8842, -0.8403, -0.5629, -0.7405,  0.9980,  0.0477,\n",
       "         -0.8239, -0.9102,  0.6071, -0.0108, -0.2900, -0.9714, -0.0615,  0.6247,\n",
       "          0.7857,  0.1302,  0.1927, -0.6812,  0.1673, -0.0993,  0.0795,  0.7112,\n",
       "         -0.9184, -0.5107, -0.1113, -0.0667, -0.4531, -0.9692,  0.9676, -0.3222,\n",
       "          0.9368,  1.0000,  0.0574, -0.8909,  0.6575,  0.2829, -0.1770,  1.0000,\n",
       "          0.8507, -0.9811, -0.5849,  0.6714, -0.5227, -0.6929,  0.9983, -0.2354,\n",
       "         -0.7700, -0.4905,  0.9825, -0.9925,  0.9925, -0.9206, -0.9796,  0.9717,\n",
       "          0.9579, -0.6452, -0.4066, -0.0207, -0.4795,  0.2233, -0.9535,  0.7728,\n",
       "          0.5710,  0.0654,  0.9287, -0.7840, -0.5979,  0.2638, -0.7195, -0.2427,\n",
       "          0.9268,  0.4880, -0.1016,  0.0842, -0.1021, -0.7058, -0.9743,  0.6204,\n",
       "          1.0000, -0.1443,  0.8048, -0.4362,  0.0031, -0.1731,  0.4618,  0.5028,\n",
       "         -0.2315, -0.8018,  0.8481, -0.9684, -0.9901,  0.6497,  0.0761, -0.1759,\n",
       "          1.0000,  0.4864,  0.0839,  0.3893,  0.9926, -0.1119,  0.3761,  0.8587,\n",
       "          0.9864, -0.1504,  0.5688,  0.8717, -0.9193, -0.3258, -0.6681, -0.1559,\n",
       "         -0.9344,  0.1101, -0.9597,  0.9759,  0.9614,  0.3605,  0.1040,  0.7078,\n",
       "          1.0000, -0.5917,  0.5109,  0.0412,  0.7339, -0.9996, -0.8517, -0.3157,\n",
       "          0.0026, -0.8213, -0.3570,  0.1982, -0.9633,  0.8113,  0.6415, -0.9870,\n",
       "         -0.9908, -0.3664,  0.8003, -0.0291, -0.9844, -0.7545, -0.5898,  0.7090,\n",
       "         -0.2109, -0.9467, -0.2253, -0.2016,  0.5199, -0.1493,  0.6024,  0.8266,\n",
       "          0.8231, -0.5374, -0.1754,  0.0580, -0.7463,  0.9043, -0.8436, -0.9374,\n",
       "         -0.0628,  1.0000, -0.5907,  0.8608,  0.7276,  0.7776, -0.0483,  0.1436,\n",
       "          0.8570,  0.1487, -0.5125, -0.9129, -0.2740, -0.3008,  0.7265,  0.5512,\n",
       "          0.5604,  0.8010,  0.7668,  0.0650,  0.0871, -0.0743,  0.9996, -0.0849,\n",
       "         -0.1160, -0.5051, -0.0225, -0.2748,  0.0068,  1.0000,  0.3780,  0.3019,\n",
       "         -0.9912, -0.8860, -0.8706,  1.0000,  0.8388, -0.7586,  0.7071,  0.6164,\n",
       "         -0.0462,  0.8134, -0.0201, -0.0714,  0.2255, -0.0094,  0.9562, -0.5392,\n",
       "         -0.9725, -0.4765,  0.5131, -0.9760,  0.9998, -0.5200, -0.1941, -0.3243,\n",
       "         -0.2828, -0.2911, -0.2730, -0.9876, -0.2392,  0.2435,  0.9743,  0.1690,\n",
       "         -0.5959, -0.9264,  0.8341,  0.7592, -0.9388, -0.9205,  0.9659, -0.9843,\n",
       "          0.7574,  1.0000,  0.2520,  0.1889,  0.1754, -0.4730,  0.3472, -0.1022,\n",
       "          0.7927, -0.9702, -0.2782, -0.2092,  0.1161, -0.1408, -0.3801,  0.5982,\n",
       "          0.1062, -0.5655, -0.5925, -0.0875,  0.4660,  0.8427, -0.0253, -0.1168,\n",
       "          0.1444, -0.1406, -0.9055, -0.3135, -0.4491, -1.0000,  0.5860, -1.0000,\n",
       "          0.5088,  0.2140, -0.1128,  0.7860,  0.5104,  0.7461, -0.7719, -0.8692,\n",
       "          0.5519,  0.7475, -0.3278, -0.6971, -0.7067,  0.2677,  0.0175,  0.1626,\n",
       "         -0.6906,  0.7084, -0.3171,  1.0000,  0.1266, -0.7760, -0.9720,  0.0352,\n",
       "         -0.1489,  1.0000, -0.9187, -0.9555,  0.3609, -0.7638, -0.8338,  0.3669,\n",
       "         -0.0417, -0.7633, -0.9328,  0.9473,  0.8456, -0.6367,  0.6765, -0.1417,\n",
       "         -0.6292, -0.1306,  0.8435,  0.9846,  0.1269,  0.9287,  0.0784, -0.4273,\n",
       "          0.9812,  0.3279,  0.3873, -0.0556,  1.0000,  0.2847, -0.9191,  0.1782,\n",
       "         -0.9848, -0.0810, -0.9676,  0.3153,  0.1275,  0.9495, -0.1190,  0.9640,\n",
       "         -0.8608, -0.0656, -0.7337, -0.5159,  0.3650, -0.9291, -0.9859, -0.9851,\n",
       "          0.5796, -0.4172,  0.0257,  0.1422, -0.0588,  0.3405,  0.3152, -1.0000,\n",
       "          0.9429,  0.4823,  0.8395,  0.9728,  0.7519,  0.5812,  0.2338, -0.9881,\n",
       "         -0.9835, -0.2110, -0.1888,  0.6948,  0.6385,  0.9043,  0.4322, -0.4351,\n",
       "         -0.4808, -0.5284, -0.7627, -0.9924,  0.3673, -0.6919, -0.9675,  0.9519,\n",
       "         -0.4329, -0.0506, -0.0726, -0.7941,  0.8796,  0.7033,  0.3724, -0.1299,\n",
       "          0.3793,  0.8816,  0.9001,  0.9847, -0.7949,  0.7426, -0.7899,  0.4381,\n",
       "          0.6875, -0.9103,  0.0795,  0.4150, -0.2578,  0.2334, -0.1644, -0.9636,\n",
       "          0.4666, -0.1503,  0.4988, -0.2862,  0.1685, -0.2980, -0.0805, -0.6332,\n",
       "         -0.7384,  0.6691,  0.4500,  0.9247,  0.7913,  0.0681, -0.8070, -0.0508,\n",
       "         -0.6784, -0.8949,  0.9178,  0.1724, -0.1454,  0.7365, -0.1111,  0.7338,\n",
       "         -0.1144, -0.3545, -0.3204, -0.6624,  0.8667, -0.7018, -0.4501, -0.4557,\n",
       "          0.5162,  0.2668,  1.0000, -0.7294, -0.9249, -0.5171, -0.3628,  0.3777,\n",
       "         -0.5442, -1.0000,  0.3820, -0.5245,  0.6752, -0.7248,  0.7441, -0.7347,\n",
       "         -0.9736, -0.0186,  0.5809,  0.7256, -0.4746, -0.7343,  0.6452, -0.6313,\n",
       "          0.9775,  0.8614, -0.6140,  0.3287,  0.7121, -0.7746, -0.6475,  0.9370]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Inference\n",
    "##############\n",
    "\n",
    "## output of model\n",
    "\n",
    "# We input some dummy inputs to show the outputs of the model.\n",
    "\n",
    "# By default, the model gives 2 outpus:\n",
    "#\n",
    "#   * last_hidden_state: is the hidden state of the input tokens, it is of dim [batch, seq_len, hidden_size].\n",
    "#     So, as our input tokens has len of 10, we have the dimension of the last_hidden_state [1, 10, 768], since the\n",
    "#     hidden size of the model is 768 (see model.config).\n",
    "#     To check its dim we can use: output.last_hidden_state.size()\n",
    "\n",
    "#   * pooler_output: is the pooled hidden state of the last_hidden_state, it is of dim [batch, hidden_size].\n",
    "#     So, with the hidden size of 768 (see model.config), we have the dimension of the pooler_output [1, 768].\n",
    "#     To check its dim we can use: output.pooler_output.size()\n",
    "\n",
    "# And all other outputs hidden_states, past_key_values, attentions, cross_attentions are None.\n",
    "\n",
    "output = model_nh(input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0414,  0.2263, -0.3680,  ..., -0.0351,  0.2934,  0.5573],\n",
       "         [ 0.6679,  0.0198, -0.5826,  ..., -0.0465,  0.5988,  0.4897],\n",
       "         [ 0.0205, -0.1866,  0.0394,  ...,  0.0380,  0.1209, -0.2524],\n",
       "         ...,\n",
       "         [ 0.6688, -0.5342,  0.1776,  ..., -0.1937,  0.1931,  0.0144],\n",
       "         [ 0.5437,  0.2672, -0.4816,  ..., -0.0911, -0.2954, -0.7393],\n",
       "         [ 0.7015,  0.1112,  0.1859,  ...,  0.3119, -0.4980, -0.4962]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9180, -0.4124, -0.8767,  0.7994,  0.6171, -0.1314,  0.8555,  0.2321,\n",
       "         -0.8629, -0.9999, -0.5016,  0.9781,  0.9815,  0.5616,  0.9504, -0.7621,\n",
       "         -0.3130, -0.6262,  0.1598, -0.5764,  0.7138,  0.9999, -0.0433,  0.3060,\n",
       "          0.3983,  0.9946, -0.6577,  0.9428,  0.9665,  0.7421, -0.6475,  0.0761,\n",
       "         -0.9916,  0.0264, -0.8849, -0.9890,  0.4022, -0.6485,  0.1285,  0.2126,\n",
       "         -0.9041,  0.1580,  1.0000, -0.1404,  0.4013, -0.1488, -1.0000,  0.2417,\n",
       "         -0.9001,  0.9400,  0.8911,  0.9445,  0.1067,  0.5272,  0.4684, -0.5014,\n",
       "         -0.0930,  0.0190, -0.1577, -0.4902, -0.6143,  0.3922, -0.8483, -0.8363,\n",
       "          0.9380,  0.8243, -0.0339, -0.2032,  0.0220, -0.2276,  0.8732,  0.1591,\n",
       "         -0.2310, -0.8445,  0.7110,  0.1510, -0.6858,  1.0000, -0.6897, -0.9828,\n",
       "          0.7484,  0.7503,  0.5812, -0.2819,  0.2771, -1.0000,  0.6091,  0.0784,\n",
       "         -0.9900,  0.0855,  0.5915, -0.2747,  0.2132,  0.6100, -0.3016, -0.4931,\n",
       "         -0.2935, -0.8641, -0.2291, -0.5081,  0.0864, -0.0739, -0.3583, -0.2904,\n",
       "          0.3562, -0.5216, -0.5203,  0.4861,  0.1030,  0.6840,  0.4439, -0.2712,\n",
       "          0.4521, -0.9585,  0.6755, -0.1613, -0.9888, -0.6031, -0.9900,  0.5915,\n",
       "         -0.3021, -0.1397,  0.9669, -0.1282,  0.2649,  0.0486, -0.9602, -1.0000,\n",
       "         -0.7574, -0.4713, -0.2150, -0.2626, -0.9828, -0.9730,  0.5225,  0.9536,\n",
       "          0.1563,  0.9997, -0.2136,  0.9485, -0.3719, -0.6730,  0.7834, -0.3762,\n",
       "          0.7243,  0.2306, -0.4817,  0.0601, -0.4628,  0.4640, -0.7264, -0.1854,\n",
       "         -0.7985, -0.9076, -0.2544,  0.9548, -0.6097, -0.9289, -0.0111, -0.1173,\n",
       "         -0.3378,  0.6740,  0.8108,  0.2503, -0.4830,  0.3745, -0.0689,  0.5639,\n",
       "         -0.8452, -0.2927,  0.3487, -0.3157, -0.8039, -0.9862, -0.2464,  0.3839,\n",
       "          0.9912,  0.7458,  0.1635,  0.8718, -0.2513,  0.7996, -0.9482,  0.9882,\n",
       "         -0.0555,  0.2064, -0.1890,  0.4028, -0.8865, -0.0885,  0.7983, -0.7286,\n",
       "         -0.8960, -0.0950, -0.4080, -0.2963, -0.7680,  0.5136, -0.1754, -0.2419,\n",
       "          0.0937,  0.9465,  0.9671,  0.7536,  0.1283,  0.6936, -0.8720, -0.5537,\n",
       "          0.1568,  0.1352,  0.0487,  0.9957, -0.4923,  0.0046, -0.9478, -0.9883,\n",
       "         -0.2782, -0.9521, -0.0494, -0.6243,  0.6812, -0.1614,  0.5789,  0.4936,\n",
       "         -0.9802, -0.8229,  0.4981, -0.3888,  0.3966, -0.2317,  0.9002,  0.9298,\n",
       "         -0.6205,  0.6034,  0.9435, -0.8472, -0.8253,  0.7836, -0.2471,  0.8767,\n",
       "         -0.5831,  0.9618,  0.9278,  0.8602, -0.9220, -0.5765, -0.8659, -0.6661,\n",
       "          0.0215, -0.1605,  0.8968,  0.6422,  0.3283,  0.6744, -0.5940,  0.9971,\n",
       "         -0.8975, -0.9586,  0.0576, -0.2522, -0.9914,  0.8209,  0.0980,  0.0083,\n",
       "         -0.3755, -0.6688, -0.9654,  0.8600,  0.1205,  0.9821, -0.4432, -0.9029,\n",
       "         -0.7366, -0.9284, -0.2439, -0.1511, -0.4176, -0.1414, -0.9587,  0.4663,\n",
       "          0.5625,  0.4952, -0.7710,  0.9984,  1.0000,  0.9798,  0.9004,  0.8526,\n",
       "         -0.9997, -0.5203,  1.0000, -0.9956, -1.0000, -0.9421, -0.6485,  0.3142,\n",
       "         -1.0000, -0.1545,  0.1454, -0.8927,  0.7181,  0.9828,  0.9933, -1.0000,\n",
       "          0.7353,  0.9568, -0.6300,  0.9767, -0.4932,  0.9749,  0.4575,  0.3063,\n",
       "         -0.0598,  0.3145, -0.8842, -0.8403, -0.5629, -0.7405,  0.9980,  0.0477,\n",
       "         -0.8239, -0.9102,  0.6071, -0.0108, -0.2900, -0.9714, -0.0615,  0.6247,\n",
       "          0.7857,  0.1302,  0.1927, -0.6812,  0.1673, -0.0993,  0.0795,  0.7112,\n",
       "         -0.9184, -0.5107, -0.1113, -0.0667, -0.4531, -0.9692,  0.9676, -0.3222,\n",
       "          0.9368,  1.0000,  0.0574, -0.8909,  0.6575,  0.2829, -0.1770,  1.0000,\n",
       "          0.8507, -0.9811, -0.5849,  0.6714, -0.5227, -0.6929,  0.9983, -0.2354,\n",
       "         -0.7700, -0.4905,  0.9825, -0.9925,  0.9925, -0.9206, -0.9796,  0.9717,\n",
       "          0.9579, -0.6452, -0.4066, -0.0207, -0.4795,  0.2233, -0.9535,  0.7728,\n",
       "          0.5710,  0.0654,  0.9287, -0.7840, -0.5979,  0.2638, -0.7195, -0.2427,\n",
       "          0.9268,  0.4880, -0.1016,  0.0842, -0.1021, -0.7058, -0.9743,  0.6204,\n",
       "          1.0000, -0.1443,  0.8048, -0.4362,  0.0031, -0.1731,  0.4618,  0.5028,\n",
       "         -0.2315, -0.8018,  0.8481, -0.9684, -0.9901,  0.6497,  0.0761, -0.1759,\n",
       "          1.0000,  0.4864,  0.0839,  0.3893,  0.9926, -0.1119,  0.3761,  0.8587,\n",
       "          0.9864, -0.1504,  0.5688,  0.8717, -0.9193, -0.3258, -0.6681, -0.1559,\n",
       "         -0.9344,  0.1101, -0.9597,  0.9759,  0.9614,  0.3605,  0.1040,  0.7078,\n",
       "          1.0000, -0.5917,  0.5109,  0.0412,  0.7339, -0.9996, -0.8517, -0.3157,\n",
       "          0.0026, -0.8213, -0.3570,  0.1982, -0.9633,  0.8113,  0.6415, -0.9870,\n",
       "         -0.9908, -0.3664,  0.8003, -0.0291, -0.9844, -0.7545, -0.5898,  0.7090,\n",
       "         -0.2109, -0.9467, -0.2253, -0.2016,  0.5199, -0.1493,  0.6024,  0.8266,\n",
       "          0.8231, -0.5374, -0.1754,  0.0580, -0.7463,  0.9043, -0.8436, -0.9374,\n",
       "         -0.0628,  1.0000, -0.5907,  0.8608,  0.7276,  0.7776, -0.0483,  0.1436,\n",
       "          0.8570,  0.1487, -0.5125, -0.9129, -0.2740, -0.3008,  0.7265,  0.5512,\n",
       "          0.5604,  0.8010,  0.7668,  0.0650,  0.0871, -0.0743,  0.9996, -0.0849,\n",
       "         -0.1160, -0.5051, -0.0225, -0.2748,  0.0068,  1.0000,  0.3780,  0.3019,\n",
       "         -0.9912, -0.8860, -0.8706,  1.0000,  0.8388, -0.7586,  0.7071,  0.6164,\n",
       "         -0.0462,  0.8134, -0.0201, -0.0714,  0.2255, -0.0094,  0.9562, -0.5392,\n",
       "         -0.9725, -0.4765,  0.5131, -0.9760,  0.9998, -0.5200, -0.1941, -0.3243,\n",
       "         -0.2828, -0.2911, -0.2730, -0.9876, -0.2392,  0.2435,  0.9743,  0.1690,\n",
       "         -0.5959, -0.9264,  0.8341,  0.7592, -0.9388, -0.9205,  0.9659, -0.9843,\n",
       "          0.7574,  1.0000,  0.2520,  0.1889,  0.1754, -0.4730,  0.3472, -0.1022,\n",
       "          0.7927, -0.9702, -0.2782, -0.2092,  0.1161, -0.1408, -0.3801,  0.5982,\n",
       "          0.1062, -0.5655, -0.5925, -0.0875,  0.4660,  0.8427, -0.0253, -0.1168,\n",
       "          0.1444, -0.1406, -0.9055, -0.3135, -0.4491, -1.0000,  0.5860, -1.0000,\n",
       "          0.5088,  0.2140, -0.1128,  0.7860,  0.5104,  0.7461, -0.7719, -0.8692,\n",
       "          0.5519,  0.7475, -0.3278, -0.6971, -0.7067,  0.2677,  0.0175,  0.1626,\n",
       "         -0.6906,  0.7084, -0.3171,  1.0000,  0.1266, -0.7760, -0.9720,  0.0352,\n",
       "         -0.1489,  1.0000, -0.9187, -0.9555,  0.3609, -0.7638, -0.8338,  0.3669,\n",
       "         -0.0417, -0.7633, -0.9328,  0.9473,  0.8456, -0.6367,  0.6765, -0.1417,\n",
       "         -0.6292, -0.1306,  0.8435,  0.9846,  0.1269,  0.9287,  0.0784, -0.4273,\n",
       "          0.9812,  0.3279,  0.3873, -0.0556,  1.0000,  0.2847, -0.9191,  0.1782,\n",
       "         -0.9848, -0.0810, -0.9676,  0.3153,  0.1275,  0.9495, -0.1190,  0.9640,\n",
       "         -0.8608, -0.0656, -0.7337, -0.5159,  0.3650, -0.9291, -0.9859, -0.9851,\n",
       "          0.5796, -0.4172,  0.0257,  0.1422, -0.0588,  0.3405,  0.3152, -1.0000,\n",
       "          0.9429,  0.4823,  0.8395,  0.9728,  0.7519,  0.5812,  0.2338, -0.9881,\n",
       "         -0.9835, -0.2110, -0.1888,  0.6948,  0.6385,  0.9043,  0.4322, -0.4351,\n",
       "         -0.4808, -0.5284, -0.7627, -0.9924,  0.3673, -0.6919, -0.9675,  0.9519,\n",
       "         -0.4329, -0.0506, -0.0726, -0.7941,  0.8796,  0.7033,  0.3724, -0.1299,\n",
       "          0.3793,  0.8816,  0.9001,  0.9847, -0.7949,  0.7426, -0.7899,  0.4381,\n",
       "          0.6875, -0.9103,  0.0795,  0.4150, -0.2578,  0.2334, -0.1644, -0.9636,\n",
       "          0.4666, -0.1503,  0.4988, -0.2862,  0.1685, -0.2980, -0.0805, -0.6332,\n",
       "         -0.7384,  0.6691,  0.4500,  0.9247,  0.7913,  0.0681, -0.8070, -0.0508,\n",
       "         -0.6784, -0.8949,  0.9178,  0.1724, -0.1454,  0.7365, -0.1111,  0.7338,\n",
       "         -0.1144, -0.3545, -0.3204, -0.6624,  0.8667, -0.7018, -0.4501, -0.4557,\n",
       "          0.5162,  0.2668,  1.0000, -0.7294, -0.9249, -0.5171, -0.3628,  0.3777,\n",
       "         -0.5442, -1.0000,  0.3820, -0.5245,  0.6752, -0.7248,  0.7441, -0.7347,\n",
       "         -0.9736, -0.0186,  0.5809,  0.7256, -0.4746, -0.7343,  0.6452, -0.6313,\n",
       "          0.9775,  0.8614, -0.6140,  0.3287,  0.7121, -0.7746, -0.6475,  0.9370]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=(tensor([[[[5.8705e-02, 4.9745e-02, 3.9709e-02,  ..., 5.1344e-02,\n",
       "           1.1608e-01, 2.9860e-01],\n",
       "          [1.7420e-01, 1.2236e-01, 1.4925e-01,  ..., 6.0957e-02,\n",
       "           7.3646e-02, 7.7530e-02],\n",
       "          [1.2550e-01, 1.2213e-01, 1.5176e-01,  ..., 6.1761e-02,\n",
       "           1.3303e-01, 7.6433e-02],\n",
       "          ...,\n",
       "          [7.3450e-02, 1.1147e-01, 1.4711e-01,  ..., 4.5698e-02,\n",
       "           2.0724e-01, 8.7237e-02],\n",
       "          [7.8291e-02, 6.5436e-02, 1.0888e-01,  ..., 7.3567e-02,\n",
       "           1.5098e-01, 1.0575e-01],\n",
       "          [8.7444e-02, 8.7519e-02, 6.9004e-02,  ..., 7.4605e-02,\n",
       "           1.5548e-01, 1.7179e-01]],\n",
       "\n",
       "         [[6.3875e-01, 1.4950e-02, 1.0467e-02,  ..., 2.1252e-03,\n",
       "           3.3349e-02, 6.8249e-03],\n",
       "          [5.8080e-03, 1.5050e-02, 1.5284e-01,  ..., 3.1724e-01,\n",
       "           1.5308e-02, 2.4948e-02],\n",
       "          [6.7189e-03, 2.3877e-02, 6.0472e-02,  ..., 1.9797e-01,\n",
       "           1.4520e-02, 4.6716e-02],\n",
       "          ...,\n",
       "          [6.0141e-03, 1.8856e-02, 3.4205e-02,  ..., 1.3424e-01,\n",
       "           8.8966e-03, 3.0796e-02],\n",
       "          [1.4280e-03, 1.8017e-01, 1.0319e-01,  ..., 2.5167e-02,\n",
       "           3.3906e-01, 1.6939e-02],\n",
       "          [3.1997e-02, 1.6336e-01, 6.5069e-02,  ..., 2.1240e-02,\n",
       "           3.7758e-01, 3.1170e-02]],\n",
       "\n",
       "         [[7.8804e-01, 3.1666e-02, 1.6761e-02,  ..., 1.3824e-02,\n",
       "           3.0867e-02, 5.1481e-02],\n",
       "          [7.8968e-01, 7.4889e-02, 6.5408e-03,  ..., 7.6176e-04,\n",
       "           7.0515e-03, 3.8066e-02],\n",
       "          [5.3267e-01, 3.8830e-01, 1.0651e-02,  ..., 6.9373e-03,\n",
       "           8.6879e-04, 8.5450e-03],\n",
       "          ...,\n",
       "          [2.0656e-01, 3.2103e-02, 1.7923e-02,  ..., 1.4492e-02,\n",
       "           4.4455e-02, 7.7460e-02],\n",
       "          [2.1611e-01, 3.2225e-02, 1.1028e-02,  ..., 1.5333e-01,\n",
       "           2.6913e-01, 1.4508e-01],\n",
       "          [2.6735e-01, 1.0032e-03, 2.0564e-03,  ..., 1.1380e-02,\n",
       "           5.7908e-01, 1.1344e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0339e-01, 8.1019e-02, 1.0097e-01,  ..., 9.7538e-02,\n",
       "           5.3828e-02, 1.8947e-02],\n",
       "          [2.2246e-01, 3.8526e-01, 1.5870e-02,  ..., 8.7729e-02,\n",
       "           2.8435e-02, 1.2018e-01],\n",
       "          [4.6060e-01, 5.2075e-02, 2.7423e-02,  ..., 1.2428e-01,\n",
       "           2.3233e-02, 1.5110e-01],\n",
       "          ...,\n",
       "          [1.2855e-01, 1.9328e-02, 2.3319e-02,  ..., 5.1601e-01,\n",
       "           3.4028e-02, 4.8954e-02],\n",
       "          [6.2172e-01, 2.1584e-02, 3.8524e-02,  ..., 3.1858e-02,\n",
       "           1.7856e-02, 1.2544e-01],\n",
       "          [5.0826e-01, 4.8311e-02, 1.2281e-01,  ..., 5.4193e-02,\n",
       "           4.0987e-02, 2.7019e-02]],\n",
       "\n",
       "         [[8.1653e-01, 4.0707e-02, 1.1582e-02,  ..., 1.2267e-02,\n",
       "           1.8990e-02, 2.5669e-02],\n",
       "          [1.5613e-03, 1.2504e-02, 7.9499e-01,  ..., 1.1849e-02,\n",
       "           2.0084e-03, 1.0729e-03],\n",
       "          [6.8025e-04, 7.3150e-03, 8.0141e-03,  ..., 1.7690e-02,\n",
       "           4.0815e-03, 2.2859e-03],\n",
       "          ...,\n",
       "          [4.4340e-02, 4.0939e-04, 2.7700e-03,  ..., 3.4922e-02,\n",
       "           6.8045e-01, 1.6180e-01],\n",
       "          [5.7228e-03, 4.7485e-04, 2.6137e-04,  ..., 1.3313e-02,\n",
       "           4.1172e-02, 9.2261e-01],\n",
       "          [2.2922e-01, 5.1252e-03, 1.9378e-03,  ..., 6.2534e-02,\n",
       "           3.0529e-01, 3.7125e-01]],\n",
       "\n",
       "         [[9.1976e-01, 1.6615e-02, 3.1761e-03,  ..., 6.3526e-04,\n",
       "           4.7721e-07, 3.1466e-02],\n",
       "          [3.3743e-01, 5.6849e-02, 1.6448e-01,  ..., 2.3104e-02,\n",
       "           3.6241e-02, 1.0211e-01],\n",
       "          [1.8141e-01, 1.6135e-01, 1.3146e-02,  ..., 9.0758e-02,\n",
       "           2.7588e-02, 3.1694e-02],\n",
       "          ...,\n",
       "          [2.3269e-01, 4.2001e-02, 2.4801e-02,  ..., 1.5048e-01,\n",
       "           4.5522e-02, 1.5671e-01],\n",
       "          [4.8542e-01, 1.8033e-02, 8.9265e-03,  ..., 7.3368e-02,\n",
       "           1.5766e-03, 2.4077e-01],\n",
       "          [7.0698e-01, 2.7258e-02, 1.3515e-02,  ..., 9.7943e-03,\n",
       "           1.5543e-03, 1.6455e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[1.8431e-01, 1.3290e-02, 6.7208e-03,  ..., 3.0478e-02,\n",
       "           2.5396e-01, 4.5695e-02],\n",
       "          [1.5265e-01, 8.1307e-02, 4.3802e-01,  ..., 6.5591e-02,\n",
       "           2.6330e-02, 1.3977e-01],\n",
       "          [6.1719e-01, 1.0276e-02, 3.4607e-04,  ..., 7.0915e-03,\n",
       "           2.2122e-01, 8.7844e-02],\n",
       "          ...,\n",
       "          [2.6280e-01, 9.0000e-03, 1.4616e-02,  ..., 7.0594e-03,\n",
       "           3.9240e-01, 1.9637e-01],\n",
       "          [5.7311e-01, 2.7772e-02, 7.4068e-03,  ..., 3.0836e-02,\n",
       "           1.5802e-01, 7.7301e-02],\n",
       "          [4.5515e-01, 1.1660e-02, 7.3946e-03,  ..., 3.9740e-02,\n",
       "           1.7317e-01, 6.3174e-02]],\n",
       "\n",
       "         [[5.0408e-01, 7.4648e-02, 3.6248e-02,  ..., 2.2884e-02,\n",
       "           7.7847e-02, 1.0926e-01],\n",
       "          [1.4149e-01, 8.3416e-03, 8.4099e-01,  ..., 7.4188e-04,\n",
       "           1.4390e-03, 3.8247e-04],\n",
       "          [2.6690e-02, 1.0643e-02, 1.8101e-02,  ..., 1.8661e-03,\n",
       "           9.4557e-03, 1.7122e-03],\n",
       "          ...,\n",
       "          [6.2937e-01, 2.5761e-04, 4.2847e-03,  ..., 8.2733e-02,\n",
       "           1.2874e-01, 9.0684e-02],\n",
       "          [5.7733e-01, 6.6388e-05, 5.8857e-06,  ..., 3.1574e-03,\n",
       "           2.0636e-02, 3.9579e-01],\n",
       "          [9.2336e-01, 1.2448e-03, 3.1458e-04,  ..., 3.6416e-03,\n",
       "           5.3816e-03, 6.2649e-02]],\n",
       "\n",
       "         [[8.4887e-01, 1.2920e-02, 5.9361e-03,  ..., 1.7302e-02,\n",
       "           1.9093e-02, 3.3372e-02],\n",
       "          [4.1173e-01, 4.2957e-02, 3.1399e-02,  ..., 9.4791e-03,\n",
       "           5.8467e-02, 4.0224e-01],\n",
       "          [5.7543e-01, 1.0626e-02, 1.2105e-02,  ..., 1.8357e-02,\n",
       "           5.9638e-02, 2.8720e-01],\n",
       "          ...,\n",
       "          [6.4051e-01, 1.7733e-02, 1.8853e-02,  ..., 1.6620e-03,\n",
       "           2.7447e-02, 2.5707e-01],\n",
       "          [4.2808e-01, 1.1093e-01, 4.8760e-02,  ..., 1.0012e-02,\n",
       "           4.2035e-02, 3.0625e-01],\n",
       "          [7.5382e-01, 6.6387e-02, 8.7873e-03,  ..., 6.7548e-03,\n",
       "           1.8019e-02, 1.0093e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[2.1765e-01, 3.5757e-02, 3.5607e-02,  ..., 5.1958e-02,\n",
       "           1.4377e-01, 1.7292e-01],\n",
       "          [5.7503e-01, 2.5641e-02, 1.3301e-02,  ..., 2.0184e-02,\n",
       "           3.6526e-02, 2.1250e-01],\n",
       "          [7.2131e-01, 1.8422e-02, 3.2928e-03,  ..., 4.0255e-02,\n",
       "           2.3621e-02, 1.3999e-01],\n",
       "          ...,\n",
       "          [6.6633e-01, 1.8792e-02, 4.1112e-02,  ..., 3.3706e-03,\n",
       "           3.3035e-02, 1.5243e-01],\n",
       "          [3.0935e-01, 2.8273e-02, 5.7082e-02,  ..., 5.6428e-02,\n",
       "           9.0352e-02, 1.8038e-01],\n",
       "          [1.0943e-01, 4.2921e-02, 4.9392e-02,  ..., 7.6460e-02,\n",
       "           1.0166e-01, 1.7920e-01]],\n",
       "\n",
       "         [[6.2071e-01, 2.7109e-02, 2.4876e-02,  ..., 2.2698e-02,\n",
       "           7.6387e-02, 1.3347e-01],\n",
       "          [8.9506e-01, 1.9094e-02, 2.3246e-03,  ..., 3.9403e-03,\n",
       "           1.5046e-02, 2.6439e-02],\n",
       "          [6.3013e-01, 2.6941e-02, 5.6115e-03,  ..., 4.8789e-02,\n",
       "           1.9325e-02, 4.5804e-02],\n",
       "          ...,\n",
       "          [4.8367e-02, 4.8088e-02, 1.3703e-01,  ..., 6.2530e-02,\n",
       "           7.1279e-02, 9.1771e-02],\n",
       "          [2.7088e-01, 5.3722e-02, 4.6435e-02,  ..., 5.0153e-02,\n",
       "           8.4866e-02, 1.1211e-01],\n",
       "          [9.3943e-01, 2.3647e-03, 1.5306e-03,  ..., 4.3126e-03,\n",
       "           1.2519e-02, 2.6297e-02]],\n",
       "\n",
       "         [[3.2841e-01, 3.7403e-02, 4.9568e-02,  ..., 2.4927e-02,\n",
       "           5.4912e-02, 2.8192e-01],\n",
       "          [2.7372e-01, 1.2818e-01, 9.1417e-04,  ..., 8.5402e-03,\n",
       "           1.8156e-01, 3.8484e-01],\n",
       "          [3.0735e-01, 3.6008e-03, 2.0493e-01,  ..., 1.5732e-02,\n",
       "           5.8324e-02, 2.6874e-01],\n",
       "          ...,\n",
       "          [1.0123e-01, 5.6481e-03, 2.2625e-02,  ..., 6.7563e-01,\n",
       "           1.7522e-02, 7.5590e-02],\n",
       "          [3.0124e-01, 1.5229e-01, 7.6243e-02,  ..., 3.9777e-02,\n",
       "           2.8049e-02, 1.5557e-01],\n",
       "          [3.8354e-01, 5.6517e-02, 1.2804e-01,  ..., 3.1901e-02,\n",
       "           4.9701e-02, 1.1078e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[9.7682e-01, 2.8479e-04, 5.4206e-04,  ..., 3.8391e-05,\n",
       "           3.3452e-03, 1.8058e-02],\n",
       "          [2.2947e-06, 1.5244e-06, 9.9999e-01,  ..., 1.4935e-06,\n",
       "           3.2112e-08, 2.2900e-10],\n",
       "          [8.1196e-07, 8.2437e-08, 3.2214e-06,  ..., 7.1736e-10,\n",
       "           1.8343e-07, 6.9151e-09],\n",
       "          ...,\n",
       "          [5.4781e-07, 1.6798e-12, 3.4955e-09,  ..., 4.2845e-07,\n",
       "           9.9999e-01, 2.5396e-07],\n",
       "          [3.3965e-03, 1.6110e-06, 3.7322e-10,  ..., 8.3560e-07,\n",
       "           2.1843e-03, 9.9426e-01],\n",
       "          [9.9158e-01, 4.3225e-06, 2.4407e-05,  ..., 1.1477e-05,\n",
       "           5.3096e-04, 7.5873e-03]],\n",
       "\n",
       "         [[7.1960e-01, 8.5454e-03, 8.8304e-03,  ..., 3.6422e-03,\n",
       "           3.9673e-02, 1.6763e-01],\n",
       "          [8.4680e-01, 1.9404e-02, 1.7839e-02,  ..., 1.7662e-04,\n",
       "           1.3624e-02, 8.9342e-02],\n",
       "          [8.5609e-01, 3.6071e-02, 2.4314e-03,  ..., 3.2330e-05,\n",
       "           5.7760e-03, 6.3114e-02],\n",
       "          ...,\n",
       "          [1.4853e-02, 2.9353e-04, 5.1333e-04,  ..., 2.1529e-04,\n",
       "           9.0614e-03, 1.8400e-02],\n",
       "          [5.4964e-01, 6.4401e-03, 1.9503e-03,  ..., 3.5718e-02,\n",
       "           6.6910e-02, 1.3179e-01],\n",
       "          [7.9880e-01, 2.7043e-03, 1.0431e-03,  ..., 1.9992e-03,\n",
       "           2.8865e-02, 1.5280e-01]],\n",
       "\n",
       "         [[7.4550e-01, 1.9179e-02, 3.1147e-02,  ..., 1.1172e-02,\n",
       "           4.6415e-02, 9.6855e-02],\n",
       "          [5.0269e-01, 8.1599e-02, 2.9153e-02,  ..., 4.5627e-03,\n",
       "           6.3221e-02, 2.5701e-01],\n",
       "          [8.2746e-01, 5.1627e-03, 1.2681e-03,  ..., 1.4092e-03,\n",
       "           4.6315e-02, 9.6800e-02],\n",
       "          ...,\n",
       "          [4.9972e-01, 5.2620e-03, 7.7712e-03,  ..., 1.7373e-04,\n",
       "           1.8632e-01, 2.4055e-01],\n",
       "          [5.3803e-01, 3.8190e-02, 1.1875e-01,  ..., 7.6597e-02,\n",
       "           4.3907e-02, 7.4653e-02],\n",
       "          [5.1365e-01, 4.5204e-02, 6.6151e-02,  ..., 3.2513e-02,\n",
       "           7.3429e-02, 1.4135e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[8.1134e-01, 6.4008e-03, 1.0261e-02,  ..., 8.1118e-04,\n",
       "           1.8852e-02, 1.3677e-01],\n",
       "          [8.3224e-07, 1.2823e-06, 1.0000e+00,  ..., 3.1839e-07,\n",
       "           1.0228e-08, 9.0130e-11],\n",
       "          [8.3591e-07, 3.2015e-07, 1.1560e-05,  ..., 2.3425e-10,\n",
       "           6.4835e-08, 1.2509e-08],\n",
       "          ...,\n",
       "          [3.9779e-06, 2.8712e-11, 2.2081e-08,  ..., 4.4598e-07,\n",
       "           9.9997e-01, 5.6249e-06],\n",
       "          [8.8534e-04, 1.9939e-06, 4.6861e-10,  ..., 2.8811e-07,\n",
       "           9.0945e-04, 9.9812e-01],\n",
       "          [9.9559e-01, 1.1551e-06, 2.9696e-05,  ..., 6.2436e-05,\n",
       "           9.3417e-04, 3.2123e-03]],\n",
       "\n",
       "         [[6.5827e-01, 3.3270e-02, 1.6360e-02,  ..., 1.0635e-02,\n",
       "           3.9245e-02, 1.9755e-01],\n",
       "          [2.5772e-01, 3.7667e-01, 4.3130e-02,  ..., 4.5822e-03,\n",
       "           1.2121e-01, 1.4975e-01],\n",
       "          [5.6494e-01, 1.2144e-02, 1.6515e-02,  ..., 2.2083e-02,\n",
       "           4.8637e-02, 1.6087e-01],\n",
       "          ...,\n",
       "          [2.3694e-01, 1.6926e-02, 1.2475e-01,  ..., 9.4741e-02,\n",
       "           8.5875e-02, 9.7129e-02],\n",
       "          [7.4980e-01, 4.2163e-02, 1.2058e-02,  ..., 2.3517e-02,\n",
       "           2.3251e-02, 8.8475e-02],\n",
       "          [8.1117e-01, 1.1085e-02, 1.4350e-02,  ..., 4.0899e-03,\n",
       "           3.3036e-02, 8.8999e-02]],\n",
       "\n",
       "         [[9.5072e-01, 1.7348e-03, 1.2002e-03,  ..., 2.0835e-03,\n",
       "           1.0904e-02, 2.7406e-02],\n",
       "          [2.8173e-01, 1.5170e-02, 2.9548e-02,  ..., 1.0632e-01,\n",
       "           1.1764e-01, 6.6090e-02],\n",
       "          [7.6736e-02, 1.6487e-02, 9.1201e-03,  ..., 2.6643e-01,\n",
       "           3.7971e-02, 1.8381e-02],\n",
       "          ...,\n",
       "          [2.0306e-01, 9.2764e-02, 1.2308e-01,  ..., 8.4637e-02,\n",
       "           1.0034e-01, 3.4258e-02],\n",
       "          [6.9125e-01, 1.7516e-02, 1.0005e-02,  ..., 1.9138e-02,\n",
       "           6.7074e-02, 1.3549e-01],\n",
       "          [9.8465e-01, 4.6063e-04, 5.0880e-04,  ..., 9.6620e-04,\n",
       "           3.6219e-03, 7.3853e-03]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.5603e-01, 1.9289e-03, 1.0949e-03,  ..., 2.5398e-03,\n",
       "           2.7494e-02, 6.0262e-01],\n",
       "          [2.0339e-01, 3.0241e-01, 6.4850e-04,  ..., 4.3109e-04,\n",
       "           3.1510e-02, 4.5898e-01],\n",
       "          [3.3641e-01, 1.0216e-03, 6.9182e-02,  ..., 1.1541e-04,\n",
       "           1.9440e-02, 5.7312e-01],\n",
       "          ...,\n",
       "          [4.4849e-01, 6.1176e-04, 4.4418e-05,  ..., 6.4503e-02,\n",
       "           2.4440e-02, 4.6117e-01],\n",
       "          [4.0936e-01, 1.9171e-03, 9.1719e-04,  ..., 2.3093e-04,\n",
       "           3.9412e-02, 5.4479e-01],\n",
       "          [2.6982e-01, 2.0056e-03, 4.5464e-03,  ..., 3.0220e-03,\n",
       "           1.7509e-02, 6.8298e-01]],\n",
       "\n",
       "         [[4.1131e-01, 9.3718e-03, 5.5397e-03,  ..., 5.2526e-03,\n",
       "           5.7249e-02, 4.8823e-01],\n",
       "          [1.5912e-01, 1.7258e-02, 6.1785e-02,  ..., 8.6088e-02,\n",
       "           1.2157e-01, 1.6390e-01],\n",
       "          [2.1069e-01, 8.0961e-02, 8.0104e-02,  ..., 1.5365e-01,\n",
       "           1.2778e-02, 1.6238e-01],\n",
       "          ...,\n",
       "          [1.6537e-01, 8.1330e-02, 1.0475e-01,  ..., 2.3564e-02,\n",
       "           4.9196e-02, 1.8373e-01],\n",
       "          [7.9115e-02, 4.5247e-02, 1.2978e-02,  ..., 1.1186e-02,\n",
       "           4.4653e-02, 7.4686e-01],\n",
       "          [5.1393e-01, 3.8397e-03, 1.1529e-03,  ..., 1.0258e-03,\n",
       "           2.6898e-02, 4.4729e-01]],\n",
       "\n",
       "         [[9.9101e-02, 1.5730e-01, 6.4322e-02,  ..., 8.9133e-02,\n",
       "           7.2592e-02, 1.9027e-01],\n",
       "          [1.0827e-01, 2.8412e-01, 7.1833e-03,  ..., 2.6829e-03,\n",
       "           3.1780e-01, 2.0034e-01],\n",
       "          [2.6819e-01, 2.9299e-02, 3.8903e-03,  ..., 5.4211e-03,\n",
       "           1.7880e-01, 4.8194e-01],\n",
       "          ...,\n",
       "          [1.8226e-01, 2.1715e-02, 6.1890e-04,  ..., 2.7923e-04,\n",
       "           3.0024e-01, 4.4910e-01],\n",
       "          [3.0384e-01, 2.7699e-02, 3.0489e-03,  ..., 5.2261e-03,\n",
       "           3.2022e-01, 2.9267e-01],\n",
       "          [1.3346e-01, 4.6800e-03, 1.3585e-02,  ..., 9.5473e-03,\n",
       "           1.0617e-02, 7.7366e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[8.4613e-01, 4.6454e-03, 5.0583e-03,  ..., 1.2966e-03,\n",
       "           2.8282e-02, 1.0090e-01],\n",
       "          [6.0504e-02, 1.2851e-02, 8.0640e-01,  ..., 2.4633e-03,\n",
       "           7.2426e-04, 7.8459e-02],\n",
       "          [1.5548e-03, 2.3926e-04, 1.7395e-03,  ..., 2.3169e-04,\n",
       "           5.6827e-05, 1.0640e-02],\n",
       "          ...,\n",
       "          [6.7219e-01, 2.6227e-04, 2.1044e-04,  ..., 2.4813e-03,\n",
       "           1.8257e-01, 1.3318e-01],\n",
       "          [7.1862e-01, 1.2318e-03, 1.8198e-04,  ..., 3.1631e-03,\n",
       "           3.9203e-02, 2.3500e-01],\n",
       "          [8.8384e-01, 5.0305e-03, 3.8637e-03,  ..., 1.2708e-03,\n",
       "           2.5622e-02, 6.6993e-02]],\n",
       "\n",
       "         [[3.5121e-01, 3.4815e-03, 3.8586e-03,  ..., 1.4133e-03,\n",
       "           5.8523e-03, 6.2276e-01],\n",
       "          [2.1192e-01, 2.4807e-02, 1.3300e-01,  ..., 1.7784e-02,\n",
       "           4.2286e-02, 2.8469e-01],\n",
       "          [4.4924e-02, 1.8388e-02, 7.1665e-03,  ..., 4.0638e-02,\n",
       "           5.1764e-03, 7.0381e-02],\n",
       "          ...,\n",
       "          [3.7183e-01, 2.1752e-02, 1.5367e-01,  ..., 6.3584e-03,\n",
       "           5.4143e-02, 2.7511e-01],\n",
       "          [1.5324e-01, 9.4192e-02, 1.5878e-01,  ..., 3.6240e-02,\n",
       "           3.6810e-02, 2.1513e-01],\n",
       "          [3.0566e-01, 1.1918e-03, 1.7911e-03,  ..., 3.5909e-04,\n",
       "           3.8182e-03, 6.8192e-01]],\n",
       "\n",
       "         [[7.4563e-01, 1.0500e-02, 3.5781e-03,  ..., 4.1917e-03,\n",
       "           4.3996e-02, 1.7430e-01],\n",
       "          [7.3390e-01, 1.2460e-02, 6.4253e-03,  ..., 5.9654e-04,\n",
       "           2.4326e-02, 1.9670e-01],\n",
       "          [5.9268e-01, 2.4429e-01, 3.0384e-02,  ..., 2.1328e-04,\n",
       "           2.0366e-03, 1.1802e-01],\n",
       "          ...,\n",
       "          [3.1823e-02, 1.6786e-02, 1.8390e-02,  ..., 9.4731e-02,\n",
       "           2.5939e-02, 1.3486e-01],\n",
       "          [1.2616e-01, 2.6611e-02, 1.8222e-02,  ..., 2.4716e-01,\n",
       "           1.1808e-01, 2.4697e-01],\n",
       "          [8.2619e-01, 2.3582e-03, 1.0684e-03,  ..., 6.6670e-04,\n",
       "           1.1560e-02, 1.4938e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.0734e-02, 3.7787e-03, 3.1384e-03,  ..., 4.4494e-03,\n",
       "           1.7794e-02, 9.2492e-01],\n",
       "          [1.7753e-02, 2.5213e-02, 1.2793e-01,  ..., 7.9114e-02,\n",
       "           1.4317e-02, 5.8836e-01],\n",
       "          [3.5153e-03, 2.6561e-02, 7.9618e-02,  ..., 8.2593e-02,\n",
       "           8.6908e-03, 3.4806e-01],\n",
       "          ...,\n",
       "          [5.1994e-02, 1.0915e-01, 5.1459e-02,  ..., 7.1875e-02,\n",
       "           1.0321e-01, 4.4989e-01],\n",
       "          [2.9521e-02, 1.6740e-02, 1.8948e-02,  ..., 5.1029e-02,\n",
       "           1.6720e-01, 6.5677e-01],\n",
       "          [6.1216e-02, 7.9615e-04, 2.6396e-04,  ..., 3.3247e-03,\n",
       "           1.6480e-01, 7.6759e-01]],\n",
       "\n",
       "         [[2.8234e-01, 4.7768e-02, 1.1824e-01,  ..., 1.5989e-01,\n",
       "           1.2126e-01, 2.8220e-02],\n",
       "          [5.1082e-03, 5.0121e-02, 1.3565e-01,  ..., 2.7087e-01,\n",
       "           8.1835e-02, 9.4617e-02],\n",
       "          [1.2362e-02, 1.1508e-01, 1.1482e-01,  ..., 4.3694e-02,\n",
       "           9.0207e-02, 2.6842e-01],\n",
       "          ...,\n",
       "          [2.1883e-03, 4.7777e-02, 1.1667e-01,  ..., 4.2706e-02,\n",
       "           1.4178e-02, 2.2128e-01],\n",
       "          [1.4584e-02, 1.1088e-01, 1.3331e-01,  ..., 4.0832e-02,\n",
       "           1.6342e-01, 1.4904e-02],\n",
       "          [5.5040e-02, 2.9349e-02, 3.8036e-02,  ..., 8.0871e-02,\n",
       "           2.4417e-02, 6.5049e-01]],\n",
       "\n",
       "         [[8.0315e-02, 1.9990e-01, 6.5645e-02,  ..., 1.5391e-01,\n",
       "           1.3979e-01, 1.2674e-01],\n",
       "          [1.2557e-01, 6.3224e-02, 2.3152e-02,  ..., 2.2223e-02,\n",
       "           7.6892e-02, 4.7480e-01],\n",
       "          [1.1992e-01, 1.1401e-02, 6.2764e-03,  ..., 1.6868e-03,\n",
       "           3.5974e-02, 7.5872e-01],\n",
       "          ...,\n",
       "          [1.4067e-01, 3.8611e-03, 2.2967e-03,  ..., 1.6670e-03,\n",
       "           2.4080e-02, 6.8670e-01],\n",
       "          [2.8862e-01, 6.7539e-02, 2.9403e-02,  ..., 5.9132e-02,\n",
       "           8.4207e-02, 1.8920e-01],\n",
       "          [1.1660e-01, 1.7563e-02, 4.6846e-03,  ..., 1.5391e-02,\n",
       "           2.6335e-02, 7.9576e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[2.1354e-02, 1.7045e-02, 9.0009e-02,  ..., 2.6351e-02,\n",
       "           6.0398e-03, 5.5062e-01],\n",
       "          [1.6686e-01, 6.8558e-03, 9.4230e-02,  ..., 4.2172e-02,\n",
       "           2.5050e-02, 4.5729e-01],\n",
       "          [8.7253e-02, 5.7412e-02, 8.5214e-02,  ..., 3.8981e-02,\n",
       "           2.9468e-02, 5.0137e-01],\n",
       "          ...,\n",
       "          [1.3148e-01, 2.5235e-02, 3.2204e-02,  ..., 1.0994e-02,\n",
       "           7.4157e-02, 5.3175e-01],\n",
       "          [1.2757e-01, 4.0639e-02, 2.0085e-02,  ..., 1.6817e-02,\n",
       "           8.4377e-02, 4.9045e-01],\n",
       "          [3.6549e-02, 1.5000e-03, 2.9747e-03,  ..., 7.8555e-03,\n",
       "           1.0093e-02, 9.0803e-01]],\n",
       "\n",
       "         [[1.7855e-02, 7.1658e-03, 6.6841e-03,  ..., 2.3010e-03,\n",
       "           1.6840e-02, 9.3367e-01],\n",
       "          [6.1270e-02, 3.6212e-02, 5.0460e-02,  ..., 3.9068e-03,\n",
       "           3.8340e-02, 7.8916e-01],\n",
       "          [1.4440e-02, 1.5388e-01, 1.0708e-02,  ..., 5.6737e-04,\n",
       "           5.4221e-03, 8.0973e-01],\n",
       "          ...,\n",
       "          [3.1744e-02, 2.3636e-01, 5.8613e-02,  ..., 3.3520e-02,\n",
       "           6.9368e-03, 2.6586e-01],\n",
       "          [3.3900e-02, 2.0367e-01, 1.1772e-01,  ..., 8.0534e-02,\n",
       "           2.9598e-02, 3.6135e-02],\n",
       "          [1.1948e-02, 1.9410e-03, 1.8637e-03,  ..., 1.5741e-03,\n",
       "           7.9718e-03, 9.6681e-01]],\n",
       "\n",
       "         [[8.9227e-02, 2.6651e-01, 4.7759e-02,  ..., 1.3612e-02,\n",
       "           7.9639e-02, 4.2071e-01],\n",
       "          [3.2540e-02, 2.8029e-02, 2.0425e-01,  ..., 5.8878e-04,\n",
       "           1.0009e-03, 7.1010e-01],\n",
       "          [2.2878e-02, 7.8087e-03, 1.7249e-02,  ..., 4.4673e-04,\n",
       "           9.7083e-04, 3.5578e-01],\n",
       "          ...,\n",
       "          [5.8843e-02, 8.1185e-03, 8.8381e-04,  ..., 1.8194e-02,\n",
       "           3.2076e-01, 5.1796e-01],\n",
       "          [3.8794e-02, 1.2148e-01, 2.9353e-03,  ..., 8.7117e-03,\n",
       "           1.6002e-01, 6.5359e-01],\n",
       "          [1.6727e-02, 1.6939e-02, 7.8083e-03,  ..., 4.4192e-03,\n",
       "           1.5722e-02, 9.1669e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.1217e-02, 3.4722e-02, 7.4727e-02,  ..., 1.3970e-03,\n",
       "           1.1722e-02, 7.7625e-01],\n",
       "          [5.7648e-02, 3.3900e-02, 8.2390e-02,  ..., 7.0452e-03,\n",
       "           2.1062e-02, 6.0276e-01],\n",
       "          [1.7446e-02, 1.9200e-02, 1.0730e-02,  ..., 5.1310e-03,\n",
       "           2.0093e-02, 6.0910e-01],\n",
       "          ...,\n",
       "          [1.8033e-02, 4.1747e-03, 8.5930e-04,  ..., 3.3660e-02,\n",
       "           6.3551e-02, 8.6364e-01],\n",
       "          [6.7689e-02, 3.8981e-02, 3.1534e-02,  ..., 8.6632e-03,\n",
       "           1.0069e-01, 7.2848e-01],\n",
       "          [1.9632e-02, 3.3860e-03, 4.0427e-03,  ..., 4.3354e-03,\n",
       "           1.3189e-02, 9.3544e-01]],\n",
       "\n",
       "         [[2.5054e-02, 4.6703e-02, 1.2805e-02,  ..., 1.3922e-02,\n",
       "           1.1543e-02, 8.5341e-01],\n",
       "          [6.7859e-03, 5.0676e-03, 6.8575e-03,  ..., 2.4839e-04,\n",
       "           1.4646e-03, 9.7559e-01],\n",
       "          [1.2794e-03, 1.0688e-02, 3.1178e-03,  ..., 1.7946e-04,\n",
       "           1.9089e-04, 9.2301e-01],\n",
       "          ...,\n",
       "          [5.5302e-03, 7.2958e-04, 1.0120e-03,  ..., 4.1875e-03,\n",
       "           7.6304e-03, 9.1963e-01],\n",
       "          [2.8350e-02, 8.7362e-03, 2.5137e-03,  ..., 4.2493e-02,\n",
       "           4.4020e-02, 8.1667e-01],\n",
       "          [6.8296e-03, 2.0462e-03, 5.3204e-04,  ..., 7.4292e-04,\n",
       "           3.0932e-03, 9.8278e-01]],\n",
       "\n",
       "         [[2.2930e-02, 1.4142e-02, 7.7839e-03,  ..., 5.3184e-03,\n",
       "           3.2207e-02, 8.9483e-01],\n",
       "          [1.0322e-01, 7.2122e-02, 1.0221e-01,  ..., 8.0318e-02,\n",
       "           1.4780e-01, 2.3751e-01],\n",
       "          [4.9369e-02, 8.2461e-02, 1.1832e-01,  ..., 1.2992e-01,\n",
       "           5.8513e-02, 3.0824e-01],\n",
       "          ...,\n",
       "          [1.5400e-02, 4.1572e-02, 3.8798e-02,  ..., 3.4236e-02,\n",
       "           1.9422e-02, 7.3783e-01],\n",
       "          [1.7099e-01, 7.3791e-02, 7.5864e-02,  ..., 6.8491e-02,\n",
       "           1.5082e-01, 2.8936e-01],\n",
       "          [2.2960e-02, 2.8993e-03, 4.2444e-04,  ..., 1.2262e-03,\n",
       "           5.1219e-03, 9.5957e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.0934e-02, 1.1012e-01, 1.0285e-02,  ..., 3.7180e-03,\n",
       "           9.9437e-02, 6.9802e-01],\n",
       "          [1.4572e-03, 1.0111e-02, 6.1257e-01,  ..., 1.3175e-03,\n",
       "           3.4450e-04, 3.0965e-01],\n",
       "          [1.9698e-03, 4.6337e-03, 1.6252e-02,  ..., 2.2645e-04,\n",
       "           6.3988e-04, 1.6286e-01],\n",
       "          ...,\n",
       "          [4.5444e-02, 8.5736e-04, 4.3856e-05,  ..., 5.8136e-03,\n",
       "           4.3075e-01, 5.0970e-01],\n",
       "          [1.9284e-02, 4.5820e-02, 1.2152e-03,  ..., 2.3462e-03,\n",
       "           1.2252e-01, 8.0589e-01],\n",
       "          [2.4590e-01, 1.0037e-02, 2.1702e-03,  ..., 1.3149e-02,\n",
       "           6.6893e-02, 6.3863e-01]],\n",
       "\n",
       "         [[3.2992e-02, 1.7978e-02, 6.2614e-03,  ..., 6.6961e-02,\n",
       "           7.5937e-02, 7.4690e-01],\n",
       "          [2.9952e-01, 3.6190e-02, 2.7942e-02,  ..., 1.2341e-03,\n",
       "           3.5724e-02, 5.8523e-01],\n",
       "          [9.5822e-02, 4.7417e-01, 4.1873e-02,  ..., 2.2513e-04,\n",
       "           4.4216e-03, 3.5268e-01],\n",
       "          ...,\n",
       "          [1.6572e-02, 1.0487e-02, 3.4012e-03,  ..., 3.7580e-02,\n",
       "           1.3475e-02, 4.3411e-01],\n",
       "          [2.4154e-02, 4.6039e-03, 3.0916e-03,  ..., 4.8120e-01,\n",
       "           1.1734e-01, 2.0348e-01],\n",
       "          [1.8832e-02, 6.0159e-03, 2.9240e-03,  ..., 5.5581e-03,\n",
       "           1.7495e-02, 9.2902e-01]],\n",
       "\n",
       "         [[2.2106e-02, 1.1721e-02, 3.5943e-03,  ..., 6.5786e-03,\n",
       "           5.0473e-02, 8.5953e-01],\n",
       "          [3.9787e-02, 1.0720e-02, 3.0310e-02,  ..., 2.8475e-02,\n",
       "           1.7035e-01, 3.6562e-01],\n",
       "          [1.1358e-02, 1.9811e-03, 4.0462e-03,  ..., 3.7471e-03,\n",
       "           1.6273e-02, 7.3731e-02],\n",
       "          ...,\n",
       "          [1.8698e-02, 1.2840e-03, 1.7910e-04,  ..., 9.3412e-04,\n",
       "           1.0020e-02, 9.6507e-01],\n",
       "          [5.7048e-02, 2.7877e-02, 5.4042e-03,  ..., 7.1379e-03,\n",
       "           1.4165e-01, 7.1210e-01],\n",
       "          [1.4132e-02, 9.4541e-03, 2.3072e-03,  ..., 8.0755e-03,\n",
       "           1.0249e-02, 9.3664e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.6383e-03, 4.4884e-01, 1.4421e-01,  ..., 9.3498e-02,\n",
       "           3.1991e-02, 6.1717e-02],\n",
       "          [3.6662e-03, 1.9729e-02, 2.5015e-03,  ..., 1.7714e-03,\n",
       "           2.4540e-03, 9.6315e-01],\n",
       "          [1.9042e-03, 6.5637e-03, 5.0183e-02,  ..., 1.0064e-03,\n",
       "           1.4049e-03, 9.0280e-01],\n",
       "          ...,\n",
       "          [3.0952e-03, 2.0501e-03, 2.7689e-03,  ..., 1.3203e-02,\n",
       "           1.0633e-03, 9.4089e-01],\n",
       "          [1.7060e-02, 2.4213e-01, 1.2380e-01,  ..., 5.4246e-02,\n",
       "           6.0014e-02, 1.4405e-01],\n",
       "          [3.1269e-03, 1.6844e-03, 1.4670e-03,  ..., 3.6745e-03,\n",
       "           2.5585e-03, 9.8096e-01]],\n",
       "\n",
       "         [[4.5476e-02, 9.2416e-02, 5.9101e-02,  ..., 9.8226e-02,\n",
       "           6.6659e-02, 2.9214e-01],\n",
       "          [7.9606e-02, 3.9547e-02, 3.8386e-02,  ..., 5.5337e-02,\n",
       "           1.0110e-01, 5.4077e-01],\n",
       "          [5.8411e-02, 6.9098e-02, 3.6969e-02,  ..., 4.5958e-02,\n",
       "           4.6292e-02, 6.0016e-01],\n",
       "          ...,\n",
       "          [7.4789e-03, 1.0691e-02, 5.5003e-02,  ..., 3.5646e-03,\n",
       "           4.1454e-03, 8.9640e-01],\n",
       "          [2.1888e-02, 3.7509e-01, 1.0435e-01,  ..., 4.4185e-02,\n",
       "           5.7562e-02, 2.0146e-01],\n",
       "          [8.5390e-03, 4.6612e-03, 6.4154e-03,  ..., 3.5036e-03,\n",
       "           6.4900e-03, 9.5556e-01]],\n",
       "\n",
       "         [[3.5991e-03, 3.9565e-02, 1.1021e-02,  ..., 5.4851e-03,\n",
       "           6.7529e-01, 5.8896e-02],\n",
       "          [3.9254e-02, 6.2357e-02, 2.4590e-02,  ..., 2.4008e-02,\n",
       "           2.4164e-02, 8.0121e-01],\n",
       "          [2.0711e-02, 1.4425e-02, 6.0692e-03,  ..., 1.0123e-02,\n",
       "           9.4958e-03, 9.1841e-01],\n",
       "          ...,\n",
       "          [1.5290e-02, 1.4347e-02, 1.9317e-02,  ..., 2.6409e-02,\n",
       "           2.4208e-02, 4.9869e-01],\n",
       "          [3.6802e-02, 9.3892e-02, 5.8604e-02,  ..., 5.5808e-02,\n",
       "           1.3310e-01, 2.1887e-01],\n",
       "          [1.8631e-02, 6.7443e-03, 4.3003e-03,  ..., 8.6402e-03,\n",
       "           3.3638e-02, 8.9630e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[6.0588e-02, 2.0143e-02, 1.2995e-03,  ..., 3.3507e-03,\n",
       "           1.0868e-01, 7.6770e-01],\n",
       "          [1.5661e-01, 2.4063e-02, 5.2312e-03,  ..., 2.4940e-03,\n",
       "           1.2379e-01, 6.7969e-01],\n",
       "          [2.5929e-01, 8.7443e-02, 1.5998e-02,  ..., 1.0396e-03,\n",
       "           4.1351e-02, 5.6861e-01],\n",
       "          ...,\n",
       "          [4.0196e-03, 7.9643e-03, 2.9254e-03,  ..., 4.6660e-02,\n",
       "           5.4178e-03, 1.9671e-01],\n",
       "          [1.4900e-02, 7.4453e-03, 1.1005e-03,  ..., 3.8089e-02,\n",
       "           1.1007e-01, 6.8626e-01],\n",
       "          [1.8616e-02, 1.5546e-02, 3.6127e-03,  ..., 3.4953e-03,\n",
       "           2.8230e-02, 9.0384e-01]],\n",
       "\n",
       "         [[1.2270e-02, 6.1896e-02, 8.7856e-03,  ..., 6.4913e-02,\n",
       "           1.7108e-02, 7.9273e-01],\n",
       "          [5.3794e-03, 9.1860e-02, 7.1003e-03,  ..., 2.4703e-03,\n",
       "           9.8791e-03, 8.8082e-01],\n",
       "          [2.6209e-03, 1.0475e-01, 2.9626e-02,  ..., 1.8982e-03,\n",
       "           2.3326e-03, 8.5543e-01],\n",
       "          ...,\n",
       "          [5.4132e-03, 3.8886e-03, 2.3053e-03,  ..., 2.3758e-03,\n",
       "           6.3472e-03, 5.5558e-01],\n",
       "          [2.3788e-02, 6.7102e-02, 9.8689e-03,  ..., 2.0659e-01,\n",
       "           3.1296e-02, 5.4833e-01],\n",
       "          [9.1321e-03, 1.4260e-02, 4.2674e-03,  ..., 5.2196e-03,\n",
       "           1.1920e-02, 9.3491e-01]],\n",
       "\n",
       "         [[3.9630e-03, 2.0455e-02, 5.2034e-03,  ..., 1.7912e-02,\n",
       "           8.9948e-01, 4.1238e-02],\n",
       "          [1.1413e-01, 8.6406e-03, 1.3369e-03,  ..., 5.0706e-04,\n",
       "           5.9684e-02, 8.1231e-01],\n",
       "          [1.7354e-02, 8.4083e-01, 3.0916e-02,  ..., 1.5711e-04,\n",
       "           1.9653e-03, 4.6981e-02],\n",
       "          ...,\n",
       "          [5.8575e-03, 1.4921e-03, 6.7063e-05,  ..., 1.6032e-02,\n",
       "           1.4894e-02, 6.2744e-01],\n",
       "          [1.4243e-03, 7.7827e-04, 2.6072e-04,  ..., 3.7212e-01,\n",
       "           2.8907e-01, 3.1941e-01],\n",
       "          [6.2376e-03, 1.0519e-02, 4.2429e-03,  ..., 1.1307e-02,\n",
       "           1.9672e-02, 9.0423e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.3091e-03, 2.3362e-01, 5.1213e-02,  ..., 3.1504e-02,\n",
       "           5.7437e-02, 5.2860e-01],\n",
       "          [1.6075e-02, 2.9503e-02, 1.3275e-02,  ..., 1.6153e-02,\n",
       "           4.8438e-02, 8.4688e-01],\n",
       "          [6.7901e-02, 1.4390e-01, 3.7171e-02,  ..., 2.0328e-02,\n",
       "           8.6041e-02, 5.7926e-01],\n",
       "          ...,\n",
       "          [5.2919e-02, 1.2877e-01, 6.1079e-02,  ..., 3.0313e-02,\n",
       "           6.9391e-02, 4.0169e-01],\n",
       "          [9.0653e-03, 3.6606e-01, 5.2627e-02,  ..., 2.1941e-02,\n",
       "           9.5810e-02, 3.6585e-01],\n",
       "          [9.8773e-03, 6.0783e-03, 2.0798e-03,  ..., 2.6614e-03,\n",
       "           2.5608e-02, 9.4413e-01]],\n",
       "\n",
       "         [[1.2322e-01, 1.1530e-01, 2.9135e-01,  ..., 1.1243e-02,\n",
       "           9.0509e-02, 2.1572e-01],\n",
       "          [2.8426e-02, 5.0794e-02, 2.7781e-01,  ..., 4.5338e-03,\n",
       "           1.6011e-02, 3.8040e-02],\n",
       "          [2.0313e-02, 1.9963e-02, 5.4932e-02,  ..., 3.0624e-03,\n",
       "           2.1930e-02, 7.2177e-02],\n",
       "          ...,\n",
       "          [5.0444e-03, 6.2672e-03, 9.5591e-04,  ..., 1.2107e-02,\n",
       "           1.9129e-03, 9.6275e-01],\n",
       "          [5.5255e-02, 1.2897e-01, 1.4143e-01,  ..., 1.2075e-02,\n",
       "           4.0896e-02, 5.3927e-01],\n",
       "          [1.3292e-02, 5.2110e-03, 3.6080e-03,  ..., 5.0385e-03,\n",
       "           5.3691e-03, 9.5516e-01]],\n",
       "\n",
       "         [[1.6106e-01, 1.6273e-01, 7.5818e-02,  ..., 1.7710e-02,\n",
       "           6.5943e-02, 4.8117e-01],\n",
       "          [5.1831e-03, 2.0660e-02, 4.5106e-01,  ..., 9.7254e-03,\n",
       "           2.6614e-03, 1.2163e-01],\n",
       "          [2.1573e-02, 7.9256e-02, 8.5463e-02,  ..., 9.5944e-04,\n",
       "           4.6174e-03, 4.6007e-01],\n",
       "          ...,\n",
       "          [9.5543e-03, 4.1518e-03, 4.5612e-04,  ..., 1.2533e-02,\n",
       "           6.7962e-02, 8.8665e-01],\n",
       "          [1.2953e-01, 3.7832e-01, 3.2632e-02,  ..., 7.7685e-03,\n",
       "           1.6622e-02, 4.1912e-01],\n",
       "          [2.5235e-02, 1.8167e-02, 9.6138e-03,  ..., 1.0558e-02,\n",
       "           2.8027e-02, 8.6214e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.3757e-02, 2.1798e-02, 9.5725e-03,  ..., 1.3815e-02,\n",
       "           2.2176e-02, 8.8228e-01],\n",
       "          [4.2456e-02, 3.6507e-02, 1.8633e-02,  ..., 1.1092e-02,\n",
       "           4.4214e-02, 7.9924e-01],\n",
       "          [9.7496e-02, 1.5868e-01, 5.1726e-02,  ..., 1.3177e-03,\n",
       "           3.9830e-02, 6.2393e-01],\n",
       "          ...,\n",
       "          [6.4204e-03, 8.3885e-03, 3.0135e-03,  ..., 8.8663e-02,\n",
       "           6.5072e-03, 4.9511e-01],\n",
       "          [1.6754e-02, 1.5875e-02, 6.9158e-03,  ..., 1.3884e-02,\n",
       "           5.1095e-02, 8.0210e-01],\n",
       "          [2.0145e-02, 1.4410e-02, 6.2184e-03,  ..., 5.9943e-03,\n",
       "           2.9601e-02, 8.9607e-01]],\n",
       "\n",
       "         [[2.1623e-02, 1.1317e-02, 1.4206e-02,  ..., 1.1352e-01,\n",
       "           9.4072e-03, 6.6940e-01],\n",
       "          [1.4412e-02, 7.9752e-03, 6.1346e-02,  ..., 9.8535e-02,\n",
       "           1.5115e-02, 2.7930e-01],\n",
       "          [2.1060e-02, 4.8280e-02, 1.7507e-02,  ..., 1.3103e-02,\n",
       "           5.6331e-03, 4.7492e-01],\n",
       "          ...,\n",
       "          [1.0117e-02, 1.8118e-03, 1.6980e-03,  ..., 6.6838e-02,\n",
       "           5.5931e-03, 8.8875e-01],\n",
       "          [9.4174e-02, 4.7752e-02, 2.4850e-02,  ..., 1.0764e-01,\n",
       "           4.9846e-02, 6.1082e-01],\n",
       "          [2.1817e-02, 4.4200e-03, 2.4253e-03,  ..., 6.1294e-03,\n",
       "           9.7545e-03, 9.4426e-01]],\n",
       "\n",
       "         [[2.2315e-03, 2.3632e-03, 7.7268e-04,  ..., 4.2007e-03,\n",
       "           9.6411e-01, 1.7378e-02],\n",
       "          [2.1698e-02, 2.7505e-02, 4.9019e-03,  ..., 9.9787e-04,\n",
       "           1.4923e-02, 9.2330e-01],\n",
       "          [1.6331e-02, 3.8192e-01, 4.0007e-02,  ..., 4.3669e-04,\n",
       "           2.4669e-03, 5.4327e-01],\n",
       "          ...,\n",
       "          [8.5052e-03, 8.1371e-03, 1.1700e-03,  ..., 2.1879e-02,\n",
       "           2.8076e-02, 6.3018e-01],\n",
       "          [3.8189e-02, 1.8260e-02, 3.0244e-03,  ..., 1.7056e-01,\n",
       "           6.1988e-02, 6.6088e-01],\n",
       "          [2.0555e-02, 8.1630e-03, 3.8978e-03,  ..., 7.0711e-03,\n",
       "           4.2004e-02, 9.0268e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[8.2082e-02, 3.8448e-02, 1.1849e-02,  ..., 2.0978e-02,\n",
       "           2.7245e-02, 7.7428e-01],\n",
       "          [3.1809e-01, 3.2318e-02, 6.2935e-03,  ..., 6.6455e-02,\n",
       "           1.0617e-01, 2.7189e-01],\n",
       "          [3.6161e-01, 5.6018e-02, 1.6628e-02,  ..., 1.5303e-02,\n",
       "           5.9584e-02, 3.6935e-01],\n",
       "          ...,\n",
       "          [4.3958e-02, 8.9718e-03, 4.8059e-04,  ..., 2.9801e-02,\n",
       "           7.5235e-03, 8.9929e-01],\n",
       "          [1.4094e-01, 2.0325e-02, 8.7280e-03,  ..., 1.2902e-02,\n",
       "           4.4995e-02, 7.4096e-01],\n",
       "          [2.5220e-02, 5.2226e-03, 1.1934e-03,  ..., 5.6824e-03,\n",
       "           5.7769e-03, 9.4877e-01]],\n",
       "\n",
       "         [[3.3681e-02, 4.8870e-02, 6.2938e-02,  ..., 4.6147e-02,\n",
       "           2.0500e-02, 5.5529e-01],\n",
       "          [2.4106e-02, 1.0535e-02, 3.7159e-03,  ..., 5.2137e-03,\n",
       "           8.5689e-02, 8.5102e-01],\n",
       "          [1.1014e-02, 1.8879e-02, 1.6136e-02,  ..., 2.5370e-03,\n",
       "           5.2788e-02, 8.7427e-01],\n",
       "          ...,\n",
       "          [6.9342e-03, 9.6869e-03, 2.4661e-03,  ..., 3.7956e-03,\n",
       "           3.2371e-02, 9.3804e-01],\n",
       "          [5.2275e-02, 2.4089e-02, 1.1575e-02,  ..., 3.9626e-02,\n",
       "           8.8150e-02, 6.7334e-01],\n",
       "          [1.2806e-02, 3.4621e-02, 2.5188e-02,  ..., 2.0580e-02,\n",
       "           8.0523e-02, 7.3995e-01]],\n",
       "\n",
       "         [[1.0950e-02, 8.4278e-02, 1.5646e-02,  ..., 6.0208e-03,\n",
       "           7.4255e-03, 8.4494e-01],\n",
       "          [1.0422e-02, 2.6980e-02, 4.1777e-01,  ..., 4.4214e-04,\n",
       "           7.4512e-03, 5.2037e-01],\n",
       "          [3.6400e-03, 3.3997e-02, 9.1886e-02,  ..., 6.6842e-05,\n",
       "           2.1055e-03, 4.4165e-01],\n",
       "          ...,\n",
       "          [6.7400e-03, 6.7198e-04, 5.8662e-05,  ..., 5.1628e-03,\n",
       "           1.3712e-02, 9.7162e-01],\n",
       "          [2.3878e-03, 3.5072e-01, 1.6850e-02,  ..., 9.6372e-03,\n",
       "           1.8791e-03, 5.9629e-01],\n",
       "          [1.1455e-02, 1.7150e-02, 7.6054e-03,  ..., 9.6054e-03,\n",
       "           9.9496e-03, 9.1710e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.2862e-02, 1.4870e-02, 5.4059e-02,  ..., 1.3564e-01,\n",
       "           1.3527e-02, 6.3449e-01],\n",
       "          [9.3238e-02, 4.3639e-02, 2.9896e-01,  ..., 4.3571e-02,\n",
       "           5.2893e-02, 4.1558e-02],\n",
       "          [1.3018e-01, 1.9912e-02, 8.6523e-02,  ..., 2.3343e-02,\n",
       "           1.1497e-01, 3.3371e-01],\n",
       "          ...,\n",
       "          [9.3694e-02, 1.5831e-02, 1.3577e-03,  ..., 2.7941e-02,\n",
       "           1.0045e-01, 7.0525e-01],\n",
       "          [8.3584e-02, 1.9158e-02, 3.6698e-02,  ..., 5.5211e-02,\n",
       "           1.8977e-02, 7.0212e-01],\n",
       "          [1.8321e-02, 7.6834e-03, 4.6339e-03,  ..., 1.9129e-02,\n",
       "           1.7815e-02, 9.0382e-01]],\n",
       "\n",
       "         [[2.4341e-03, 2.4086e-02, 5.5747e-03,  ..., 1.7718e-02,\n",
       "           5.3865e-03, 9.3069e-01],\n",
       "          [1.0841e-02, 1.3852e-01, 4.7947e-02,  ..., 3.5209e-02,\n",
       "           4.5918e-02, 4.9923e-01],\n",
       "          [5.6846e-03, 4.2238e-01, 3.7271e-02,  ..., 9.2328e-03,\n",
       "           1.0600e-02, 4.5851e-01],\n",
       "          ...,\n",
       "          [1.3182e-02, 8.2846e-02, 6.4915e-03,  ..., 9.5657e-02,\n",
       "           1.7583e-02, 7.3114e-01],\n",
       "          [1.9473e-03, 3.6741e-02, 9.1943e-03,  ..., 1.4584e-02,\n",
       "           6.6558e-03, 8.9637e-01],\n",
       "          [2.6119e-03, 1.1932e-02, 2.3007e-03,  ..., 8.5813e-03,\n",
       "           3.8624e-03, 9.6381e-01]],\n",
       "\n",
       "         [[3.1183e-02, 9.0184e-03, 5.0257e-03,  ..., 5.5062e-02,\n",
       "           5.8750e-02, 7.7947e-01],\n",
       "          [5.6899e-02, 4.9613e-02, 1.4466e-02,  ..., 7.4209e-03,\n",
       "           1.3449e-01, 7.1722e-01],\n",
       "          [1.1010e-01, 1.1973e-01, 2.6504e-02,  ..., 1.5005e-03,\n",
       "           1.5865e-01, 5.6325e-01],\n",
       "          ...,\n",
       "          [3.2487e-02, 1.7318e-01, 9.8826e-02,  ..., 2.0788e-02,\n",
       "           2.6247e-02, 2.7570e-01],\n",
       "          [2.9877e-02, 3.3669e-02, 4.8904e-03,  ..., 3.9959e-03,\n",
       "           5.1410e-02, 8.2569e-01],\n",
       "          [1.3060e-02, 1.3068e-02, 5.0455e-03,  ..., 6.0115e-03,\n",
       "           2.3722e-02, 9.0981e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[4.3705e-02, 1.7806e-01, 9.3461e-02,  ..., 1.3796e-02,\n",
       "           8.1808e-02, 5.0577e-01],\n",
       "          [3.7349e-02, 2.2191e-01, 7.5465e-02,  ..., 5.4634e-03,\n",
       "           1.1237e-02, 6.0718e-01],\n",
       "          [1.6757e-02, 4.6256e-02, 3.1438e-01,  ..., 3.1110e-03,\n",
       "           1.2624e-02, 5.0665e-01],\n",
       "          ...,\n",
       "          [3.4817e-03, 1.9259e-03, 7.4172e-04,  ..., 1.4646e-01,\n",
       "           1.2726e-03, 8.3624e-01],\n",
       "          [2.0273e-02, 2.0288e-02, 2.3904e-02,  ..., 5.5169e-03,\n",
       "           1.9185e-02, 8.9025e-01],\n",
       "          [3.1838e-03, 3.9491e-02, 1.2623e-02,  ..., 1.3266e-02,\n",
       "           5.7225e-04, 9.0397e-01]],\n",
       "\n",
       "         [[1.4113e-02, 1.8918e-02, 2.7138e-02,  ..., 8.9770e-03,\n",
       "           2.7369e-02, 8.6212e-01],\n",
       "          [1.4338e-01, 1.8567e-02, 5.4950e-02,  ..., 1.0085e-02,\n",
       "           4.8946e-03, 7.2365e-01],\n",
       "          [8.2834e-02, 2.7479e-02, 3.3173e-02,  ..., 2.1091e-03,\n",
       "           1.9772e-03, 7.2236e-01],\n",
       "          ...,\n",
       "          [8.1158e-02, 2.1453e-02, 1.3150e-02,  ..., 1.3805e-01,\n",
       "           1.4260e-02, 7.0097e-01],\n",
       "          [2.2245e-01, 3.0865e-02, 2.0271e-02,  ..., 3.0527e-02,\n",
       "           1.0666e-02, 4.9334e-01],\n",
       "          [9.9336e-02, 3.3100e-02, 2.5240e-02,  ..., 4.3070e-02,\n",
       "           1.6244e-02, 7.3422e-01]],\n",
       "\n",
       "         [[1.1884e-02, 4.1511e-03, 3.0358e-03,  ..., 7.8669e-03,\n",
       "           2.2727e-02, 9.2852e-01],\n",
       "          [2.4701e-02, 7.2868e-03, 9.3735e-03,  ..., 2.8365e-02,\n",
       "           3.6862e-02, 8.4903e-01],\n",
       "          [1.6570e-01, 5.1694e-02, 5.8094e-02,  ..., 5.0912e-03,\n",
       "           8.0042e-02, 5.4861e-01],\n",
       "          ...,\n",
       "          [5.4283e-02, 1.4524e-01, 6.2350e-02,  ..., 1.4052e-02,\n",
       "           5.6199e-02, 4.0979e-01],\n",
       "          [4.9529e-02, 5.2901e-02, 2.2237e-02,  ..., 9.8713e-03,\n",
       "           4.6003e-02, 7.3854e-01],\n",
       "          [2.7127e-02, 7.4119e-02, 4.1172e-02,  ..., 3.5895e-02,\n",
       "           5.0614e-02, 6.5376e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[2.8875e-02, 3.6334e-02, 1.3394e-02,  ..., 1.3579e-02,\n",
       "           2.2484e-02, 8.5117e-01],\n",
       "          [4.7232e-02, 9.7166e-02, 2.4101e-02,  ..., 8.8076e-03,\n",
       "           3.5076e-02, 7.3961e-01],\n",
       "          [1.7778e-02, 8.6750e-02, 1.6404e-02,  ..., 2.3414e-03,\n",
       "           5.3179e-02, 7.8733e-01],\n",
       "          ...,\n",
       "          [2.2345e-02, 2.2353e-02, 3.5596e-03,  ..., 7.1806e-02,\n",
       "           2.2224e-02, 6.2716e-01],\n",
       "          [2.0882e-02, 1.1595e-02, 1.0755e-02,  ..., 3.0392e-02,\n",
       "           3.6032e-02, 8.5735e-01],\n",
       "          [4.6696e-02, 6.1862e-02, 2.0742e-02,  ..., 4.5469e-02,\n",
       "           5.8164e-02, 6.8839e-01]],\n",
       "\n",
       "         [[7.6962e-02, 4.9141e-02, 2.3105e-01,  ..., 1.7775e-02,\n",
       "           2.8952e-02, 4.4297e-01],\n",
       "          [2.3549e-01, 5.3521e-02, 4.3977e-01,  ..., 1.0038e-02,\n",
       "           2.1178e-02, 3.8846e-02],\n",
       "          [3.2618e-01, 5.6838e-02, 1.8005e-01,  ..., 2.4375e-02,\n",
       "           3.2848e-02, 1.5826e-01],\n",
       "          ...,\n",
       "          [6.2939e-02, 1.2860e-02, 8.7355e-03,  ..., 1.0447e-02,\n",
       "           1.0548e-02, 8.8082e-01],\n",
       "          [1.9191e-01, 1.7720e-02, 3.1081e-02,  ..., 7.2048e-03,\n",
       "           4.9057e-03, 7.0966e-01],\n",
       "          [7.8218e-02, 1.2527e-01, 1.5451e-01,  ..., 3.8477e-02,\n",
       "           2.1191e-02, 4.0024e-01]],\n",
       "\n",
       "         [[7.9287e-02, 6.4005e-02, 3.9566e-02,  ..., 4.8512e-02,\n",
       "           7.6407e-02, 4.2752e-01],\n",
       "          [4.3271e-02, 4.8913e-03, 5.4163e-03,  ..., 4.8501e-02,\n",
       "           5.4289e-02, 6.9838e-01],\n",
       "          [1.7456e-02, 2.3599e-03, 1.1545e-03,  ..., 7.3785e-02,\n",
       "           4.8826e-02, 7.6565e-01],\n",
       "          ...,\n",
       "          [3.1595e-02, 1.3029e-02, 4.0268e-02,  ..., 2.4698e-03,\n",
       "           2.7982e-02, 8.0107e-01],\n",
       "          [4.3635e-02, 4.9541e-02, 1.6938e-02,  ..., 5.2673e-02,\n",
       "           7.2611e-02, 6.2509e-01],\n",
       "          [1.7937e-01, 8.7113e-02, 8.7187e-02,  ..., 4.5221e-02,\n",
       "           2.3610e-01, 1.2780e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[6.3114e-02, 2.7119e-01, 1.2626e-01,  ..., 3.6002e-02,\n",
       "           1.4266e-01, 1.1871e-01],\n",
       "          [6.5682e-02, 3.0000e-01, 6.6824e-02,  ..., 2.2694e-02,\n",
       "           2.5396e-01, 1.8391e-01],\n",
       "          [5.0615e-02, 2.7000e-01, 1.4038e-01,  ..., 1.7906e-02,\n",
       "           3.2585e-01, 9.5279e-02],\n",
       "          ...,\n",
       "          [1.5024e-02, 3.4814e-02, 2.3210e-02,  ..., 2.4150e-02,\n",
       "           7.0615e-01, 1.2332e-01],\n",
       "          [2.8475e-02, 2.5991e-02, 3.5240e-02,  ..., 5.7385e-02,\n",
       "           5.1126e-01, 2.4240e-01],\n",
       "          [4.7665e-02, 6.2154e-02, 4.7857e-02,  ..., 4.2216e-02,\n",
       "           3.5738e-01, 3.1871e-01]],\n",
       "\n",
       "         [[5.1656e-02, 3.0868e-02, 2.9886e-02,  ..., 1.0077e-01,\n",
       "           5.9941e-01, 1.3079e-01],\n",
       "          [1.3595e-02, 2.8469e-02, 9.5246e-03,  ..., 2.8760e-02,\n",
       "           7.1686e-01, 1.9143e-01],\n",
       "          [2.4542e-03, 4.4536e-03, 4.5783e-01,  ..., 1.0069e-02,\n",
       "           3.9657e-01, 5.9700e-02],\n",
       "          ...,\n",
       "          [1.8454e-03, 5.0729e-04, 3.6136e-04,  ..., 6.3096e-01,\n",
       "           2.9822e-01, 5.9348e-02],\n",
       "          [8.7906e-03, 2.2120e-03, 1.2158e-03,  ..., 4.0716e-03,\n",
       "           8.5534e-01, 1.2189e-01],\n",
       "          [6.7224e-03, 2.3520e-03, 1.3034e-03,  ..., 6.8712e-03,\n",
       "           7.6320e-01, 2.1486e-01]],\n",
       "\n",
       "         [[4.8051e-02, 4.3158e-02, 7.8240e-02,  ..., 1.1848e-01,\n",
       "           4.0896e-01, 1.4255e-01],\n",
       "          [8.9076e-02, 2.4859e-01, 2.0023e-01,  ..., 5.6754e-03,\n",
       "           9.8962e-02, 1.1528e-01],\n",
       "          [1.1021e-01, 1.3894e-01, 4.8153e-01,  ..., 5.4896e-03,\n",
       "           1.0558e-01, 5.6112e-02],\n",
       "          ...,\n",
       "          [2.2854e-02, 8.2214e-03, 3.4206e-03,  ..., 3.4337e-01,\n",
       "           1.7267e-01, 9.7365e-02],\n",
       "          [3.5208e-02, 1.8483e-02, 1.0874e-02,  ..., 3.3259e-02,\n",
       "           7.2526e-01, 1.2017e-01],\n",
       "          [9.8529e-02, 4.1178e-02, 1.4884e-02,  ..., 5.7356e-02,\n",
       "           3.4458e-01, 3.4973e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[4.3122e-02, 1.2196e-01, 5.3237e-03,  ..., 3.2275e-03,\n",
       "           4.3168e-01, 3.6785e-01],\n",
       "          [1.0312e-02, 2.5264e-01, 8.4668e-03,  ..., 7.0019e-03,\n",
       "           3.9108e-01, 3.0681e-01],\n",
       "          [3.8171e-03, 1.9006e-02, 7.2587e-01,  ..., 1.1187e-03,\n",
       "           9.1370e-02, 6.5276e-02],\n",
       "          ...,\n",
       "          [1.8629e-03, 1.6322e-03, 1.0837e-04,  ..., 3.2446e-01,\n",
       "           5.1417e-01, 1.4773e-01],\n",
       "          [4.3313e-03, 5.2337e-03, 2.6454e-03,  ..., 4.7656e-03,\n",
       "           8.3140e-01, 1.4133e-01],\n",
       "          [5.3638e-03, 7.7388e-03, 1.8628e-03,  ..., 4.3794e-03,\n",
       "           7.3709e-01, 2.3567e-01]],\n",
       "\n",
       "         [[1.8980e-01, 1.2601e-01, 8.4786e-02,  ..., 6.8060e-04,\n",
       "           3.7081e-01, 1.4426e-01],\n",
       "          [2.6577e-01, 3.6513e-01, 2.8595e-02,  ..., 9.5877e-05,\n",
       "           2.1404e-03, 3.2514e-01],\n",
       "          [3.6285e-02, 2.1607e-03, 4.4781e-01,  ..., 5.0976e-06,\n",
       "           5.8564e-04, 4.0492e-02],\n",
       "          ...,\n",
       "          [2.0330e-01, 1.9487e-04, 4.6169e-05,  ..., 4.7510e-01,\n",
       "           3.1712e-03, 2.6732e-01],\n",
       "          [2.1813e-01, 7.1999e-02, 6.2446e-02,  ..., 4.8935e-02,\n",
       "           2.0383e-01, 1.9921e-01],\n",
       "          [4.6171e-01, 5.6490e-02, 2.2960e-02,  ..., 7.2481e-03,\n",
       "           6.8830e-02, 3.1134e-01]],\n",
       "\n",
       "         [[2.8860e-02, 1.5834e-01, 3.5526e-01,  ..., 4.7523e-02,\n",
       "           1.3489e-01, 6.8968e-02],\n",
       "          [6.4176e-03, 3.9375e-02, 1.3292e-01,  ..., 1.6071e-01,\n",
       "           1.4546e-01, 6.0108e-02],\n",
       "          [8.0405e-03, 1.5795e-02, 3.5033e-02,  ..., 7.6959e-02,\n",
       "           5.5418e-01, 1.1690e-01],\n",
       "          ...,\n",
       "          [4.6469e-03, 1.7577e-01, 2.1710e-02,  ..., 1.5870e-02,\n",
       "           5.8211e-01, 1.8123e-01],\n",
       "          [1.5991e-01, 1.6965e-02, 1.5884e-02,  ..., 2.2309e-02,\n",
       "           5.6032e-01, 1.3942e-01],\n",
       "          [6.2536e-02, 6.5078e-02, 5.3280e-02,  ..., 8.0924e-02,\n",
       "           3.3554e-01, 2.6192e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.1099, 0.0323, 0.0585,  ..., 0.0466, 0.2980, 0.2410],\n",
       "          [0.0140, 0.0050, 0.0118,  ..., 0.0041, 0.5883, 0.3433],\n",
       "          [0.0055, 0.0008, 0.0046,  ..., 0.0022, 0.6408, 0.3343],\n",
       "          ...,\n",
       "          [0.0096, 0.0039, 0.0032,  ..., 0.0032, 0.6186, 0.3525],\n",
       "          [0.0094, 0.0058, 0.0015,  ..., 0.0034, 0.5744, 0.3967],\n",
       "          [0.0097, 0.0051, 0.0014,  ..., 0.0030, 0.5772, 0.3958]],\n",
       "\n",
       "         [[0.0844, 0.0744, 0.1451,  ..., 0.1101, 0.1155, 0.1223],\n",
       "          [0.0353, 0.0070, 0.0103,  ..., 0.0097, 0.4981, 0.3865],\n",
       "          [0.0142, 0.0051, 0.0034,  ..., 0.0027, 0.5681, 0.3949],\n",
       "          ...,\n",
       "          [0.0347, 0.0213, 0.0155,  ..., 0.0332, 0.3704, 0.3761],\n",
       "          [0.0120, 0.0048, 0.0027,  ..., 0.0042, 0.5567, 0.4053],\n",
       "          [0.0119, 0.0046, 0.0030,  ..., 0.0042, 0.5574, 0.4041]],\n",
       "\n",
       "         [[0.0647, 0.2083, 0.1497,  ..., 0.1567, 0.0405, 0.0465],\n",
       "          [0.0235, 0.0971, 0.0489,  ..., 0.0474, 0.3561, 0.3410],\n",
       "          [0.0261, 0.0316, 0.1042,  ..., 0.0814, 0.3400, 0.3020],\n",
       "          ...,\n",
       "          [0.0392, 0.0183, 0.0167,  ..., 0.0675, 0.4206, 0.3732],\n",
       "          [0.0208, 0.0049, 0.0036,  ..., 0.0100, 0.5309, 0.4073],\n",
       "          [0.0178, 0.0061, 0.0041,  ..., 0.0120, 0.5215, 0.4149]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.1087, 0.1134, 0.1140,  ..., 0.0528, 0.0099, 0.0123],\n",
       "          [0.0695, 0.0379, 0.0282,  ..., 0.0259, 0.2869, 0.2088],\n",
       "          [0.0528, 0.0219, 0.0353,  ..., 0.0116, 0.3224, 0.2397],\n",
       "          ...,\n",
       "          [0.0311, 0.0085, 0.0064,  ..., 0.0850, 0.3998, 0.3019],\n",
       "          [0.0205, 0.0035, 0.0021,  ..., 0.0119, 0.5600, 0.3843],\n",
       "          [0.0244, 0.0038, 0.0025,  ..., 0.0129, 0.5547, 0.3824]],\n",
       "\n",
       "         [[0.0156, 0.0647, 0.0662,  ..., 0.1366, 0.1434, 0.1862],\n",
       "          [0.0124, 0.0062, 0.0069,  ..., 0.0114, 0.5699, 0.3557],\n",
       "          [0.0275, 0.0336, 0.0545,  ..., 0.0144, 0.3800, 0.3332],\n",
       "          ...,\n",
       "          [0.0075, 0.0029, 0.0023,  ..., 0.0117, 0.5798, 0.3735],\n",
       "          [0.0074, 0.0031, 0.0023,  ..., 0.0027, 0.5918, 0.3840],\n",
       "          [0.0065, 0.0028, 0.0021,  ..., 0.0027, 0.5862, 0.3918]],\n",
       "\n",
       "         [[0.0038, 0.0281, 0.0068,  ..., 0.1368, 0.3932, 0.3535],\n",
       "          [0.0041, 0.0059, 0.0027,  ..., 0.0084, 0.6029, 0.3613],\n",
       "          [0.0096, 0.0109, 0.0185,  ..., 0.0170, 0.5280, 0.3724],\n",
       "          ...,\n",
       "          [0.0587, 0.0256, 0.0085,  ..., 0.0268, 0.4324, 0.4059],\n",
       "          [0.0042, 0.0036, 0.0023,  ..., 0.0050, 0.5756, 0.3930],\n",
       "          [0.0036, 0.0033, 0.0020,  ..., 0.0043, 0.5739, 0.4001]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)), cross_attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## show other outputs\n",
    "\n",
    "# After setting output_attentions to true, the attentions is no longer None.\n",
    "\n",
    "# So we can set those arguments when loading the model to control the outputs.\n",
    "\n",
    "model_nh = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True)\n",
    "output = model_nh(input_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Model with Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.2068, -0.3478]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load model\n",
    "\n",
    "# import task specific model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# load model\n",
    "# By default, the classes are 2\n",
    "model_cla = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# model inference\n",
    "out_cla = model_cla(input_ids)\n",
    "\n",
    "out_cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model\n",
    "\n",
    "# Compared to the model without head (called BertModel), the model with head is called BertForSequenceClassification\n",
    "# which is a wrapped BertModel base model.\n",
    "# This wraped model has an extra layer at the end : Linear(in_features=768, out_features=2, bias=True).\n",
    "\n",
    "model_cla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4765,  0.2972, -0.2578, -0.2620,  0.4579,  0.1838,  0.5107,  0.1503,\n",
       "         -0.1623,  0.0592]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change output classes\n",
    "\n",
    "# load model\n",
    "# We set the output classes to 10\n",
    "model_cla = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=10)\n",
    "\n",
    "# model inference\n",
    "out_cla = model_cla(input_ids)\n",
    "\n",
    "out_cla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the below, restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this.\n",
    "# Should be run the first\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load dataset\n",
    "#################\n",
    "\n",
    "# In this class, we don't do the tokenization\n",
    "# As in the hf_transformers_basics_tokenizer.ipynb, tokenization of batched size is faster\n",
    "# So we don't do the tokenization here, since it will be less efficient\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    label2id = {\"negative\":0, \"positive\": 1}\n",
    "    id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "    def __init__(self, ckp):\n",
    "\n",
    "        super().__init__()\n",
    "        self.ckp = ckp\n",
    "        data = load_dataset(ckp)\n",
    "        self.data = {\"text\":[], \"label\": []}\n",
    "        # there are 3 classes, we select only the 2: neg and pos\n",
    "        for i in range(len(data[\"train\"][\"review\"])):\n",
    "            if data[\"train\"][i][\"division\"] in [\"negative\", \"positive\"]:\n",
    "                self.data[\"text\"].append(data[\"train\"][i][\"review\"])\n",
    "                # convert string label to numerical label\n",
    "                self.data[\"label\"].append(MyDataset.label2id.get(data[\"train\"][i][\"division\"]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return self.data[\"text\"][index], self.data[\"label\"][index]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data[\"text\"])\n",
    "\n",
    "\n",
    "ckp_data = \"davidberg/sentiment-reviews\"   \n",
    "data = MyDataset(ckp_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548\n",
      "('able play youtube alexa', 1)\n",
      "('able recognize indian accent really well drop function helpful call device talk person near device smart plug schedule work seamlessly con would sound kindloud but lack clarity mid frequency need tweeked optimum clarity rarely device doesnt respond call alexa', 1)\n",
      "('absolute smart device amazon connect external sub woofer sound amaze recons voice even close room like almost collection songs english hindi must quite moneys worth', 1)\n",
      "('absolutely amaze new member family control home voice connect home anywhere world', 1)\n",
      "('absolutely amaze previously sceptical invest money but arrive worth ityou absolutely buy wont regret cheer', 1)\n",
      "('absolutely cheat customer if buy amazon product definitely want buy amazon prime members also case if song want play absolutely need amazon prime membership otherwise can not play music app no google apps not work amazon alexa if anybody want amazon alexa go google home everything also free cost app', 1)\n",
      "('absolutely house hold item fair price intuitive speech recognition superb use hindi english sometimes hinglish also big thank amazon team even place draw room call alexa bedroom ears us play favourite songs flawlessly overall best fit must guess guests visit us purchase enjoy performance must buy', 1)\n",
      "('absolutely mind blow device beautiful shape excellent work nice performance verry easy usebut problem no battery storage work pluggedotherwise amaze', 1)\n",
      "('accessibility limit amazon apps music appsexcellent use bluetooth speaker', 0)\n",
      "('accord work software no google spot not consider perfect sound good', 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "for i in range(10):\n",
    "    print(data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3194, 354)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) split data\n",
    "###############\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "trainset, validset = random_split(data, lengths=(0.9, 0.1))\n",
    "\n",
    "len(trainset), len(validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) tokenizer\n",
    "##############\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def collate_fct(batch):\n",
    "\n",
    "    texts, labels = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "    toks = tokenizer(texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    toks[\"labels\"] = torch.tensor(labels)\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) dataloader\n",
    "###############\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=collate_fct)\n",
    "validloader = DataLoader(validset, batch_size=32, shuffle=False, collate_fn=collate_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2525,  8224,  ...,     0,     0,     0],\n",
       "        [  101,  4067,  2643,  ...,     0,     0,     0],\n",
       "        [  101,  3452,  2204,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,  2131,  2197,  ...,     0,     0,     0],\n",
       "        [  101, 21688,  2614,  ...,     0,     0,     0],\n",
       "        [  101,  2204,  4031,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5) load model\n",
    "###############\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# sent to gpu\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) define optimizer\n",
    "#####################\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) evaluation\n",
    "###############\n",
    "\n",
    "def eval():\n",
    "\n",
    "    acc_count = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in validloader:\n",
    "\n",
    "        # if there is GPU, send the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        output = model(**batch)\n",
    "\n",
    "        pred = torch.argmax(output.logits, dim=-1)\n",
    "\n",
    "        # count correct labels\n",
    "        acc_count += (pred.int() == batch[\"labels\"].int()).sum()\n",
    "\n",
    "    return acc_count / len(validset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) define Training\n",
    "####################\n",
    "\n",
    "def train(epoch=3, log_step=50):\n",
    "\n",
    "    gStep = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in trainloader:\n",
    "            \n",
    "            # if there is GPU, send the data to GPU\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(**batch)\n",
    "\n",
    "            output.loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if gStep % log_step == 0:\n",
    "\n",
    "                print(f\"{e+1} / {epoch} - global step: {gStep}, loss: {output.loss.item()}\")\n",
    "\n",
    "            gStep += 1\n",
    "\n",
    "        acc = eval()\n",
    "\n",
    "        print(f\"{e+1} / {epoch} - acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 3 - global step: 0, loss: 0.6653264760971069\n",
      "1 / 3 - global step: 50, loss: 0.10414525121450424\n",
      "1 / 3 - acc: 0.9519774317741394\n",
      "2 / 3 - global step: 100, loss: 0.1194240152835846\n",
      "2 / 3 - global step: 150, loss: 0.18372759222984314\n",
      "2 / 3 - acc: 0.9519774317741394\n",
      "3 / 3 - global step: 200, loss: 0.058997586369514465\n",
      "3 / 3 - global step: 250, loss: 0.27312278747558594\n",
      "3 / 3 - acc: 0.9548022747039795\n"
     ]
    }
   ],
   "source": [
    "# 9) train\n",
    "##########\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intput: I am not sure I like it., prediction: negative\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "###########\n",
    "\n",
    "## manual\n",
    "\n",
    "text = \"I am not sure I like it.\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    inputs = {k:v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    output = model(**inputs)\n",
    "\n",
    "    logits = output.logits\n",
    "\n",
    "    pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    print(f\"intput: {text}, prediction: {MyDataset.id2label.get(pred.item())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 15:01:48.750048: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 15:01:48.750100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 15:01:48.752253: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-19 15:01:48.763197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 15:01:50.920003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "## pipeline of transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# define model config\n",
    "# if not, the result will be the default value: LABEL_0/LABEL_1\n",
    "model.config.id2label = MyDataset.id2label\n",
    "\n",
    "#  we need a task type name for the pipeline\n",
    "# In this case, the name is \"text-classification\".\n",
    "# If we don't know the name, we can just put whatever, and an error message will show up.\n",
    "# At the end of the error message, there is a list of all type names that we can choose.\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.6834433674812317}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
