{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, it will:\n",
    "\n",
    "    I. present the evaluate module\n",
    "    II. How to use\n",
    "    III. Apply to Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a module to compute different metrics.\n",
    "Here is good description of the module: https://huggingface.co/docs/evaluate/a_quick_tour.\n",
    "\n",
    "This site: https://huggingface.co/evaluate-metric contains the available metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 17:29:06.000889: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 17:29:06.000949: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 17:29:06.003230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-19 17:29:06.015190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 17:29:08.106749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "########\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## show available modules\n",
    "\n",
    "# evaluate.list_evaluation_modules()\n",
    "\n",
    "# select\n",
    "evaluate.list_evaluation_modules(include_community=False, with_details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
      "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
      " Where:\n",
      "TP: True positive\n",
      "TN: True negative\n",
      "FP: False positive\n",
      "FN: False negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## print the description\n",
    "\n",
    "# one can show the doc of this method:\n",
    "# print(acc.__doc__)\n",
    "# Or by calling the members, one can show the doc as well \n",
    "\n",
    "print(acc.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Args:\n",
      "    predictions (`list` of `int`): Predicted labels.\n",
      "    references (`list` of `int`): Ground truth labels.\n",
      "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
      "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
      "\n",
      "Returns:\n",
      "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1-A simple example\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.5}\n",
      "\n",
      "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
      "        >>> print(results)\n",
      "        {'accuracy': 3.0}\n",
      "\n",
      "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
      "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
      "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
      "        >>> print(results)\n",
      "        {'accuracy': 0.8778625954198473}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## print the usage\n",
    "\n",
    "print(acc.inputs_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5}\n"
     ]
    }
   ],
   "source": [
    "## for pairs of comparisons\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "ref = [0, 1, 2, 0, 1, 2]\n",
    "pre = [0, 1, 1, 2, 1, 0]\n",
    "\n",
    "for r, p in zip(ref, pre) :\n",
    "    acc.add(reference=r, prediction=p)\n",
    "\n",
    "print(acc.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5}\n"
     ]
    }
   ],
   "source": [
    "## batched results\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "refs = [[0, 1, 2, 0, 1, 2], [0, 1, 2, 0, 1, 2]]\n",
    "pres = [[0, 1, 1, 2, 1, 0], [0, 1, 1, 2, 1, 0]]\n",
    "\n",
    "for r, p in zip(refs, pres) :\n",
    "    acc.add_batch(references=r, predictions=p)\n",
    "\n",
    "print(acc.compute())\n",
    "\n",
    "# notes: function \"add\" accepts either \"reference\" and \"references\" as vairiable name\n",
    "# but not \"add_batch\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine several criteria\n",
    "\n",
    "metrics = evaluate.combine([\"accuracy\", \"recall\", \"f1\", \"precision\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6666666666666666, 'recall': 0.75, 'f1': 0.75, 'precision': 0.75}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref = [0, 1, 1, 0, 1, 1]\n",
    "pre = [0, 1, 1, 1, 1, 0]\n",
    "metrics.compute(references=ref, predictions=pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot\n",
    "\n",
    "# evaluate provides also a way to visualize (plot) the results for comparison\n",
    "\n",
    "# from evaluate.visualization import radar_plot\n",
    "# data = [{\"accuracy\": 0.8, \"precision\": 0.7, \"f1\": 0.6, \"latency_in_seconds\": 10}, ...]\n",
    "# models = [\"model1\", ...]\n",
    "# plot = radar_plot(data=data, model_names=models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Update Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the Training section from 'hf_transformers_basics_datasets.ipynb\" and replace the manually defined evaluation function by the evaluation module explain above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this.\n",
    "# Should be run the first\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "    num_rows: 3548\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) load dataset\n",
    "#################\n",
    "\n",
    "# not changed\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ckp_data = \"davidberg/sentiment-reviews\"\n",
    "\n",
    "data = load_dataset(ckp_data, split=\"train\")\n",
    "\n",
    "data = data.filter(lambda column: \"neutral\" not in column[\"division\"]) # filter out neutral\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 3193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'review', 'polarity', 'division'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) split data\n",
    "###############\n",
    "\n",
    "# not changed\n",
    "\n",
    "split_data = data.train_test_split(test_size=0.1)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734516a4815e4431b82d14368c2657e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abaf113c4a3456db1ebb31a3a9f8ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) tokenizer\n",
    "##############\n",
    "\n",
    "# not changed\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "label2id = {\"negative\":0, \"positive\": 1}\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def process(batch):\n",
    "\n",
    "    toks = tokenizer(batch[\"review\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    toks[\"labels\"] = torch.tensor([label2id.get(item) for item in batch[\"division\"]])\n",
    "\n",
    "    return toks\n",
    "\n",
    "tokenized_data = split_data.map(process, batched=True, remove_columns=data.column_names)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 17:56:22.144518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 17:56:22.144586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 17:56:22.146834: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-19 17:56:22.158900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 17:56:24.277432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 4) dataloader\n",
    "###############\n",
    "\n",
    "# not changed\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "trainset, validset = tokenized_data[\"train\"], tokenized_data[\"test\"]\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=DataCollatorWithPadding(tokenizer))\n",
    "validloader = DataLoader(validset, batch_size=32, shuffle=False, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5) load model\n",
    "###############\n",
    "\n",
    "# not changed\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# sent to gpu\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) define optimizer\n",
    "#####################\n",
    "\n",
    "# not changed\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) evaluation\n",
    "###############\n",
    "\n",
    "# replace the count of labels by the metrics defined by evaluate\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metrics_fct = evaluate.combine([\"accuracy\", \"f1\"])\n",
    "\n",
    "def eval():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for batch in validloader:\n",
    "\n",
    "        # if there is GPU, send the data to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        output = model(**batch)\n",
    "\n",
    "        pred = torch.argmax(output.logits, dim=-1)\n",
    "\n",
    "        metrics_fct.add_batch(predictions=pred.int(), references=batch[\"labels\"].int())\n",
    "\n",
    "    return metrics_fct.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Train\n",
    "##########\n",
    "\n",
    "# not changed\n",
    "\n",
    "def train(epoch=3, log_step=50):\n",
    "\n",
    "    gStep = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in trainloader:\n",
    "            \n",
    "            # if there is GPU, send the data to GPU\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(**batch)\n",
    "\n",
    "            output.loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if gStep % log_step == 0:\n",
    "\n",
    "                print(f\"{e+1} / {epoch} - global step: {gStep}, loss: {output.loss.item()}\")\n",
    "\n",
    "            gStep += 1\n",
    "\n",
    "        metrics = eval()\n",
    "\n",
    "        print(f\"{e+1} / {epoch} - {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 3 - global step: 0, loss: 0.12130105495452881\n",
      "1 / 3 - global step: 50, loss: 0.17807073891162872\n",
      "1 / 3 - {'accuracy': 0.952112676056338, 'f1': 0.9704347826086956}\n",
      "2 / 3 - global step: 100, loss: 0.0437583364546299\n",
      "2 / 3 - global step: 150, loss: 0.129874587059021\n",
      "2 / 3 - {'accuracy': 0.9549295774647887, 'f1': 0.9719298245614035}\n",
      "3 / 3 - global step: 200, loss: 0.03587239980697632\n",
      "3 / 3 - global step: 250, loss: 0.006216248031705618\n",
      "3 / 3 - {'accuracy': 0.9464788732394366, 'f1': 0.9663716814159292}\n"
     ]
    }
   ],
   "source": [
    "# not changed\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
