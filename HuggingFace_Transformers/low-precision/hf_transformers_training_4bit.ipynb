{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA\n",
    "\n",
    "Using 4bit quatization for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this\n",
    "# Shoulmd be run the first\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Quantization using 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data:  [1.2300000190734863, 1.559999942779541, 1.6100000143051147, 4.579999923706055]\n",
      "absmax:  4.579999923706055\n",
      "scale factor:  1.5283843049359103\n",
      "quatized data:  [2, 2, 2, 7]\n",
      "reversed data:  [1.30859375, 1.30859375, 1.30859375, 4.578125]\n"
     ]
    }
   ],
   "source": [
    "# quantization example using 4 bit\n",
    "# Since its limited bits, the original values are rounded abruptly.\n",
    "# The effect of the outliers is more prominent her than for 8bit quatization, \n",
    "# even though the outlier value is not far away.\n",
    "\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1.23, 1.56, 1.61, 4.58])\n",
    "\n",
    "print(\"original data: \", x.tolist())\n",
    "\n",
    "# absmax\n",
    "\n",
    "x_absmax = torch.max(torch.abs(x))\n",
    "\n",
    "print(\"absmax: \", x_absmax.item())\n",
    "\n",
    "# get scale\n",
    "# the boundaries for 4bit: -7, 7\n",
    "\n",
    "scale = 7 / x_absmax.item()\n",
    "\n",
    "print(\"scale factor: \", scale)\n",
    "\n",
    "# quatization\n",
    "\n",
    "q_x = torch.round(x * scale).to(torch.int16)\n",
    "\n",
    "print(\"quatized data: \", q_x.tolist())\n",
    "\n",
    "# reverse\n",
    "\n",
    "x_re = q_x.to(torch.float16) / scale\n",
    "\n",
    "print(\"reversed data: \", x_re.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problems:\n",
    "  * 4bit has lower bondaries than 8bit, the rounding errors are greater.\n",
    "  * outlier effect is more important for 4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4086deed6b84c7bb4dd945d7dd72b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(-0.1, 0.1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the distribution of the weights for pretrained Llama2 model\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "ckp = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(ckp, low_cpu_mem_usage=True, torch_dtype=torch.half, device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "# get weights\n",
    "weights = []\n",
    "\n",
    "for i, param in enumerate(model.parameters()):\n",
    "    if i % 1000 == 0:\n",
    "        weights.append(param.view(-1))\n",
    "\n",
    "weights = torch.cat(weights)\n",
    "\n",
    "# # get histogram\n",
    "\n",
    "plt.hist(weights.cpu().detach().numpy(), 200)\n",
    "plt.xlim([-0.1, 0.1]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "  - quantile: project values according to the quantile interval\n",
    "    * find the distribution limites of the weights (we don't know the expected weight distribution)\n",
    "    * devide the interval into 16 sub-intervals (2^4=16)\n",
    "    * quantize the values according to the 16 intervals\n",
    "    * use 16 float scale factors to invert the quantization\n",
    "    * for invert the quantiztion, find the nearest value and replace by the corresponding float scale factor\n",
    "\n",
    "Since the weight distribution is normal (see the figure above), we normalize all weight values between [-1, 1] and quantize them into 16 values. But in practice we devide the weights into blocs of 64, normalize the weight within the bloc and do the quantization to minimize outlier effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. 4bit Normal Float (NF4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The empirical values for NF4 for block size of 64\n",
    "# the reference : https://ar5iv.labs.arxiv.org/html/2306.06965\n",
    "\n",
    "nf4 = [-1.0, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.09105, 0.0, 0.07958, 0.1609, 0.2461, 0.3379, 0.4407, 0.5626, 0.7229, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data:  [1.2300000190734863, 1.559999942779541, 1.6100000143051147, 4.579999923706055]\n",
      "absmax:  4.579999923706055\n",
      "normalized values:  tensor([0.2686, 0.3406, 0.3515, 1.0000])\n",
      "quatized data:  [10, 11, 11, 15]\n",
      "reversed data:  [1.1271378993988037, 1.5475820302963257, 1.5475820302963257, 4.579999923706055]\n"
     ]
    }
   ],
   "source": [
    "# example using NF4 for quantization\n",
    "# the results show more granular quantization than before.\n",
    "\n",
    "x = torch.tensor([1.23, 1.56, 1.61, 4.58])\n",
    "\n",
    "print(\"original data: \", x.tolist())\n",
    "\n",
    "# absmax\n",
    "\n",
    "x_absmax = torch.max(torch.abs(x))\n",
    "\n",
    "print(\"absmax: \", x_absmax.item())\n",
    "\n",
    "# normalization\n",
    "\n",
    "x_norm = x / x_absmax\n",
    "\n",
    "print(\"normalized values: \", x_norm)\n",
    "\n",
    "# quatization using nf4\n",
    "q_x, float_x = [], []\n",
    "for i in x_norm.tolist():\n",
    "    d = 90000\n",
    "    c = -1\n",
    "    for ind, reg in enumerate(nf4):\n",
    "        dist = abs(reg-i)\n",
    "        if dist < d:\n",
    "            d = dist\n",
    "            c = ind\n",
    "    q_x.append(c)\n",
    "    float_x.append(nf4[c])\n",
    "\n",
    "print(\"quatized data: \", q_x)\n",
    "\n",
    "# reverse\n",
    "\n",
    "x_re = torch.tensor(float_x) * x_absmax.item()\n",
    "\n",
    "print(\"reversed data: \", x_re.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for 4bit, we need some extra information:\n",
    "\n",
    "  * 16 nf4 float values (fixed so can be ignored)\n",
    "  * a float absmax for each block: so we need f32 / 64 = 0.5bit extra bit for this\n",
    "\n",
    "So we can do another quantization for the absmax:\n",
    "  * for a block of 256, do a 8bit quantization\n",
    "  * so we have 8 / 256 + 32 / (64 * 256) ~ 0.127 bit\n",
    "  * we have save 0.373 bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another optimization can be transfert part of unused resource to CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 11:34:02.790965: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-26 11:34:02.791034: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-26 11:34:02.794016: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-26 11:34:02.807408: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-26 11:34:04.932285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_data = \"yahma/alpaca-cleaned\"\n",
    "ckp = \"NousResearch/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'instruction', 'input'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare dataset\n",
    "\n",
    "# load dataset\n",
    "data = load_dataset(ckp_data, split=\"train[:1000]\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "def process(sample):\n",
    "\n",
    "    MAX_LEN = 256\n",
    "\n",
    "    human = tokenizer(\"Human: \" + \"\\n\".join([sample[\"instruction\"], sample[\"input\"]]).strip() + \"\\n\\nAssistant: \", add_special_tokens=False)\n",
    "    ml = tokenizer(sample[\"output\"], add_special_tokens=False)\n",
    "\n",
    "    input_ids = human[\"input_ids\"] + ml[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "    attention_mask = human[\"attention_mask\"] + ml[\"attention_mask\"] + [1]\n",
    "    labels = [-100] * len(human[\"input_ids\"]) + ml[\"input_ids\"] + [tokenizer.eos_token_id]\n",
    "\n",
    "    if len(input_ids) > MAX_LEN:\n",
    "\n",
    "        input_ids = input_ids[:MAX_LEN]\n",
    "        attention_mask = attention_mask[:MAX_LEN]\n",
    "        labels = labels[:MAX_LEN]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize dataset\n",
    "tokenized_data = data.map(process, remove_columns=data.column_names)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9f1837a1724d5fab20b1138ebd7928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# after loading, 4B of GPU memory for llama2-7B\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(ckp, \n",
    "                                             low_cpu_mem_usage=True, \n",
    "                                             torch_dtype=torch.half, \n",
    "                                             device_map=\"cuda:0\",\n",
    "                                             load_in_4bit=True, # load model in 4 bit\n",
    "                                             bnb_4bit_compute_dtype=torch.half, # use 4bit quantization\n",
    "                                             bnb_4bit_quant_type=\"nf4\", # use nf4 quantization\n",
    "                                             bnb_4bit_use_double_quant=True # enbale double quantization\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32000, 4096]) torch.float16\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([22544384, 1]) torch.uint8\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.norm.weight torch.Size([4096]) torch.float16\n",
      "lm_head.weight torch.Size([32000, 4096]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"NousResearch/Llama-2-7b-chat-hf\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": true,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\" # use paged optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model, \n",
    "    args=args,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/Qingyi/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 08:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31, training_loss=1.6292650776524698, metrics={'train_runtime': 500.921, 'train_samples_per_second': 1.996, 'train_steps_per_second': 0.062, 'total_flos': 5803885670768640.0, 'train_loss': 1.6292650776524698, 'epoch': 0.992})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. indeference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: hello\\n\\nAssistant:  Hello! How can I help you today?\\n\\nHuman: I'm feeling really down and I don't know why. Can you help me?\\n\\nAssistant: Of course, I'm here to listen and help. It sounds like you're feeling a bit down and unsure of why. Can you tell me more about what's going on and how you're feeling?\\n\\nHuman: *sigh* I don't know...I just feel like nothing is going right in my life. I'm not happy with my job, my relationships are struggling, and I feel like I'm not good enough.\\n\\nAssistant: It sounds like you're feeling a bit overwhelmed and unhappy with different areas of your life. It's completely normal to feel this way at times, and it's important to remember that you're not alone in feeling this way. Many people experience similar feelings, and there are things you can do to help manage and improve your situation.\\n\\nHuman: *sniffles* I know...I just feel so lost and unsure of what to do.\\n\\nAssistant:\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.eval()\n",
    "text = \"hello\"\n",
    "input = tokenizer(\"Human: \" + text + \"\\n\\nAssistant: \", return_tensors=\"pt\").to(peft_model.device)\n",
    "output = peft_model.generate(**input, max_length=256, eos_token_id=tokenizer.eos_token_id)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to run all model to see the memory allocation.\n",
    "\n",
    "NVIDIA GeForce RTX 3090 \n",
    "\n",
    "CUDA Version: 12.3\n",
    "\n",
    "24GB\n",
    "\n",
    "Llama2\n",
    "\n",
    "\n",
    "| Base Model| training Params   | Load Mem | Training Meme |\n",
    "|---------- |----------         |----------|----------     |\n",
    "|  7B       | 4.2M              | 4B       | 4.5B          |\n",
    "| 13B       | 6.5M              | 7.3B     | 7.8B          |\n",
    "| 70B       | OOM               | OOM      | OOM           |\n",
    "\n",
    "So from the comparaison, our rule of thumb is the GPU memory should have at least half of the model size to be able to run the model with NF4.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
