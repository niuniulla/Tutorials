{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation\n",
    "\n",
    "Infused Adapter by Inhibiting and Amplifying Inner Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://pbs.twimg.com/media/FShJxGtXEAAE09o.jpg:large\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://pbs.twimg.com/media/FShJxGtXEAAE09o.jpg:large', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 16:45:59.165432: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-25 16:45:59.165490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-25 16:45:59.167726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-25 16:45:59.179693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 16:46:01.314582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# prepare task, we use the same example as butfit\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "\n",
    "ckp_data = \"yahma/alpaca-cleaned\"\n",
    "ckp = \"bigscience/bloomz-1b1\"\n",
    "\n",
    "# load dataset\n",
    "data = load_dataset(ckp_data, split=\"train[:1000]\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "\n",
    "# process data\n",
    "def process(sample):\n",
    "\n",
    "    MAX_LEN = 256\n",
    "\n",
    "    human = tokenizer(\"Human: \" + \"\\n\".join([sample[\"instruction\"], sample[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    ml = tokenizer(sample[\"output\"] + tokenizer.eos_token)\n",
    "\n",
    "    input_ids = human[\"input_ids\"] + ml[\"input_ids\"]\n",
    "    attention_mask = human[\"attention_mask\"] + ml[\"attention_mask\"]\n",
    "    labels = [-100] * len(human[\"input_ids\"]) + ml[\"input_ids\"]\n",
    "\n",
    "    if len(input_ids) > MAX_LEN:\n",
    "\n",
    "        input_ids = input_ids[:MAX_LEN]\n",
    "        attention_mask = attention_mask[:MAX_LEN]\n",
    "        labels = labels[:MAX_LEN]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_data = data.map(process, remove_columns=data.column_names)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(ckp, low_cpu_mem_usage=True)\n",
    "\n",
    "# send to device\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. IA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IA3Config(peft_type=<PeftType.IA3: 'IA3'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, target_modules=None, feedforward_modules=None, fan_in_fan_out=False, modules_to_save=None, init_ia3_weights=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import IA3Config, TaskType, get_peft_model\n",
    "\n",
    "config = IA3Config(task_type=TaskType.CAUSAL_LM)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): IA3Model(\n",
       "    (model): BloomForCausalLM(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 1536)\n",
       "        (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 4608x1 (cuda:0)])\n",
       "              )\n",
       "              (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): Linear(\n",
       "                (base_layer): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x6144 (cuda:0)])\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 258,048 || all params: 1,065,572,352 || trainable%: 0.0242\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 05:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.877100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.741700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.747000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=1.874089599609375, metrics={'train_runtime': 344.1672, 'train_samples_per_second': 8.717, 'train_steps_per_second': 1.09, 'total_flos': 1670689992499200.0, 'train_loss': 1.874089599609375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in the article, they use lr=3e-3\n",
    "# with this lr, the training converges more effectively\n",
    "\n",
    "# define training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../tmp/checkpoint\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-3\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: List five steps for comparing two products.\n",
      "None\n",
      "\n",
      "Assistant: 1. Know the product characteristics and benefits. \n",
      "2. Know the characteristics and benefits of the opposite (i.e. contrast).\n",
      "3. Identify comparable (same source, product, and process) characteristics and benefits. 4. Compare the benefits. 5. Evaluate the pros and cons of the comparison.\n"
     ]
    }
   ],
   "source": [
    "def generate(_model, _tokenizer, instruction, input=None):\n",
    "\n",
    "    prompt = \"human: {}\\n{}\".format(instruction, input).strip() + \"\\n\\nAssistant: \"\n",
    "    inputs = _tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(_model.device)\n",
    "\n",
    "    generation_output = _model.generate(\n",
    "        input_ids=input_ids,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256, \n",
    "        do_sample=True\n",
    "    )\n",
    "    for seq in generation_output:\n",
    "        output = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "        print(output)\n",
    "\n",
    "generate(peft_model, tokenizer, \"List five steps for comparing two products.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
