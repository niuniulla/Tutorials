{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompte Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation\n",
    "\n",
    "This consists of adding a small part of the embedding model to the base model:\n",
    "\n",
    "    ____________________________________________________________\n",
    "    |      prompt embedding          |    Embedding            |\n",
    "    ------------------------------------------------------------\n",
    "    |_____________ __________________|____________ ____________|\n",
    "                  |                               |\n",
    "                Prompt                     base model Input     \n",
    "\n",
    "So the training was only done on the prompt embedding model and leave the base model intact.\n",
    "This reduce drastically the number of training parameters.\n",
    "\n",
    "Here, we did both finetuning: soft and hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Parameter-efficient fine-tuning (PEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil use peft module from hugging face to do finetuning. The supported models can be found: \n",
    "https://huggingface.co/docs/peft/v0.6.2/en/index for this type of finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install peft\n",
    "\n",
    "# uncomment and run the following line if no peft is installed, otherwise, skip\n",
    "\n",
    "# !python -m pip install peft --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the same example as bitfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 15:21:32.943273: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-25 15:21:32.943335: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-25 15:21:32.945568: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-25 15:21:32.957841: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 15:21:35.804084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# prepare example\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "\n",
    "ckp_data = \"yahma/alpaca-cleaned\"\n",
    "ckp = \"bigscience/bloomz-1b1\"\n",
    "\n",
    "# load dataset\n",
    "data = load_dataset(ckp_data, split=\"train[:1000]\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "\n",
    "# process data\n",
    "def process(sample):\n",
    "\n",
    "    MAX_LEN = 256\n",
    "\n",
    "    human = tokenizer(\"Human: \" + \"\\n\".join([sample[\"instruction\"], sample[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    ml = tokenizer(sample[\"output\"] + tokenizer.eos_token)\n",
    "\n",
    "    input_ids = human[\"input_ids\"] + ml[\"input_ids\"]\n",
    "    attention_mask = human[\"attention_mask\"] + ml[\"attention_mask\"]\n",
    "    labels = [-100] * len(human[\"input_ids\"]) + ml[\"input_ids\"]\n",
    "\n",
    "    if len(input_ids) > MAX_LEN:\n",
    "\n",
    "        input_ids = input_ids[:MAX_LEN]\n",
    "        attention_mask = attention_mask[:MAX_LEN]\n",
    "        labels = labels[:MAX_LEN]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_data = data.map(process, remove_columns=data.column_names)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(ckp, low_cpu_mem_usage=True)\n",
    "\n",
    "# send to device\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1.065314304 GB\n",
      "total required memory:  21.31 GB\n"
     ]
    }
   ],
   "source": [
    "# compute model size\n",
    "\n",
    "params = sum(param.numel() for param in model.parameters())\n",
    "print(\"model size: \", params/1e9, \"GB\")\n",
    "print(\"total required memory: \", round(params/1e9 * (4 + 4 + 12), 2), \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. soft prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For soft prompt, there is no concrete value for the added prompt, so the model will just intitiate ramdomly its value and learn with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, num_virtual_tokens=10, token_dim=None, num_transformer_submodules=None, num_attention_heads=None, num_layers=None, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None, tokenizer_kwargs=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config for soft prompt\n",
    "\n",
    "from peft import PromptTuningConfig, get_peft_model, TaskType, PromptTuningInit\n",
    "\n",
    "config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): BloomForCausalLM(\n",
       "    (transformer): BloomModel(\n",
       "      (word_embeddings): Embedding(250880, 1536)\n",
       "      (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(10, 1536)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(250880, 1536)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrap model\n",
    "\n",
    "# So the original bloom model becomes peft model\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,360 || all params: 1,065,329,664 || trainable%: 0.0014\n"
     ]
    }
   ],
   "source": [
    "# show parameters of wrapped model\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path='bigscience/bloomz-1b1', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, num_virtual_tokens=10, token_dim=1536, num_transformer_submodules=1, num_attention_heads=16, num_layers=24, prompt_tuning_init=<PromptTuningInit.RANDOM: 'RANDOM'>, prompt_tuning_init_text=None, tokenizer_name_or_path=None, tokenizer_kwargs=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to the previous config, some parameters have changed, such as token_dim, num_layers...\n",
    "# since there is an update of those parameters from model.\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 05:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.785100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.674500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=2.6220494079589844, metrics={'train_runtime': 337.8862, 'train_samples_per_second': 8.879, 'train_steps_per_second': 1.11, 'total_flos': 1670056200806400.0, 'train_loss': 2.6220494079589844, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here, the loss seems not decreasing\n",
    "# actually, it may decrease, but since the decrease is so slow that it is hard to see from their values here.\n",
    "\n",
    "# define training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../tmp/checkpoint\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'human: List five steps for comparing two products.\\n\\nAssistant:  What are the steps for comparing two products?'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# before finetuning we try the model prediction\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=model.device)\n",
    "human = \"human: {}\\n{}\".format(\"List five steps for comparing two products.\", \"\").strip() + \"\\n\\nAssistant: \"\n",
    "pipe(human, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: List five steps for comparing two products.\n",
      "None\n",
      "\n",
      "Assistant:  No\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "# if we use the pipeline as in bitfit, we got error: The model 'PeftModelForCausalLM' is not \n",
    "# supported for text-generation. Because the pipeline doesn't support causal ml yet.\n",
    "# so we have to generate the result by ourself.\n",
    "\n",
    "# the result doesn't seem to be right, which means that the training is not good.\n",
    "# This is due to that the training is not enough. For soft prompt training, we need more\n",
    "# training to be effective.\n",
    "\n",
    "def generate(model, tokenizer, instruction, input=None):\n",
    "\n",
    "    prompt = \"human: {}\\n{}\".format(instruction, input).strip() + \"\\n\\nAssistant: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    for seq in generation_output:\n",
    "        output = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "        print(output)\n",
    "\n",
    "generate(peft_model, tokenizer, \"List five steps for comparing two products.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. hard prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrarary to the soft prompt and as indicated by its name, hard prompt take an intial prompt value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTuningConfig(peft_type=<PeftType.PROMPT_TUNING: 'PROMPT_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, num_virtual_tokens=7, token_dim=None, num_transformer_submodules=None, num_attention_heads=None, num_layers=None, prompt_tuning_init=<PromptTuningInit.TEXT: 'TEXT'>, prompt_tuning_init_text='below is a human model dialog.', tokenizer_name_or_path='bigscience/bloomz-1b1', tokenizer_kwargs=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config\n",
    "from peft import PromptTuningConfig, get_peft_model, TaskType, PromptTuningInit\n",
    "\n",
    "# hard prompt\n",
    "\n",
    "config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                            prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "                            prompt_tuning_init_text=\"below is a human model dialog.\",\n",
    "                            num_virtual_tokens=len(tokenizer(\"below is a human model dialog.\")[\"input_ids\"]),\n",
    "                            tokenizer_name_or_path=ckp\n",
    "                            )\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): BloomForCausalLM(\n",
       "    (transformer): BloomModel(\n",
       "      (word_embeddings): Embedding(250880, 1536)\n",
       "      (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEmbedding(\n",
       "      (embedding): Embedding(7, 1536)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(250880, 1536)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrap the model\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,752 || all params: 1,065,325,056 || trainable%: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# compred to soft prompt we got fewer parameters to train since out prompt is shorter\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 05:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.193100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=2.3334760030110675, metrics={'train_runtime': 335.4992, 'train_samples_per_second': 8.942, 'train_steps_per_second': 1.118, 'total_flos': 1670056200806400.0, 'train_loss': 2.3334760030110675, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to soft prompt, we can see perceivably that the loss decreases\n",
    "\n",
    "# define training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../tmp/checkpoint\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: List five steps for comparing two products.\n",
      "None\n",
      "\n",
      "Assistant: Product A is a product that is sold in a store. Product B is a product that is sold in a store. Product A is better than product B.\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "generate(peft_model, tokenizer, \"List five steps for comparing two products.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load trained peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default the trainer save the model every 500 steps.\n",
    "# The correspond files can be found in the directory defined in the \"out_dir\" in training arguments.\n",
    "# The files are called:\n",
    "# - adapter_config.json\n",
    "# - dadapter_model.bin\n",
    "# which contains the trained prompt model\n",
    "\n",
    "# to load the saved models and use them, we do the following:\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# load base model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(ckp)\n",
    "\n",
    "ckp_peft = \"./checkpoint/checkpoint_100\" # example of checkpoint, replace this with the path of the checkpoint define in training argument\n",
    "\n",
    "# construct peft model\n",
    "peft_model = PeftModel.from_pretrained(model=model, model_id=ckp_peft) # make sure the base model is in cpu\n",
    "\n",
    "# inference\n",
    "peft_model = peft_model.cuda()\n",
    "generate(peft_model, tokenizer, \"List five steps for comparing two products.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the results are not good, but the hard prompt finetuning is better than the soft one. \n",
    "With more training, the results will get better. Also, use do_sample to generate more possible results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
