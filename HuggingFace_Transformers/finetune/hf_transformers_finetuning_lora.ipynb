{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of lora is to modify the weight matrices using 2 smaller matrices. And the smaller matrices were represented by a layer parallel to the original matrix.\n",
    "\n",
    "transformers > 4.34\n",
    "peft > 0.5\n",
    "accelerate > 0.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-25 16:26:48.564700: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-25 16:26:48.564764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-25 16:26:48.567038: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-25 16:26:48.579223: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-25 16:26:50.721733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# prepare task, we use the same example as butfit\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "\n",
    "ckp_data = \"yahma/alpaca-cleaned\"\n",
    "ckp = \"bigscience/bloomz-1b1\"\n",
    "\n",
    "# load dataset\n",
    "data = load_dataset(ckp_data, split=\"train[:1000]\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "\n",
    "# process data\n",
    "def process(sample):\n",
    "\n",
    "    MAX_LEN = 256\n",
    "\n",
    "    human = tokenizer(\"Human: \" + \"\\n\".join([sample[\"instruction\"], sample[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    ml = tokenizer(sample[\"output\"] + tokenizer.eos_token)\n",
    "\n",
    "    input_ids = human[\"input_ids\"] + ml[\"input_ids\"]\n",
    "    attention_mask = human[\"attention_mask\"] + ml[\"attention_mask\"]\n",
    "    labels = [-100] * len(human[\"input_ids\"]) + ml[\"input_ids\"]\n",
    "\n",
    "    if len(input_ids) > MAX_LEN:\n",
    "\n",
    "        input_ids = input_ids[:MAX_LEN]\n",
    "        attention_mask = attention_mask[:MAX_LEN]\n",
    "        labels = labels[:MAX_LEN]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_data = data.map(process, remove_columns=data.column_names)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(ckp, low_cpu_mem_usage=True)\n",
    "\n",
    "# send to device\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1.065314304 GB\n",
      "total required memory:  21.31 GB\n"
     ]
    }
   ],
   "source": [
    "# compute model size\n",
    "\n",
    "params = sum(param.numel() for param in model.parameters())\n",
    "print(\"model size: \", params/1e9, \"GB\")\n",
    "print(\"total required memory: \", round(params/1e9 * (4 + 4 + 12), 2), \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BloomForCausalLM(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 1536)\n",
       "        (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4608, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 1,066,493,952 || trainable%: 0.1106\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 05:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.568900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.194600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.964000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=2.0956065470377605, metrics={'train_runtime': 347.2099, 'train_samples_per_second': 8.64, 'train_steps_per_second': 1.08, 'total_flos': 1672953534259200.0, 'train_loss': 2.0956065470377605, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../tmp/checkpoint\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: List five steps for comparing two products.\n",
      "None\n",
      "\n",
      "Assistant: 1. Identify the main differences between the two products. 2. Determine the main advantages and disadvantages of each product. 3. Determine the main advantages and disadvantages of each product. 4. Determine the main advantages and disadvantages of each product. 5. Determine the main advantages and disadvantages of each product.\n"
     ]
    }
   ],
   "source": [
    "def generate(_model, _tokenizer, instruction, input=None):\n",
    "\n",
    "    prompt = \"human: {}\\n{}\".format(instruction, input).strip() + \"\\n\\nAssistant: \"\n",
    "    inputs = _tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(_model.device)\n",
    "\n",
    "    generation_output = _model.generate(\n",
    "        input_ids=input_ids,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    for seq in generation_output:\n",
    "        output = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "        print(output)\n",
    "\n",
    "generate(peft_model, tokenizer, \"List five steps for comparing two products.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Config Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.transformer.word_embeddings.weight\n",
      "base_model.model.transformer.word_embeddings_layernorm.weight\n",
      "base_model.model.transformer.word_embeddings_layernorm.bias\n",
      "base_model.model.transformer.h.0.input_layernorm.weight\n",
      "base_model.model.transformer.h.0.input_layernorm.bias\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.0.self_attention.dense.weight\n",
      "base_model.model.transformer.h.0.self_attention.dense.bias\n",
      "base_model.model.transformer.h.0.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.0.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.0.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.0.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.1.input_layernorm.weight\n",
      "base_model.model.transformer.h.1.input_layernorm.bias\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.self_attention.dense.weight\n",
      "base_model.model.transformer.h.1.self_attention.dense.bias\n",
      "base_model.model.transformer.h.1.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.1.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.1.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.1.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.2.input_layernorm.weight\n",
      "base_model.model.transformer.h.2.input_layernorm.bias\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.self_attention.dense.weight\n",
      "base_model.model.transformer.h.2.self_attention.dense.bias\n",
      "base_model.model.transformer.h.2.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.2.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.2.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.2.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.3.input_layernorm.weight\n",
      "base_model.model.transformer.h.3.input_layernorm.bias\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.self_attention.dense.weight\n",
      "base_model.model.transformer.h.3.self_attention.dense.bias\n",
      "base_model.model.transformer.h.3.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.3.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.3.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.3.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.4.input_layernorm.weight\n",
      "base_model.model.transformer.h.4.input_layernorm.bias\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.self_attention.dense.weight\n",
      "base_model.model.transformer.h.4.self_attention.dense.bias\n",
      "base_model.model.transformer.h.4.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.4.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.4.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.4.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.5.input_layernorm.weight\n",
      "base_model.model.transformer.h.5.input_layernorm.bias\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.self_attention.dense.weight\n",
      "base_model.model.transformer.h.5.self_attention.dense.bias\n",
      "base_model.model.transformer.h.5.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.5.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.5.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.5.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.6.input_layernorm.weight\n",
      "base_model.model.transformer.h.6.input_layernorm.bias\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.self_attention.dense.weight\n",
      "base_model.model.transformer.h.6.self_attention.dense.bias\n",
      "base_model.model.transformer.h.6.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.6.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.6.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.6.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.7.input_layernorm.weight\n",
      "base_model.model.transformer.h.7.input_layernorm.bias\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.self_attention.dense.weight\n",
      "base_model.model.transformer.h.7.self_attention.dense.bias\n",
      "base_model.model.transformer.h.7.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.7.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.7.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.7.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.8.input_layernorm.weight\n",
      "base_model.model.transformer.h.8.input_layernorm.bias\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.self_attention.dense.weight\n",
      "base_model.model.transformer.h.8.self_attention.dense.bias\n",
      "base_model.model.transformer.h.8.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.8.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.8.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.8.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.9.input_layernorm.weight\n",
      "base_model.model.transformer.h.9.input_layernorm.bias\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.self_attention.dense.weight\n",
      "base_model.model.transformer.h.9.self_attention.dense.bias\n",
      "base_model.model.transformer.h.9.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.9.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.9.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.9.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.10.input_layernorm.weight\n",
      "base_model.model.transformer.h.10.input_layernorm.bias\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.self_attention.dense.weight\n",
      "base_model.model.transformer.h.10.self_attention.dense.bias\n",
      "base_model.model.transformer.h.10.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.10.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.10.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.10.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.11.input_layernorm.weight\n",
      "base_model.model.transformer.h.11.input_layernorm.bias\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.self_attention.dense.weight\n",
      "base_model.model.transformer.h.11.self_attention.dense.bias\n",
      "base_model.model.transformer.h.11.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.11.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.11.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.11.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.12.input_layernorm.weight\n",
      "base_model.model.transformer.h.12.input_layernorm.bias\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.12.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.12.self_attention.dense.weight\n",
      "base_model.model.transformer.h.12.self_attention.dense.bias\n",
      "base_model.model.transformer.h.12.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.12.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.12.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.12.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.13.input_layernorm.weight\n",
      "base_model.model.transformer.h.13.input_layernorm.bias\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.13.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.13.self_attention.dense.weight\n",
      "base_model.model.transformer.h.13.self_attention.dense.bias\n",
      "base_model.model.transformer.h.13.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.13.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.13.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.13.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.14.input_layernorm.weight\n",
      "base_model.model.transformer.h.14.input_layernorm.bias\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.14.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.14.self_attention.dense.weight\n",
      "base_model.model.transformer.h.14.self_attention.dense.bias\n",
      "base_model.model.transformer.h.14.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.14.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.14.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.14.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.15.input_layernorm.weight\n",
      "base_model.model.transformer.h.15.input_layernorm.bias\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.15.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.15.self_attention.dense.weight\n",
      "base_model.model.transformer.h.15.self_attention.dense.bias\n",
      "base_model.model.transformer.h.15.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.15.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.15.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.15.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.16.input_layernorm.weight\n",
      "base_model.model.transformer.h.16.input_layernorm.bias\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.16.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.16.self_attention.dense.weight\n",
      "base_model.model.transformer.h.16.self_attention.dense.bias\n",
      "base_model.model.transformer.h.16.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.16.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.16.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.16.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.17.input_layernorm.weight\n",
      "base_model.model.transformer.h.17.input_layernorm.bias\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.17.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.17.self_attention.dense.weight\n",
      "base_model.model.transformer.h.17.self_attention.dense.bias\n",
      "base_model.model.transformer.h.17.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.17.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.17.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.17.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.18.input_layernorm.weight\n",
      "base_model.model.transformer.h.18.input_layernorm.bias\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.18.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.18.self_attention.dense.weight\n",
      "base_model.model.transformer.h.18.self_attention.dense.bias\n",
      "base_model.model.transformer.h.18.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.18.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.18.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.18.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.19.input_layernorm.weight\n",
      "base_model.model.transformer.h.19.input_layernorm.bias\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.19.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.19.self_attention.dense.weight\n",
      "base_model.model.transformer.h.19.self_attention.dense.bias\n",
      "base_model.model.transformer.h.19.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.19.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.19.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.19.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.20.input_layernorm.weight\n",
      "base_model.model.transformer.h.20.input_layernorm.bias\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.20.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.20.self_attention.dense.weight\n",
      "base_model.model.transformer.h.20.self_attention.dense.bias\n",
      "base_model.model.transformer.h.20.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.20.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.20.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.20.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.21.input_layernorm.weight\n",
      "base_model.model.transformer.h.21.input_layernorm.bias\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.21.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.21.self_attention.dense.weight\n",
      "base_model.model.transformer.h.21.self_attention.dense.bias\n",
      "base_model.model.transformer.h.21.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.21.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.21.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.21.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.22.input_layernorm.weight\n",
      "base_model.model.transformer.h.22.input_layernorm.bias\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.22.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.22.self_attention.dense.weight\n",
      "base_model.model.transformer.h.22.self_attention.dense.bias\n",
      "base_model.model.transformer.h.22.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.22.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.22.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.22.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.h.23.input_layernorm.weight\n",
      "base_model.model.transformer.h.23.input_layernorm.bias\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.base_layer.weight\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.base_layer.bias\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_A.default.weight\n",
      "base_model.model.transformer.h.23.self_attention.query_key_value.lora_B.default.weight\n",
      "base_model.model.transformer.h.23.self_attention.dense.weight\n",
      "base_model.model.transformer.h.23.self_attention.dense.bias\n",
      "base_model.model.transformer.h.23.post_attention_layernorm.weight\n",
      "base_model.model.transformer.h.23.post_attention_layernorm.bias\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.weight\n",
      "base_model.model.transformer.h.23.mlp.dense_h_to_4h.bias\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.weight\n",
      "base_model.model.transformer.h.23.mlp.dense_4h_to_h.bias\n",
      "base_model.model.transformer.ln_f.weight\n",
      "base_model.model.transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "# to show all trainables layers in the model\n",
    "\n",
    "for name, param in peft_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,654,208 || all params: 1,067,968,512 || trainable%: 0.2485\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BloomForCausalLM(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 1536)\n",
       "        (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4608, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): lora.Linear(\n",
       "                (base_layer): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=6144, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Layers to add lora\n",
    "\n",
    "# use parameter \"target_modules\" to add lora weights to the layers\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, target_modules=[\"query_key_value\", \"dense_4h_to_h\"])\n",
    "peft_model_var = get_peft_model(model, config)\n",
    "print(peft_model_var.print_trainable_parameters())\n",
    "peft_model_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 388,005,888 || all params: 1,453,320,192 || trainable%: 26.6979\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## add other trainable layers besides lora\n",
    "\n",
    "# here we enable the training for word_embeddings layers\n",
    "# It can be seen that the number of trainable parameters increased\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                    target_modules=[\"query_key_value\", \"dense_4h_to_h\"], \n",
    "                    modules_to_save=[\"word_embeddings\"])\n",
    "\n",
    "peft_model_var = get_peft_model(model, config)\n",
    "print(peft_model_var.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. combine weights\n",
    "\n",
    "Here we would like to integrate lora weights to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained models\n",
    "\n",
    "ckp = \"bigscience/bloomz-1b1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(ckp)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lora weights\n",
    "\n",
    "peft_ckp = \"./checkpoint/checkpoint-100\" # changed to the wanted checkpoint path\n",
    "peft_model = PeftModel.from_pretrained(model, model_id=peft_ckp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 1536)\n",
       "    (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "          (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integrate weights\n",
    "# the merged model becomes \"BloomForCausalLM\" instead of \"PeftModelForCausalLM\"\n",
    "\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save merged model\n",
    "\n",
    "merged_model.save_pretraind(\"./checkpoint/lora/merged_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
