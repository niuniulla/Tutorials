{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From prompt tuning, we see that soft prompt tuning is not as effective as hard prompt tuning. However, there are ways to improve soft prompt tuning, one of which is p-tuning.\n",
    "\n",
    "It consists of adding an encoding model to the embedding model:\n",
    "\n",
    "    ____________________________________________________________\n",
    "    |      prompt embedding          |    Embedding            |\n",
    "    ------------------------------------------------------------\n",
    "    |_____________ __________________|____________ ____________|\n",
    "                  |                               |\n",
    "            Prompt Encoder                  base model Input\n",
    "                  ^\n",
    "                  |\n",
    "        lsmt/mlp + prompt Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare example\n",
    "\n",
    "# we use the same example as butfit\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "\n",
    "ckp_data = \"yahma/alpaca-cleaned\"\n",
    "ckp = \"bigscience/bloomz-1b1\"\n",
    "\n",
    "# load dataset\n",
    "data = load_dataset(ckp_data, split=\"train[:1000]\")\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "\n",
    "# process data\n",
    "def process(sample):\n",
    "\n",
    "    MAX_LEN = 256\n",
    "\n",
    "    human = tokenizer(\"Human: \" + \"\\n\".join([sample[\"instruction\"], sample[\"input\"]]).strip() + \"\\n\\nAssistant: \")\n",
    "    ml = tokenizer(sample[\"output\"] + tokenizer.eos_token)\n",
    "\n",
    "    input_ids = human[\"input_ids\"] + ml[\"input_ids\"]\n",
    "    attention_mask = human[\"attention_mask\"] + ml[\"attention_mask\"]\n",
    "    labels = [-100] * len(human[\"input_ids\"]) + ml[\"input_ids\"]\n",
    "\n",
    "    if len(input_ids) > MAX_LEN:\n",
    "\n",
    "        input_ids = input_ids[:MAX_LEN]\n",
    "        attention_mask = attention_mask[:MAX_LEN]\n",
    "        labels = labels[:MAX_LEN]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# tokenize dataset\n",
    "tokenized_data = data.map(process, remove_columns=data.column_names)\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(ckp, low_cpu_mem_usage=True)\n",
    "\n",
    "# send to device\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1.065314304 GB\n",
      "total required memory:  21.31 GB\n"
     ]
    }
   ],
   "source": [
    "# compute model size\n",
    "\n",
    "params = sum(param.numel() for param in model.parameters())\n",
    "print(\"model size: \", params/1e9, \"GB\")\n",
    "print(\"total required memory: \", round(params/1e9 * (4 + 4 + 12), 2), \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. P-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptEncoderConfig(peft_type=<PeftType.P_TUNING: 'P_TUNING'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, num_virtual_tokens=10, token_dim=None, num_transformer_submodules=None, num_attention_heads=None, num_layers=None, encoder_reparameterization_type=<PromptEncoderReparameterizationType.MLP: 'MLP'>, encoder_hidden_size=None, encoder_num_layers=2, encoder_dropout=0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config\n",
    "\n",
    "from peft import PromptEncoderConfig, get_peft_model, TaskType\n",
    "\n",
    "# soft prompt\n",
    "# by default the encoder type is mlp\n",
    "config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): BloomForCausalLM(\n",
       "    (transformer): BloomModel(\n",
       "      (word_embeddings): Embedding(250880, 1536)\n",
       "      (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (h): ModuleList(\n",
       "        (0-23): 24 x BloomBlock(\n",
       "          (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attention): BloomAttention(\n",
       "            (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BloomMLP(\n",
       "            (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (gelu_impl): BloomGelu()\n",
       "            (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1536, out_features=250880, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PromptEncoder(\n",
       "      (embedding): Embedding(10, 1536)\n",
       "      (mlp_head): Sequential(\n",
       "        (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(250880, 1536)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrap model\n",
    "\n",
    "peft_model = get_peft_model(model, config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,097,856 || all params: 1,072,412,160 || trainable%: 0.6619\n"
     ]
    }
   ],
   "source": [
    "# compared to prompt tuning, the trainable parameters is much greater\n",
    "\n",
    "# trainable params: 15,360 || all params: 1,065,329,664 || trainable%: 0.0014\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 05:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.352100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.994600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.994800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.987700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=2.0450433654785156, metrics={'train_runtime': 331.0104, 'train_samples_per_second': 9.063, 'train_steps_per_second': 1.133, 'total_flos': 1670056200806400.0, 'train_loss': 2.0450433654785156, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compared to prompt tuning, the training is much slower since the trainable parameters are greater\n",
    "# but the loss decreases more too.\n",
    "\n",
    "# define training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../tmp/checkpoint\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
    ")\n",
    "\n",
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: List five steps for comparing two products.\n",
      "None\n",
      "\n",
      "Assistant:  No\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "# if we use the pipeline as in bitfit, we got error: The model 'PeftModelForCausalLM' is not \n",
    "# supported for text-generation. Because the pipeline doesn't support causal ml yet.\n",
    "# so we have to generate the result by ourself.\n",
    "\n",
    "# the result doesn't seem to be right, which means that the training is not good.\n",
    "# This is due to that the training is not enough. For soft prompt training, we need more\n",
    "# training to be effective.\n",
    "\n",
    "def generate(model, tokenizer, instruction, input=None):\n",
    "\n",
    "    prompt = \"human: {}\\n{}\".format(instruction, input).strip() + \"\\n\\nAssistant: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    for seq in generation_output:\n",
    "        output = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "        print(output)\n",
    "\n",
    "generate(model, tokenizer, \"List five steps for comparing two products.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: List five steps for comparing two products.\n",
      "None\n",
      "\n",
      "Assistant: 1. Choose a product to compare. 2. Choose a comparison method. 3. Choose a reference product. 4. Choose a reference method. 5. Choose a reference reference product.\n"
     ]
    }
   ],
   "source": [
    "# inference after finetuning\n",
    "\n",
    "generate(peft_model, tokenizer, \"List five steps for comparing two products.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Layer Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the some parameters of the p-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to lstm instead of mlp\n",
    "# Depending on the vaiable values, the trainable parameter numbers will \n",
    "# change and will impact the trainign process and result\n",
    "\n",
    "from peft import PromptEncoderReparameterizationType\n",
    "\n",
    "config = PromptEncoderConfig(task_type=TaskType.CAUSAL_LM, num_virtual_tokens=10,\n",
    "                             encoder_reparameterization_type=PromptEncoderReparameterizationType.LSTM,\n",
    "                             encoder_dropout=0.1,\n",
    "                             encoder_num_layers=5,\n",
    "                             encoder_hidden_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 129,075,712 || all params: 1,194,390,016 || trainable%: 10.8068\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# compared to 3 layer mlp, there are 7m parameters to train, we have here 129M to train\n",
    "\n",
    "model =  get_peft_model(model, config)\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-Tuning can improve the soft prompt finetuning performance but with more trainable parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
