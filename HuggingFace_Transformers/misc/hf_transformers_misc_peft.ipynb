{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Parameter-Efficient Fine-Tuning (PEFT) is a lib to help finetuning large models. The doc is: https://huggingface.co/docs/peft/index.\n",
    "\n",
    "To customize finetuning using peft.\n",
    "\n",
    "The steps:\n",
    " 1. define configuration: config = XXXConfig(task_type=)\n",
    " 2. get model: peft_model = get_peft_model(model, config)\n",
    "\n",
    "for loading, we use:\n",
    " - peft_model = PeftModel.from_pretrained(chekpoint)\n",
    " - integration: model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. custome model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a simple model as base model\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lora\n",
    "\n",
    "# This requires the peft to add lora to the layer 0\n",
    "\n",
    "config = LoraConfig(target_modules=[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the base model to peft model\n",
    "\n",
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=100, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (default): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (default): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (default): Linear(in_features=8, out_features=100, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It changed from base model (sequential model) to a peft model.\n",
    "\n",
    "# The layer 0 in the base model was wraped as \"lora.Linear\",\n",
    "# in which 2 additional linear layers were added: lora_A and lora_B\n",
    "# but their names are \"default\".\n",
    "\n",
    "# The 2 others layers in the base model were unchanged.\n",
    "\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. multiple adpater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have several loras with the same base model, we don't need to load several time the base model. Instead, we load only once the base model and active the lora we need at each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "# we create 2 lora with a base model and save them\n",
    "# 2 folders named \"lora1\" and \"lora2\" were created.\n",
    "# and in each folder, stores the files concerning the lora info\n",
    "\n",
    "path = \"../tmp/checkpoint/\"\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "model\n",
    "\n",
    "# lora 1 add lora to the layer 0\n",
    "config1 = LoraConfig(target_modules=[\"0\"])\n",
    "model1 = get_peft_model(model, config1)\n",
    "model1.save_pretrained(path+\"lora1\")\n",
    "\n",
    "# lora 1 add lora to the layer 2\n",
    "config2 = LoraConfig(target_modules=[\"2\"])\n",
    "model2 = get_peft_model(model, config2)\n",
    "model2.save_pretrained(path+\"lora2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, we recreate the base model (we can also load it if saved)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=100, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (lora1): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (lora1): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (lora1): Linear(in_features=8, out_features=100, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=100, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load lora 1\n",
    "\n",
    "# so the name of the added layers becomes \"lora1\" instead of \"default\"\n",
    "# since we provide a adapter name using \"adapter_name=\"\n",
    "# This is required for pultiple lora.\n",
    "\n",
    "load_model = PeftModel.from_pretrained(model, model_id=path+\"lora1\", adapter_name=\"lora1\")\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Sequential(\n",
       "      (0): lora.Linear(\n",
       "        (base_layer): Linear(in_features=10, out_features=100, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (lora1): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (lora1): Linear(in_features=10, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (lora1): Linear(in_features=8, out_features=100, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "      (1): ReLU()\n",
       "      (2): lora.Linear(\n",
       "        (base_layer): Linear(in_features=100, out_features=10, bias=True)\n",
       "        (lora_dropout): ModuleDict(\n",
       "          (lora2): Identity()\n",
       "        )\n",
       "        (lora_A): ModuleDict(\n",
       "          (lora2): Linear(in_features=100, out_features=8, bias=False)\n",
       "        )\n",
       "        (lora_B): ModuleDict(\n",
       "          (lora2): Linear(in_features=8, out_features=10, bias=False)\n",
       "        )\n",
       "        (lora_embedding_A): ParameterDict()\n",
       "        (lora_embedding_B): ParameterDict()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then load the lora2\n",
    "# the second layer was loaded and named 'lora2'\n",
    "\n",
    "load_model.load_adapter(path+\"lora2\", adapter_name='lora2')\n",
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lora1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see which adapter is active\n",
    "\n",
    "# By default, the first loaded adapter is active\n",
    "\n",
    "load_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1971, -0.1050,  0.1308, -0.2249, -0.0797, -0.0572, -0.3061,  0.0732,\n",
       "          0.0448,  0.1013]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on lora1 since it is the active one\n",
    "\n",
    "load_model(torch.ones((1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1971, -0.1050,  0.1308, -0.2249, -0.0797, -0.0572, -0.3061,  0.0732,\n",
       "          0.0448,  0.1013]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we set all parameters of lora1 to 0 to get the base model outputs\n",
    "# this is the same as before, it means that the lora1's parameters were initialized to 0\n",
    "\n",
    "for name, params in load_model.named_parameters():\n",
    "\n",
    "    if name in [\"base_model.model.0.lora_A.lora1.weight\", \"base_model.model.0.lora_B.lora1.weight\"]:\n",
    "        params.data = torch.zeros(params.size())\n",
    "        \n",
    "load_model(torch.ones((1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to lora 2\n",
    "\n",
    "load_model.set_adapter(\"lora2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lora2'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this shows that the active lora was changed to lora2\n",
    "\n",
    "load_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1971, -0.1050,  0.1308, -0.2249, -0.0797, -0.0572, -0.3061,  0.0732,\n",
       "          0.0448,  0.1013]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on lora 2\n",
    "# this is the base model output since the lora 2 is initialized to 0 too\n",
    "\n",
    "load_model(torch.ones((1,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. get base model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1971, -0.1050,  0.1308, -0.2249, -0.0797, -0.0572, -0.3061,  0.0732,\n",
       "          0.0448,  0.1013]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the base model output since the loras's parameters were set to 0\n",
    "\n",
    "load_model(torch.ones((1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1971, -0.1050,  0.1308, -0.2249, -0.0797, -0.0572, -0.3061,  0.0732,\n",
      "          0.0448,  0.1013]])\n"
     ]
    }
   ],
   "source": [
    "# we can disable the adapters\n",
    "# we get the same result as before, which means the adapters were effectively disabled\n",
    "\n",
    "with load_model.disable_adapter():\n",
    "    print(load_model(torch.ones((1,10))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
