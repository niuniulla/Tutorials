{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, it will:\n",
    "\n",
    "    I. explain the NER problem.\n",
    "    II. Model\n",
    "    III. Realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation\n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "It allows to abstract designated entity (eg. name, place, brand, product...) in a text.\n",
    "There are 2 parts of this task:\n",
    " * recognize the position of the entory\n",
    " * classify the entiry\n",
    "\n",
    "example: Paul is at the the parc.\n",
    "\n",
    "| type    |  entity |\n",
    "----------|:-------:|\n",
    "| place   |  parc   |\n",
    "| subject |  Paul   |\n",
    "\n",
    "\n",
    "### 2. labeling methods\n",
    "\n",
    "- IOB\n",
    "- IOE\n",
    "- IOBES\n",
    "- BILOU\n",
    "\n",
    "where \n",
    "\n",
    "* I designates the middle of the entity, B the begining and O is not the entity.\n",
    "* E designates the end of the entity, S a single word for entity.\n",
    "* Sometimes, I is placed by M.\n",
    "* the name after the balise is the class\n",
    "\n",
    "* B-Person: the bigining of a person's name\n",
    "* I-Person: the middle of a person's name\n",
    "* B-Place: the bigining of a place's name\n",
    "* I-Place: the middle of a place's name\n",
    "* O is outside of all entities\n",
    "\n",
    "### 3. evaluation\n",
    "\n",
    "- metrics: precision recall, F1\n",
    "\n",
    "eg.\n",
    "\n",
    "\n",
    "|truth:| The | old | lady | whirled | round | and | snatched | her  | skirts | out | of   | danger |\n",
    "|------|:---:|----:|-----:|--------:|------:|----:|---------:|-----:|-------:|----:|-----:|-----:|\n",
    "|gold: | <font color=\"blue\">b-A</font> | <font color=\"blue\">i-A</font> | <font color=\"blue\">i-A</font>  |  <font color=\"blue\">e-A</font>    | o   | o   | b-DSE    |i-DSE | e-DSE  | o   | <font color=\"yellow\">b-T </font>   | <font color=\"yellow\">e-T</font>    |\n",
    "|pred: | o   | <font color=\"pink\">b-A</font> |<font color=\"pink\">e-A</font>  |   o      | b-DSE | o   | <font color=\"red\">b-DSE</font>   |<font color=\"red\">i-DSE</font> | <font color=\"red\">e-DSE</font>  | o   | b-T  | i-T    |\n",
    "\n",
    "\n",
    "The truth has 3 objects marked as blue, white and yellow. The prediction identified 2 objects marked as pink and red, where red is the correct one.\n",
    "\n",
    "So the corresponding confusion matrix is as follow where the columns are the ground truth and the lines are the predictions: \n",
    "\n",
    "|   | P  | N  | \n",
    "|---|---|---|\n",
    "| P | 1  | 1 |\n",
    "| N | 2  | - |\n",
    "\n",
    "$$Precision = \\frac{1}{1+1}$$\n",
    "$$Recall = \\frac{1}{1+2}$$\n",
    "$$F1 = 2 * \\frac{P * R}{P + R} = 2 * \\frac{1/2 * 1/3}{1/2 + 1/3}$$\n",
    "\n",
    "Where \n",
    "* precision measure the rate of correct identifications over the true entities. \n",
    "* recall measures the rate of correct identifications over all predictions. \n",
    "* f1 is the averaged rate of precision and recall.\n",
    "\n",
    "Inspired from : https://huggingface.co/learn/nlp-course/chapter7/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, this is a classification problem,  the model used is AutoModelForTokenClassification, with the bert base model.\n",
    "\n",
    "This model uses bert base model to encode the input text, and output the classes (num_labels) of each tokens according to the classes we defined. So the output dimension is [batch, seq_len, classes].\n",
    "\n",
    "In the class's init function:\n",
    "\n",
    "```python\n",
    "    self.num_labels = config.num_labels\n",
    "    self.bert = BertModel(config, add_pooling_layer=False)\n",
    "    ...\n",
    "    self.dropout = nn.Dropout(classifier_dropout)\n",
    "    self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "```\n",
    "Where num_labels is the number of the labels. In our case, it is 3.\n",
    "\n",
    "In the forward function:\n",
    "\n",
    "```python\n",
    "    outputs = self.bert(\n",
    "            input_ids,\n",
    "            ...\n",
    "        )\n",
    "    sequence_output = outputs[0]\n",
    "    sequence_output = self.dropout(sequence_output)\n",
    "    logits = self.classifier(sequence_output)\n",
    "```\n",
    "\n",
    "The input is encoded using bert model. The output of bert model is then put into a linear layer to project the hidden values to the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not installed, uncomment and run this\n",
    "\n",
    "# !python -m pip install seqeval --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this.\n",
    "# Should be run the first\n",
    "# skip this if you don't need.\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defin repos for data and model\n",
    "\n",
    "# data\n",
    "# https://huggingface.co/datasets/ashleyliu31/tech_product_names_ner\n",
    "\n",
    "ckp_data = \"ashleyliu31/tech_product_names_ner\"\n",
    "\n",
    "# model\n",
    "\n",
    "ckp = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 12:09:09.062874: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-20 12:09:09.062934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-20 12:09:09.065212: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-20 12:09:09.078091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 12:09:11.176857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "## import\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. download dataset\n",
    "\n",
    "Depending on dataset, the formats can vary. For example, the easiest case is where the text was provided cleaned and cut and the labels was provided.\n",
    "But in our case, the text is in string format, with punctuations. So we need to do some basic cleaning before usage.\n",
    "In some other cases, the data may be more complex and need more actions.\n",
    "So, one should always check the data content and make sure it responds the use case before using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'sentence', 'word_labels'],\n",
       "    num_rows: 5828\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "\n",
    "data = load_dataset(ckp_data, cache_dir='../tmp/ner', split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': Value(dtype='int64', id=None),\n",
       " 'sentence': Value(dtype='string', id=None),\n",
       " 'word_labels': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the features of the dataset\n",
    "\n",
    "data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 10, 'sentence': 'The HP Pavilion x360 13-s101tu is highly versatile with its 360-degree hinge and touchscreen display.', 'word_labels': 'O, B-pn, I-pn, I-pn, I-pn, O, O, O, O, O, O, O, O, O, O, O'}\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# if we look closer to the data, we realize that the format of the data are all in string\n",
    "# we have to cut them apart and do some clean-ups to make sure that the text corresponds to the labels\n",
    "\n",
    "# After some inspections, the rules for processing are:\n",
    "#   - the punctuation counted in the labeling\n",
    "#   - words linked by \"-\" counted as one word\n",
    "#   - the words linked by \"'\" should be separated into 2 words, remove the \"'\"\n",
    "\n",
    "\n",
    "ind = 10\n",
    "print(data[ind])\n",
    "print(len(data[ind][\"sentence\"].split(\" \")))\n",
    "print(len(data[ind][\"word_labels\"].split(\", \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but one thing for sure is that the labels consist of 3 symbols that we need later\n",
    "# so we define them here\n",
    "\n",
    "label2id = {'O':0, 'B-pn':1, 'I-pn':2}\n",
    "id2label = {0:'O', 1:'B-pn', 2:'I-pn'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'sentence', 'word_labels'],\n",
       "        num_rows: 4662\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'sentence', 'word_labels'],\n",
       "        num_rows: 1166\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the dataset was already split, skip this\n",
    "\n",
    "# we split data since the original dataset don't provide test set\n",
    "# Only dataset can be split, not the datadict\n",
    "\n",
    "split_data = data.train_test_split(test_size=0.2)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  {'Unnamed: 0': 1816, 'sentence': \"I'm keen on the Asus EeeBook E402WA-WH21, any discounts available?\", 'word_labels': 'O, O, O, O, O, B-pn, I-pn, I-pn, O, O, O, O, O'}\n",
      "text:  ['I', 'm', 'keen', 'on', 'the', 'Asus', 'EeeBook', 'E402WA-WH21', ',', 'any', 'discounts', 'available', '?']\n",
      "tokens:  {'input_ids': [101, 1045, 1049, 10326, 2006, 1996, 2004, 2271, 25212, 15878, 14659, 1041, 12740, 2475, 4213, 1011, 1059, 2232, 17465, 1010, 2151, 19575, 2015, 2800, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "labels:  ['O', 'O', 'O', 'O', 'O', 'B-pn', 'I-pn', 'I-pn', 'O', 'O', 'O', 'O', 'O']\n",
      "text len:  13\n",
      "token len:  26\n",
      "labels len:  13\n"
     ]
    }
   ],
   "source": [
    "# After some tweaks, I found a way to treat the data.\n",
    "# For both sentence and labels:\n",
    "# - remove punctuations (except - and . for model names)\n",
    "# - split the string by space\n",
    "\n",
    "ind = 169 # pick a sample\n",
    "\n",
    "# remove , and . at the end of the sentence (so we replace \". \" instead of \".\")\n",
    "# split the sentence into list\n",
    "sen = split_data[\"train\"][ind][\"sentence\"].replace(\",\", \" , \").replace(\". \", \" . \").replace(\"?\", \" ? \").replace(\"'\", \" \").split()\n",
    "\n",
    "labels = split_data[\"train\"][ind][\"word_labels\"].split(\", \")\n",
    "\n",
    "# we tokenize the sentence\n",
    "# the option \"is_split_into_words\" works on list instead of string\n",
    "tok = tokenizer(sen, is_split_into_words=True)\n",
    "\n",
    "# It can be seen that the tokenized sentence has different length as the labels\n",
    "# This is because the way the tokenizer separate the sentence, which is not solely based\n",
    "# on word level, but the label was based on word.                                  \n",
    "\n",
    "print(\"data: \", split_data[\"train\"][ind])\n",
    "print(\"text: \", sen)\n",
    "print(\"tokens: \", tok)\n",
    "print(\"labels: \", labels)\n",
    "\n",
    "print(\"text len: \", len(sen))\n",
    "print(\"token len: \", len(tok[\"input_ids\"]))\n",
    "print(\"labels len: \", len(split_data[\"train\"][ind][\"word_labels\"].split(\", \")))\n",
    "\n",
    "assert(len(sen) == len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'm', 'keen', 'on', 'the', 'as', '##us', 'ee', '##eb', '##ook', 'e', '##40', '##2', '##wa', '-', 'w', '##h', '##21', ',', 'any', 'discount', '##s', 'available', '?', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 5, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 8, 9, 10, 10, 11, 12, None]\n"
     ]
    }
   ],
   "source": [
    "# The tokenizer provides information about the tokens such as the word it tokenized.\n",
    "# Where None is the special token marks the begining and the end of the sentence\n",
    "# we should take consideration of this info when we re-construct the label\n",
    "\n",
    "# For more details, see hf_transformers_basics_3_tokenizer.ipynb\n",
    "\n",
    "print(tok.tokens()) # show the tokens\n",
    "print(tok.word_ids()) # show the ids of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by putting all above mentioned processing together, we define the function \n",
    "# to preprocess the data\n",
    "\n",
    "def process(examples) :\n",
    "\n",
    "    # split text\n",
    "    texts = [ex.replace(\", \", \" , \").replace(\". \", \" . \").replace(\"?\", \" ? \").replace(\"'s\", \" \").replace(\"’s\", \" \").split() for ex in examples[\"sentence\"]]\n",
    "    \n",
    "    # split labels\n",
    "    labels_raw = [ex.split(\", \") for ex in examples[\"word_labels\"]]\n",
    "\n",
    "    # tokenization of text\n",
    "    tokenized = tokenizer(texts, is_split_into_words=True, max_length=128, truncation=True)\n",
    "    \n",
    "    labels = []\n",
    "\n",
    "    for i, lab in enumerate(labels_raw):\n",
    "\n",
    "        if len(lab) < len(texts[i]):\n",
    "            print(lab, texts[i], examples[\"sentence\"][i])\n",
    "        \n",
    "        # convert string label to num label\n",
    "        lab = [label2id[x] for x in lab] \n",
    "\n",
    "        # match label length to token length\n",
    "        label = [-100 if l is None else lab[int(l)] for l in tokenized.word_ids(batch_index=i)] \n",
    "\n",
    "        labels.append(label)\n",
    "\n",
    "        \n",
    "        assert len(label) == len(tokenized[\"input_ids\"][i])\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5561151f431247f4918aa69ea894cddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4662 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e61c63f207b47938c291ec9fc94b405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'sentence', 'word_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4662\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'sentence', 'word_labels', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1166\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the data\n",
    "\n",
    "tokenized_data = split_data.map(process, batched=True)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': 4050, 'sentence': 'I found the Ziox Astra Titan 4G to be durable with decent performance.', 'word_labels': 'O, O, O, B-pn, I-pn, I-pn, I-pn, O, O, O, O, O, O, O', 'input_ids': [101, 1045, 2179, 1996, 1062, 3695, 2595, 2004, 6494, 16537, 1018, 2290, 2000, 2022, 25634, 2007, 11519, 2836, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, -100]}\n"
     ]
    }
   ],
   "source": [
    "# show preprocessed data\n",
    "print(tokenized_data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# By default, the model output dim is 2.\n",
    "# However, we need a 3-class classifier.\n",
    "# So to change the output dim, we use the option \"num_labels\" to specify the required dim.\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(ckp, num_labels=len(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can check the output dim is indeed 3\n",
    "\n",
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"seqeval\", module_type: \"metric\", features: {'predictions': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='label'), length=-1, id='sequence')}, usage: \"\"\"\n",
       "Produces labelling scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions: List of List of predicted labels (Estimated targets as returned by a tagger)\n",
       "    references: List of List of reference labels (Ground truth (correct) target values)\n",
       "    suffix: True if the IOB prefix is after type, False otherwise. default: False\n",
       "    scheme: Specify target tagging scheme. Should be one of [\"IOB1\", \"IOB2\", \"IOE1\", \"IOE2\", \"IOBES\", \"BILOU\"].\n",
       "        default: None\n",
       "    mode: Whether to count correct entity labels with incorrect I/B tags as true positives or not.\n",
       "        If you want to only count exact matches, pass mode=\"strict\". default: None.\n",
       "    sample_weight: Array-like of shape (n_samples,), weights for individual samples. default: None\n",
       "    zero_division: Which value to substitute as a metric value when encountering zero division. Should be on of 0, 1,\n",
       "        \"warn\". \"warn\" acts as 0, but the warning is raised.\n",
       "\n",
       "Returns:\n",
       "    'scores': dict. Summary of the scores for overall and per type\n",
       "        Overall:\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure,\n",
       "        Per type:\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': F1 score, also known as balanced F-score or F-measure\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [['O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> references = [['O', 'O', 'O', 'B-MISC', 'I-MISC', 'I-MISC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
       "    >>> seqeval = evaluate.load(\"seqeval\")\n",
       "    >>> results = seqeval.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['MISC', 'PER', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
       "    >>> print(results[\"overall_f1\"])\n",
       "    0.5\n",
       "    >>> print(results[\"PER\"][\"f1\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define evaluation module\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eval_metric(preds):\n",
    "\n",
    "    pred, label = preds\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "    pred_word_labels = []\n",
    "    label_word_labels = []\n",
    "\n",
    "    for p, l in zip(pred, label) :\n",
    "        if len(p) != len(l):\n",
    "            print(p)\n",
    "            print(l)\n",
    "\n",
    "        pred_word_labels.append([id2label[i] for i, j in zip(p, l) if j != -100])\n",
    "\n",
    "        label_word_labels.append([id2label[i] for i in l if i != -100 ])\n",
    "\n",
    "    result = seqeval.compute(predictions=pred_word_labels, references=label_word_labels, mode=\"strict\", scheme=\"IOB2\")\n",
    "    \n",
    "    return {'f1': result[\"overall_f1\"]} # we have to reture a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Qingyi/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "        output_dir=\"../tmp/checkpoints\",\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=128,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        metric_for_best_model=\"f1\",\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=10,\n",
    "        use_cpu=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=eval_metric,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. train + eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 05:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2252492904663086,\n",
       " 'eval_f1': 0.046061722708429294,\n",
       " 'eval_runtime': 22.7805,\n",
       " 'eval_samples_per_second': 51.184,\n",
       " 'eval_steps_per_second': 0.439}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219/219 13:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.032999</td>\n",
       "      <td>0.945546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>0.970661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.016284</td>\n",
       "      <td>0.979146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=219, training_loss=0.042451643678423474, metrics={'train_runtime': 828.9591, 'train_samples_per_second': 16.872, 'train_steps_per_second': 0.264, 'total_flos': 245454797909268.0, 'train_loss': 0.042451643678423474, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.014311173930764198,\n",
       " 'eval_f1': 0.9818445896877269,\n",
       " 'eval_runtime': 20.1221,\n",
       " 'eval_samples_per_second': 57.946,\n",
       " 'eval_steps_per_second': 0.497,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_data[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "##########\n",
    "\n",
    "# use pipeline to evaluate the model\n",
    "pipe_ner = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"google-bert/bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-pn\",\n",
      "    \"2\": \"I-pn\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B-pn\": 1,\n",
      "    \"I-pn\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we have to customized the label-id conversion to output meaningful result\n",
    "# otherwise, the results will only show default labels\n",
    "\n",
    "model.config.label2id = label2id\n",
    "model.config.id2label = id2label\n",
    "print(model.config) # show the configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'pn',\n",
       "  'score': 0.9971613,\n",
       "  'word': 'hp laserjet',\n",
       "  'start': 10,\n",
       "  'end': 21},\n",
       " {'entity_group': 'pn',\n",
       "  'score': 0.9889905,\n",
       "  'word': 'apple laserwriter',\n",
       "  'start': 36,\n",
       "  'end': 53},\n",
       " {'entity_group': 'pn',\n",
       "  'score': 0.6539879,\n",
       "  'word': 'cx',\n",
       "  'start': 92,\n",
       "  'end': 94}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_ner(\"The first HP LaserJet and the first Apple LaserWriter used the same print engine, the Canon CX engine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toks:  tensor([[  101,  1996,  2034,  6522,  9138, 15759,  1998,  1996,  2034,  6207,\n",
      "          9138, 15994,  2109,  1996,  2168,  6140,  3194,  1010,  1996,  9330,\n",
      "          1039,  2595,  3194,   102]])\n",
      "logits:  tensor([[[ 3.8971, -1.8646, -1.2216],\n",
      "         [ 5.5416, -2.4371, -2.2886],\n",
      "         [ 4.0385, -1.0274, -2.5045],\n",
      "         [-1.5644,  5.1154, -3.4098],\n",
      "         [-2.5015, -1.8554,  4.3861],\n",
      "         [-2.1167, -2.1242,  4.0641],\n",
      "         [ 2.3298, -1.1273, -0.3853],\n",
      "         [ 4.5360, -1.5070, -2.2281],\n",
      "         [ 3.1980, -0.2681, -2.2158],\n",
      "         [-0.2964,  3.8050, -2.9993],\n",
      "         [-2.2740, -1.6701,  4.0373],\n",
      "         [-1.6740, -1.6256,  3.5832],\n",
      "         [ 5.0597, -2.8325, -1.4988],\n",
      "         [ 5.7788, -3.1890, -2.2796],\n",
      "         [ 4.7521, -2.6377, -1.8266],\n",
      "         [ 4.3349, -2.4598, -1.4123],\n",
      "         [ 5.6720, -2.7675, -1.8111],\n",
      "         [ 4.5814, -2.5581, -1.3707],\n",
      "         [ 5.7639, -2.7239, -2.0398],\n",
      "         [ 2.6216,  0.9343, -3.0131],\n",
      "         [ 0.4343, -1.0765,  1.2448],\n",
      "         [ 0.5471, -1.2209,  1.3670],\n",
      "         [ 2.4605, -2.2622,  0.5296],\n",
      "         [ 6.0138, -2.8643, -2.8050]]], grad_fn=<ViewBackward0>)\n",
      "predicted class ids:  tensor([0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0])\n",
      "predicted class labels:  ['O', 'O', 'O', 'B-pn', 'I-pn', 'I-pn', 'O', 'O', 'O', 'B-pn', 'I-pn', 'I-pn', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-pn', 'I-pn', 'O', 'O']\n",
      "entity ids:  [[6522, 9138, 15759], [6207, 9138, 15994], [1039, 2595], []]\n",
      "entity  0 :  hp laserjet\n",
      "entity  1 :  apple laserwriter\n",
      "entity  2 :  cx\n"
     ]
    }
   ],
   "source": [
    "# manual\n",
    "########\n",
    "\n",
    "toks = tokenizer(\"The first HP LaserJet and the first Apple LaserWriter used the same print engine, the Canon CX engine\", return_tensors=\"pt\")\n",
    "print(\"toks: \", toks.input_ids)\n",
    "\n",
    "logits = model(**toks).logits\n",
    "print(\"logits: \", logits)\n",
    "\n",
    "id_cls = logits.argmax(axis=-1)\n",
    "print(\"predicted class ids: \", id_cls[0])\n",
    "\n",
    "preds_cls = [id2label.get(p.item()) for p in id_cls[0]]\n",
    "print(\"predicted class labels: \", preds_cls)\n",
    "\n",
    "\n",
    "word_ids = []\n",
    "word = []\n",
    "old = -1\n",
    "for ind, i in enumerate(id_cls[0].numpy()):\n",
    "    if old != 0 and i == 0:\n",
    "        word = []\n",
    "        word_ids.append(word)\n",
    "    if i != 0:\n",
    "        word.append(toks.input_ids[0][ind].item())\n",
    "    old = i\n",
    "print(\"entity ids: \", word_ids)\n",
    "\n",
    "for i, w in enumerate(word_ids):\n",
    "    if len(w) > 0:\n",
    "        print(\"entity \", i, \": \", tokenizer.decode(w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
