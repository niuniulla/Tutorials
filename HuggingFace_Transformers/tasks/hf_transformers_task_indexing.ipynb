{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, it will:\n",
    "\n",
    "    I. explain the faq problem.\n",
    "    III. Realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The faq problem is to match the question and return the answer of the question which is closest to the query. The model is the same as the match.\n",
    "For this problem, there isn't a precise model but a strategy to get the final result.\n",
    "The idea behind this is to \n",
    " * use the model of similarity to encode all candidates and find several candidates\n",
    " * use the matching model to find the best one among the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.10/site-packages (from faiss-cpu) (1.26.2)\n",
      "Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    }
   ],
   "source": [
    "# install extra module\n",
    "\n",
    "!python -m pip install faiss-cpu --break-system-packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this.\n",
    "# Should be run the first\n",
    "# skip this if you don't need.\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defin repos for data and model\n",
    "\n",
    "# data\n",
    "\n",
    "ckp_data = \"akshatshah1103/retail-faq\"\n",
    "\n",
    "# model\n",
    "\n",
    "ckp = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 11:48:43.007063: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-21 11:48:43.007115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-21 11:48:43.009259: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-21 11:48:43.020202: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-21 11:48:45.092759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a041f6bfc0da4f5e91dbd4eff681d177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd9f15192e14601a402c47dbb5ffdab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef224dda011d468aa996f0cc9b4e07b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e1c2cab32e4e239627cc2b60825e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2fc8abb230488984adbaff11aa9735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['FAQ', 'Response'],\n",
       "        num_rows: 112\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(ckp_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FAQ': \"What are your store's operating hours?\",\n",
       " 'Response': 'Our store is open from 10:00 AM to 8:00 PM, Monday through Saturday, and from 11:00 AM to 6:00 PM on Sundays.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['FAQ', 'Response'],\n",
       "        num_rows: 89\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['FAQ', 'Response'],\n",
       "        num_rows: 23\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = data[\"train\"].train_test_split(test_size=0.2)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimilarityModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the same model as for matching\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch.nn import CosineEmbeddingLoss, CosineSimilarity\n",
    "\n",
    "class SimilarityModel(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        #print(input_ids)\n",
    "        s1_input_ids, s2_input_ids = input_ids[0,:], input_ids[:,1]\n",
    "        s1_attention_mask, s2_attention_mask = attention_mask[0,:], attention_mask[:,1]\n",
    "        s1_token_type_ids, s2_token_type_ids = token_type_ids[0,:], token_type_ids[:,1]\n",
    "\n",
    "        s1_outputs = self.bert(\n",
    "            s1_input_ids,\n",
    "            attention_mask=s1_attention_mask,\n",
    "            token_type_ids=s1_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        s1_pooled_output = s1_outputs[1]\n",
    "\n",
    "        s2_outputs = self.bert(\n",
    "            s2_input_ids,\n",
    "            attention_mask=s2_attention_mask,\n",
    "            token_type_ids=s2_token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        s2_pooled_output = s2_outputs[1]\n",
    "\n",
    "        simi = CosineSimilarity()(s1_pooled_output, s2_pooled_output)\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "\n",
    "            loss_fct = CosineEmbeddingLoss(0.3)\n",
    "            loss = loss_fct(s1_pooled_output, s2_pooled_output, labels)\n",
    "\n",
    "        output = (simi,)\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "\n",
    "model = SimilarityModel.from_pretrained(ckp)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:01<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# encode questions\n",
    "from tqdm import tqdm\n",
    "\n",
    "def encode_batch(data, batch_size):\n",
    "\n",
    "    encodes = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for i in tqdm(range(0, len(data[\"train\"]), batch_size)):\n",
    "\n",
    "            batch_sens = [data[\"train\"][ind+i][\"FAQ\"] for ind in range(batch_size) if ind+i < len(data[\"train\"])]\n",
    "\n",
    "            toks = tokenizer(batch_sens, max_length=128, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "            vec = model.bert(**toks)[1] # encode the data into a vector of length of hidden size (768)\n",
    "\n",
    "            encodes.append(vec)\n",
    "\n",
    "    encodes = torch.cat(encodes, dim=0).cpu().numpy()\n",
    "\n",
    "    return encodes\n",
    "\n",
    "encodes = encode_batch(data, 16)\n",
    "print(encodes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. create indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss_avx2.IndexFlatIP; proxy of <Swig Object of type 'faiss::IndexFlatIP *' at 0x7fc5880b3b40> >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing using faiss\n",
    "\n",
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatIP(768)\n",
    "faiss.normalize_L2(encodes)\n",
    "\n",
    "index.add(encodes)\n",
    "\n",
    "index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. encode query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "# test search\n",
    "\n",
    "def encode(question):\n",
    "\n",
    "    with torch.inference_mode():\n",
    "\n",
    "            toks = tokenizer(question, max_length=128, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "            vec = model.bert(**toks)[1]\n",
    "\n",
    "    return vec.cpu().numpy()\n",
    "\n",
    "ques = \"When it open\"\n",
    "\n",
    "vec = encode(ques)\n",
    "\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.91554874, 0.91554874, 0.91554874, 0.91554874, 0.91554874,\n",
      "        0.91554874, 0.91554874, 0.91554874, 0.91554874, 0.91554874,\n",
      "        0.88546276, 0.88546276, 0.88546276, 0.88546276, 0.8487283 ,\n",
      "        0.8466814 , 0.8257435 , 0.81071067, 0.7890881 , 0.75049907]],\n",
      "      dtype=float32), array([[ 31,  30,  29,  28,  27,  26,  25,  24,  23,  22,  67,  66,  65,\n",
      "         64, 110, 102,  92,  97, 100,  16]]))\n",
      "['How can I track my order?', 'How can I track my order?', 'How can I track my order?', 'How can I track my order?', 'How can I track my order?', 'How can I track my order?', 'How can I track my order?', 'How can I track my order?', 'How can I track my order?', 'How can I track my order?', 'Can I return an item without a receipt?', 'Can I return an item without a receipt?', 'Can I return an item without a receipt?', 'Can I return an item without a receipt?', 'How do I track my order when I order through Dunzo or Blinkit?', 'How do I cancel a pre-order if I change my mind?', 'Do you carry organic produce in your grocery section?', 'Can I pre-order upcoming electronic gadgets in advance?', 'Can I pre-order groceries for a specific date or event?', 'Do you have a customer support hotline?']\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# search\n",
    "\n",
    "faiss.normalize_L2(vec)\n",
    "tops = index.search(vec, 20) # find top 20 candidats\n",
    "\n",
    "print(tops)\n",
    "\n",
    "res = []\n",
    "for score, ind in zip(tops[0][0], tops[1][0]):\n",
    "\n",
    "    ind = int(ind)\n",
    "    matched_ques = data[\"train\"][ind][\"FAQ\"]\n",
    "    matched_resp = data[\"train\"][ind][\"Response\"]\n",
    "    res.append([matched_ques, matched_resp])\n",
    "\n",
    "candidates = {}\n",
    "candidates[\"question\"] = [i for i, j in res]\n",
    "candidates[\"respond\"] = [j for i, j in res]\n",
    "\n",
    "print(candidates[\"question\"])\n",
    "print(type(candidates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. refine result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# match the recalls\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "cross_model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15)\n"
     ]
    }
   ],
   "source": [
    "# we match the candidates to the query question to find the best one\n",
    "\n",
    "questions = [ques] * len(candidates[\"question\"])\n",
    "\n",
    "toks = tokenizer(questions, candidates[\"question\"], max_length=128, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "toks = {k: v.to(cross_model.device) for k, v in toks.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    logits = cross_model(**toks).logits.squeeze()\n",
    "    res = torch.argmax(logits, dim=-1)\n",
    "    print(res)\n",
    "\n",
    "res = int(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How do I cancel a pre-order if I change my mind?', \"To cancel a pre-order, simply contact our customer support team at [customer support email or phone number]. They'll assist you in canceling your pre-order and processing any necessary refunds.\"]\n"
     ]
    }
   ],
   "source": [
    "matched_ques = candidates[\"question\"][res]\n",
    "matched_resp = candidates[\"respond\"][res]\n",
    "print([matched_ques, matched_resp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
