{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, it will:\n",
    "\n",
    "    I. Explain the problem\n",
    "    II. Model\n",
    "    III. Realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Definition\n",
    "\n",
    "This is a case of MRC problem. Instead of locate the answer in the text explained in MRC, \n",
    "\n",
    "The problem can be summarized as :\n",
    " - given a context (optional) and a question\n",
    " - providing several choices\n",
    " - predict one or several answers\n",
    "\n",
    "Example:\n",
    " - context: earthworms can regrow segments that break off\t\n",
    " - Question: what do earthworms do when a segment breaks off \n",
    " - choices: (a) dies (b) regrows it (c) reproduces (d) sediment (e) root growth (f) migrate (g) stops growing (h) roots\t\n",
    " - answer: b\n",
    "\n",
    "\n",
    "\n",
    "### 2. data processing\n",
    "\n",
    "    ______________________________________________________________________________\n",
    "    |CLS|       context       |SEP|      question     |SEP|      choice1     |SEP|\n",
    "    ------------------------------------------------------------------------------\n",
    "    |CLS|       context       |SEP|      question     |SEP|      choice2     |SEP|\n",
    "    ------------------------------------------------------------------------------\n",
    "    ...                                     ... \n",
    "    ...                                     ...\n",
    "\n",
    "    Then\n",
    "     ___________________________\n",
    "     |CLS1| |CLS2| |CLS3| |CLS4|    ->  choice\n",
    "     ---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used is AutoModelForMultipleChoice, with the bert base model.\n",
    "\n",
    "This model uses bert base model to encode the input text, and output the classes (num_labels) of each tokens according to the classes we defined. So the output dimension is [batch, seq_len, classes].\n",
    "\n",
    "In the class's init function:\n",
    "\n",
    "```python\n",
    "    self.num_labels = config.num_labels\n",
    "    self.bert = BertModel(config, add_pooling_layer=False)\n",
    "    ...\n",
    "    self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "```\n",
    "Where num_labels in this context is 1. We don't have to redefine.\n",
    "\n",
    "In the forward function:\n",
    "\n",
    "```python\n",
    "    num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
    "    input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None # [batch * num_choice, seq_len]\n",
    "    ...\n",
    "    outputs = self.bert(\n",
    "            input_ids,\n",
    "            ...\n",
    "        )\n",
    "    pooled_output = outputs[1]                      # [batch * num_choice, hidden_size]\n",
    "\n",
    "    pooled_output = self.dropout(pooled_output)\n",
    "    logits = self.classifier(pooled_output)         # [batch * num_choice, 1]\n",
    "    reshaped_logits = logits.view(-1, num_choices)  # [batch,  num_choice]\n",
    "```\n",
    "\n",
    "The input is encoded using bert model. The output of bert model is then put into a linear layer to project the hidden values to the choice space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this.\n",
    "# Should be run the first\n",
    "# skip this if you don't need.\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defin repos for data and model\n",
    "\n",
    "# data\n",
    "\n",
    "ckp_data = \"layoric/labeled-multiple-choice-explained\"\n",
    "\n",
    "# model\n",
    "\n",
    "ckp = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 21:46:46.656753: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-20 21:46:46.656817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-20 21:46:46.659076: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-20 21:46:46.671389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 21:46:48.772172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['formatted_question', 'combinedfact', 'answerKey', 'topic', '__index_level_0__', 'explanation'],\n",
       "        num_rows: 9098\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(ckp_data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'formatted_question': 'lightning can be bad for what? (a) the environment (b) rainstorms (c) destruction (d) visibility (e) thunder (f) the sun (g) the weather. (h) transportation',\n",
       " 'combinedfact': 'lightning can be bad for the environment.',\n",
       " 'answerKey': 'a',\n",
       " 'topic': 'electricity',\n",
       " '__index_level_0__': 34080,\n",
       " 'explanation': 'b) Rainstorms: Lightning is actually a natural phenomenon that occurs during rainstorms. It is not bad for rainstorms, but rather a part of the storm.\\n\\nc) Destruction: While lightning can cause destruction, the question is asking what lightning can be bad for, not what it can cause.\\n\\nd) Visibility: Lightning can actually improve visibility during a storm by illuminating the surroundings. It is not bad for visibility.\\n\\ne) Thunder: Thunder is actually caused by lightning, so it is not bad for thunder.\\n\\nf) The sun: Lightning is not related to the sun, so it is not bad for the sun.\\n\\ng) The weather: Lightning is a part of the weather, so it is not bad for the weather.\\n\\nh) Transportation: Lightning can be dangerous for transportation, but the question is asking what lightning can be bad for in general, not specifically for transportation.\\n\\nTherefore, the only logical answer is a) the environment. Lightning can start wildfires, damage trees and other vegetation, and cause soil erosion, which can all have negative impacts on the environment.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['formatted_question', 'combinedfact', 'answerKey', 'topic', '__index_level_0__', 'explanation'],\n",
       "        num_rows: 7278\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['formatted_question', 'combinedfact', 'answerKey', 'topic', '__index_level_0__', 'explanation'],\n",
       "        num_rows: 1820\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data = data[\"train\"].train_test_split(test_size=0.2)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing data\n",
    "# this should be adapted according to the dataset\n",
    "\n",
    "import re\n",
    "\n",
    "def process(samples):\n",
    "\n",
    "    contexts = []\n",
    "    questions_choices = []\n",
    "    answers = []\n",
    "    toks={}\n",
    "\n",
    "    for ind in range(len(samples[\"topic\"])):\n",
    "        \n",
    "        context = samples[\"combinedfact\"][ind]\n",
    "        \n",
    "        # all choices are prefix with (X) where X can be a-h\n",
    "        # so we get the choices by spliting by (X)\n",
    "        ctx = re.split(\" \\([a-z]\\) \", samples[\"formatted_question\"][ind])\n",
    "\n",
    "        # the question preceed the choices\n",
    "        question = ctx[0]\n",
    "\n",
    "        # combine the input as in the presentation\n",
    "        # - context + question + choice_X\n",
    "        for i, c in enumerate(ctx[1:]):\n",
    "            contexts.append(context)\n",
    "            questions_choices.append(question + \" \" + c)\n",
    "\n",
    "        while(i < 7):\n",
    "            contexts.append(context)\n",
    "            questions_choices.append(question + \" unKnown\")\n",
    "            i += 1 \n",
    "\n",
    "        answers.append(label2id.get(samples[\"answerKey\"][ind]))\n",
    "\n",
    "    # tokenization\n",
    "    # (batch, 8*seq_len)\n",
    "    toks = tokenizer(contexts, questions_choices, truncation=\"only_first\", max_length=128, padding=\"max_length\")\n",
    "\n",
    "    # rearrange the question-choices\n",
    "    # (batch, 8, seq_len)\n",
    "    toks = {k: [v[i:i+8] for i in range(0, len(v), 8)] for k, v in toks.items()}\n",
    "    toks[\"labels\"] = answers\n",
    "\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72534573fdcd42c582bbd48863bcf6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd3c173922f4550996dfc5106e2dcae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['formatted_question', 'combinedfact', 'answerKey', 'topic', '__index_level_0__', 'explanation', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7278\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['formatted_question', 'combinedfact', 'answerKey', 'topic', '__index_level_0__', 'explanation', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1820\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data = split_data.map(process, batched=True)\n",
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 5492, 3596, 17530, 1997, 2300, 1012, 102, 2054, 3596, 17530, 1997, 2300, 1029, 13016, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5492, 3596, 17530, 1997, 2300, 1012, 102, 2054, 3596, 17530, 1997, 2300, 1029, 5492, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5492, 3596, 17530, 1997, 2300, 1012, 102, 2054, 3596, 17530, 1997, 2300, 1029, 3221, 17530, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5492, 3596, 17530, 1997, 2300, 1012, 102, 2054, 3596, 17530, 1997, 2300, 1029, 1037, 4400, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5492, 3596, 17530, 1997, 2300, 1012, 102, 2054, 3596, 17530, 1997, 2300, 1029, 4714, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5492, 3596, 17530, 1997, 2300, 1012, 102, 2054, 3596, 17530, 1997, 2300, 1029, 1037, 14017, 10421, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5492, 3596, 17530, 1997, 2300, 1012, 102, 2054, 3596, 17530, 1997, 2300, 1029, 4542, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5492, 3596, 17530, 1997, 2300, 1012, 102, 2054, 3596, 17530, 1997, 2300, 1029, 19688, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# they only differ at the end\n",
    "\n",
    "print(tokenized_data[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7278, 8, 128)\n"
     ]
    }
   ],
   "source": [
    "# show the shape of the input\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(np.array(tokenized_data[\"train\"][\"input_ids\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(ckp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultipleChoice(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. define metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only compare the choice, exact match\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "def metric(pred):\n",
    "\n",
    "    preds, refs = pred\n",
    "\n",
    "    preds = preds.argmax(axis=-1)\n",
    "\n",
    "    return acc.compute(predictions=preds, references=refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir=\"../tmp/checkpoints\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    "        load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    compute_metrics=metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. train + eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='57' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/57 04:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.0795347690582275,\n",
       " 'eval_accuracy': 0.14725274725274726,\n",
       " 'eval_runtime': 26.5249,\n",
       " 'eval_samples_per_second': 68.615,\n",
       " 'eval_steps_per_second': 2.149}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='684' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [684/684 13:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.059893</td>\n",
       "      <td>0.984066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.025800</td>\n",
       "      <td>0.056948</td>\n",
       "      <td>0.987363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.062097</td>\n",
       "      <td>0.987363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=684, training_loss=0.07388382962883085, metrics={'train_runtime': 811.4543, 'train_samples_per_second': 26.907, 'train_steps_per_second': 0.843, 'total_flos': 1.1489430405574656e+16, 'train_loss': 0.07388382962883085, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no predefined pipeline by hf for multi choice problem\n",
    "\n",
    "import torch\n",
    "\n",
    "class MultipleChoicePipeline:\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.device = model.device\n",
    "\n",
    "    def preprocess(self, context, question, choices):\n",
    "\n",
    "        ctx, qs = [], []\n",
    "        for choice in choices:\n",
    "            ctx.append(context)\n",
    "            qs.append(question + \" \" + choice)\n",
    "        return tokenizer(ctx, qs, truncation=\"only_first\", max_length=128, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    def predict(self, input):\n",
    "\n",
    "        input = {k:v.unsqueeze(0).to(self.device) for k, v in input.items()}\n",
    "        return self.model(**input).logits\n",
    "\n",
    "    def postprocess(self, logits, choices):\n",
    "\n",
    "        pred = torch.argmax(logits, dim=-1).cpu().item()\n",
    "\n",
    "        return choices[pred]\n",
    "\n",
    "    def __call__(self, context, question, choices):\n",
    "\n",
    "        input = self.preprocess(context, question, choices)\n",
    "        logits = self.predict(input)\n",
    "        result = self.postprocess(logits, choices)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = MultipleChoicePipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'environment'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"lightning can be bad for the environment.\", \"lightning can be bad for what?\", [\"environment\", \"rainstorms\", \"destruction\", \"visibility\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
