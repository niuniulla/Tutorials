{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Reading Comprehension (MRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, it will:\n",
    "\n",
    "    I. explain the MRC problem.\n",
    "    II. Model\n",
    "    III. Realization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Definition\n",
    " \n",
    "\n",
    "MRC is class of problem where model provides answers based on the context given at the input. More precisely, we input a context text and a questions, and the model outputs a answer based on the inputs.\n",
    "\n",
    "The answers can be:\n",
    "\n",
    " - filling blanks\n",
    " - multi-choices\n",
    " - generate answers\n",
    " - extraction of a passage\n",
    "\n",
    "Example: question-answering\n",
    "\n",
    "**context**: Mercury is the closest planet to the sun. As such, it circles the sun faster than all the other planets, which is why Romans named it after the swift-footed messenger god Mercury. Mercury was known since at least Sumerian times roughly 5,000 years ago, where it was often associated with Nabu, the god of writing. Mercury was also given separate names for its appearance as both a morning star and as an evening star. Greek astronomers knew, however, that the two names referred to the same body. Heraclitus believed that both Mercury and Venus orbited the Sun, not the Earth.\n",
    "\n",
    "    **Question1**: Which is the cloest planet to the sun?\n",
    "    **Answer1**: Mercury\n",
    "    **Question2**: Who is Nabu?\n",
    "    **Answer2**: the god of writing\n",
    "    **Question3**: What Heraclitus believed?\n",
    "    **Answer3**: both Mercury and Venus orbited the Sun, not the Earth\n",
    "\n",
    "In this case, the labels are the starting and ending positions of the answers.\n",
    "\n",
    "### 2. evaluation\n",
    "\n",
    "The metrics:\n",
    "\n",
    "* Exact Match (EM): fully matched\n",
    "* f1\n",
    "\n",
    "Example: \n",
    "* reference: Paris\n",
    "* ground Truth: Paris of France\n",
    "\n",
    "* EM = 0\n",
    "* f1 = (2 * 1/1 * 1 / 3) / (1 / 1 + 1 / 3) = 0.5\n",
    "\n",
    "### 3. data processing\n",
    "\n",
    "The texts was concatenated as:\n",
    "\n",
    "        ______________________________________________________________________________\n",
    "        |CLS|       Question       |SEP|                    context              |SEP|\n",
    "        ------------------------------------------------------------------------------\n",
    "\n",
    "If the context is too long, we can either:\n",
    "\n",
    "* truncate and ignore the text longer than a length - waste of data\n",
    "* using a sliding window to get a section of the text - more complex to realize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used is AutoModelForQuestionAnswering, with the bert base model.\n",
    "\n",
    "This model uses bert base model to encode the input text, and output the classes (num_labels) of each tokens according to the classes we defined. So the output dimension is [batch, seq_len, classes].\n",
    "\n",
    "In the class's init function:\n",
    "\n",
    "```python\n",
    "    self.num_labels = config.num_labels\n",
    "    self.bert = BertModel(config, add_pooling_layer=False)\n",
    "    ...\n",
    "    self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
    "```\n",
    "Where num_labels in this context is 2. We don't have to redefine.\n",
    "\n",
    "In the forward function:\n",
    "\n",
    "```python\n",
    "    outputs = self.bert(\n",
    "            input_ids,\n",
    "            ...\n",
    "        )\n",
    "    sequence_output = outputs[0]\n",
    "\n",
    "    logits = self.qa_outputs(sequence_output)           # [batch, seq_len, 2]\n",
    "    start_logits, end_logits = logits.split(1, dim=-1)  # [batch, seq_len, 1] for each\n",
    "    start_logits = start_logits.squeeze(-1).contiguous()\n",
    "    end_logits = end_logits.squeeze(-1).contiguous()\n",
    "```\n",
    "\n",
    "The input is encoded using bert model. The output of bert model is then put into a linear layer to project the hidden values to the positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to set the gpu to use\n",
    "# Since I have 2 GPUs and I only want to use one, I need to run this.\n",
    "# Should be run the first\n",
    "# skip this if you don't need.\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or \"0,1\" for multiple GPUs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defin repos for data and model\n",
    "\n",
    "# data\n",
    "\n",
    "ckp_data = \"cjlovering/natural-questions-short\"\n",
    "\n",
    "# model\n",
    "\n",
    "ckp = \"google-bert/bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 17:58:37.129243: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-20 17:58:37.129306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-20 17:58:37.131566: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-20 17:58:37.142966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 17:58:39.249348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['has_correct_context', 'id', 'questions', 'name', 'contexts', 'answers'],\n",
       "        num_rows: 13933\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['has_correct_context', 'id', 'questions', 'name', 'contexts', 'answers'],\n",
       "        num_rows: 871\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(ckp_data, cache_dir=\"../tmp/mrc\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has_correct_context': True,\n",
       " 'id': '5495190773098085777',\n",
       " 'questions': [{'input_text': \"when does stephanie die in grey's anatomy\"}],\n",
       " 'name': \"Stephanie Edwards (Grey's Anatomy)\",\n",
       " 'contexts': \"Dr. Stephanie Edwards Grey 's Anatomy character The Season 12 Promotional Photo of Jerrika Hinton as Stephanie Edwards First appearance Going , Going , Gone ( 9.01 ) September 27 , 2012 ( as recurring cast ) `` Seal Our Fate '' ( 10.01 ) September 26 , 2013 ( as series regular ) Last appearance `` Ring of Fire '' ( 13.24 ) May 18 , 2017 Created by Shonda Rhimes Portrayed by Jerrika Hinton Information Full name Stephanie Edwards Nickname ( s ) Grumpy Steph Dr. Lavender Title M.D. Significant other ( s ) Jackson Avery Kyle Diaz ( deceased )\",\n",
       " 'answers': [{'candidate_id': 0,\n",
       "   'input_text': 'short',\n",
       "   'span_end': 324,\n",
       "   'span_start': 296,\n",
       "   'span_text': \"`` Ring of Fire '' ( 13.24 )\"}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split Data\n",
    "\n",
    "Since the downloaded dataset is already split, so we skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='google-bert/bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckp)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2043, 2515, 11496, 3280, 1999, 4462, 1005, 1055, 13336, 102, 2852, 1012, 11496, 7380, 4462, 1005, 1055, 13336, 2839, 1996, 2161, 2260, 10319, 6302, 1997, 15333, 18752, 2912, 9374, 2239, 2004, 11496, 7380, 2034, 3311, 2183, 1010, 2183, 1010, 2908, 1006, 1023, 1012, 5890, 1007, 2244, 2676, 1010, 2262, 1006, 2004, 10694, 3459, 1007, 1036, 1036, 7744, 2256, 6580, 1005, 1005, 1006, 2184, 1012, 5890, 1007, 2244, 2656, 1010, 2286, 1006, 2004, 2186, 3180, 1007, 2197, 3311, 1036, 1036, 3614, 1997, 2543, 1005, 1005, 1006, 2410, 1012, 2484, 1007, 2089, 2324, 1010, 2418, 2580, 2011, 26822, 8943, 1054, 14341, 2229, 6791, 2011, 15333, 18752, 2912, 9374, 2239, 2592, 2440, 2171, 11496, 7380, 8367, 1006, 1055, 1007, 24665, 24237, 2100, 3357, 2232, 2852, 1012, 20920, 2516, 1049, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# truncation=\"only_second\" means only cut the text_pair if length greater the max_length\n",
    "\n",
    "# token_type_ids indicate if the token comes from the question or the context\n",
    "\n",
    "ind = 0\n",
    "sample = data[\"train\"][ind]\n",
    "tok = tokenizer(text=sample[\"questions\"][0]['input_text'], \n",
    "                text_pair=sample[\"contexts\"],\n",
    "                max_length=128, truncation=\"only_second\", padding=\"max_length\")\n",
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': \"when does stephanie die in grey's anatomy\"}\n",
      "Dr. Stephanie Edwards Grey 's Anatomy character The Season 12 Promotional Photo of Jerrika Hinton as Stephanie Edwards First appearance Going , Going , Gone ( 9.01 ) September 27 , 2012 ( as recurring cast ) `` Seal Our Fate '' ( 10.01 ) September 26 , 2013 ( as series regular ) Last appearance `` Ring of Fire '' ( 13.24 ) May 18 , 2017 Created by Shonda Rhimes Portrayed by Jerrika Hinton Information Full name Stephanie Edwards Nickname ( s ) Grumpy Steph Dr. Lavender Title M.D. Significant other ( s ) Jackson Avery Kyle Diaz ( deceased )\n",
      "[101, 2043, 2515, 11496, 3280, 1999, 4462, 1005, 1055, 13336, 102, 2852, 1012, 11496, 7380, 4462, 1005, 1055, 13336, 2839, 1996, 2161, 2260, 10319, 6302, 1997, 15333, 18752, 2912, 9374, 2239, 2004, 11496, 7380, 2034, 3311, 2183, 1010, 2183, 1010, 2908, 1006, 1023, 1012, 5890, 1007, 2244, 2676, 1010, 2262, 1006, 2004, 10694, 3459, 1007, 1036, 1036, 7744, 2256, 6580, 1005, 1005, 1006, 2184, 1012, 5890, 1007, 2244, 2656, 1010, 2286, 1006, 2004, 2186, 3180, 1007, 2197, 3311, 1036, 1036, 3614, 1997, 2543, 1005, 1005, 1006, 2410, 1012, 2484, 1007, 2089, 2324, 1010, 2418, 2580, 2011, 26822, 8943, 1054, 14341, 2229, 6791, 2011, 15333, 18752, 2912, 9374, 2239, 2592, 2440, 2171, 11496, 7380, 8367, 1006, 1055, 1007, 24665, 24237, 2100, 3357, 2232, 2852, 1012, 20920, 2516, 1049, 102]\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# show some info\n",
    "\n",
    "print(sample[\"questions\"][0])\n",
    "print(sample[\"contexts\"])\n",
    "print(tok[\"input_ids\"])\n",
    "print(len(tok[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the token contains 3 components:\n",
    "# - input_ids: contains the combined tokens of question and contexts\n",
    "# - token_type_ids: to indicate it is question (0) or contexts (1)\n",
    "# - attention_mask -\n",
    "\n",
    "tok.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(101, 0), (2043, 0), (2515, 0), (11496, 0), (3280, 0), (1999, 0), (4462, 0), (1005, 0), (1055, 0), (13336, 0), (102, 0), (2852, 1), (1012, 1), (11496, 1), (7380, 1), (4462, 1), (1005, 1), (1055, 1), (13336, 1), (2839, 1), (1996, 1), (2161, 1), (2260, 1), (10319, 1), (6302, 1), (1997, 1), (15333, 1), (18752, 1), (2912, 1), (9374, 1), (2239, 1), (2004, 1), (11496, 1), (7380, 1), (2034, 1), (3311, 1), (2183, 1), (1010, 1), (2183, 1), (1010, 1), (2908, 1), (1006, 1), (1023, 1), (1012, 1), (5890, 1), (1007, 1), (2244, 1), (2676, 1), (1010, 1), (2262, 1), (1006, 1), (2004, 1), (10694, 1), (3459, 1), (1007, 1), (1036, 1), (1036, 1), (7744, 1), (2256, 1), (6580, 1), (1005, 1), (1005, 1), (1006, 1), (2184, 1), (1012, 1), (5890, 1), (1007, 1), (2244, 1), (2656, 1), (1010, 1), (2286, 1), (1006, 1), (2004, 1), (2186, 1), (3180, 1), (1007, 1), (2197, 1), (3311, 1), (1036, 1), (1036, 1), (3614, 1), (1997, 1), (2543, 1), (1005, 1), (1005, 1), (1006, 1), (2410, 1), (1012, 1), (2484, 1), (1007, 1), (2089, 1), (2324, 1), (1010, 1), (2418, 1), (2580, 1), (2011, 1), (26822, 1), (8943, 1), (1054, 1), (14341, 1), (2229, 1), (6791, 1), (2011, 1), (15333, 1), (18752, 1), (2912, 1), (9374, 1), (2239, 1), (2592, 1), (2440, 1), (2171, 1), (11496, 1), (7380, 1), (8367, 1), (1006, 1), (1055, 1), (1007, 1), (24665, 1), (24237, 1), (2100, 1), (3357, 1), (2232, 1), (2852, 1), (1012, 1), (20920, 1), (2516, 1), (1049, 1), (102, 1)]\n"
     ]
    }
   ],
   "source": [
    "# to show which part of the tokens belongs to question and which to context\n",
    "# the second term in each tuple designates whether it is question or context\n",
    "\n",
    "print([(t, l) for t, l in zip(tok[\"input_ids\"], tok[\"token_type_ids\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## offset mapping\n",
    "\n",
    "samples = data[\"train\"].select(range(5))\n",
    "\n",
    "question = []\n",
    "for i in range(5):\n",
    "    question.append(samples[i][\"questions\"][0][\"input_text\"])\n",
    "\n",
    "\n",
    "toks = tokenizer(text=question, \n",
    "                 text_pair=samples[\"contexts\"],\n",
    "                 max_length=128, \n",
    "                 truncation=\"only_second\", \n",
    "                 padding=\"max_length\", \n",
    "                 return_offsets_mapping=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 4),\n",
       " (5, 8),\n",
       " (9, 12),\n",
       " (13, 16),\n",
       " (17, 21),\n",
       " (22, 24),\n",
       " (25, 28),\n",
       " (29, 35),\n",
       " (36, 44),\n",
       " (0, 0),\n",
       " (0, 3),\n",
       " (4, 10),\n",
       " (11, 19),\n",
       " (20, 23),\n",
       " (24, 26),\n",
       " (27, 35),\n",
       " (36, 43),\n",
       " (44, 50),\n",
       " (51, 56),\n",
       " (57, 63),\n",
       " (64, 71),\n",
       " (72, 75),\n",
       " (76, 78),\n",
       " (79, 86),\n",
       " (87, 93),\n",
       " (94, 97),\n",
       " (97, 100),\n",
       " (100, 106),\n",
       " (107, 116),\n",
       " (117, 123),\n",
       " (124, 127),\n",
       " (128, 132),\n",
       " (133, 136),\n",
       " (137, 138),\n",
       " (139, 141),\n",
       " (142, 145),\n",
       " (146, 151),\n",
       " (152, 161),\n",
       " (162, 164),\n",
       " (165, 173),\n",
       " (174, 176),\n",
       " (177, 186),\n",
       " (187, 192),\n",
       " (193, 194),\n",
       " (194, 195),\n",
       " (196, 202),\n",
       " (203, 205),\n",
       " (206, 211),\n",
       " (212, 214),\n",
       " (215, 216),\n",
       " (217, 221),\n",
       " (222, 223),\n",
       " (224, 227),\n",
       " (228, 235),\n",
       " (236, 245),\n",
       " (246, 248),\n",
       " (249, 253),\n",
       " (254, 256),\n",
       " (257, 258),\n",
       " (259, 263),\n",
       " (264, 265),\n",
       " (266, 270),\n",
       " (271, 273),\n",
       " (274, 281),\n",
       " (282, 284),\n",
       " (285, 292),\n",
       " (293, 300),\n",
       " (301, 303),\n",
       " (304, 310),\n",
       " (311, 314),\n",
       " (315, 321),\n",
       " (322, 323),\n",
       " (324, 330),\n",
       " (331, 339),\n",
       " (340, 348),\n",
       " (349, 354),\n",
       " (355, 358),\n",
       " (359, 366),\n",
       " (367, 370),\n",
       " (371, 379),\n",
       " (380, 381),\n",
       " (382, 385),\n",
       " (386, 394),\n",
       " (395, 407),\n",
       " (408, 417),\n",
       " (418, 421),\n",
       " (422, 424),\n",
       " (425, 432),\n",
       " (433, 436),\n",
       " (437, 446),\n",
       " (447, 450),\n",
       " (451, 454),\n",
       " (454, 457),\n",
       " (457, 461),\n",
       " (462, 464),\n",
       " (465, 471),\n",
       " (472, 475),\n",
       " (476, 482),\n",
       " (483, 484),\n",
       " (485, 489),\n",
       " (490, 499),\n",
       " (500, 501),\n",
       " (502, 505),\n",
       " (506, 512),\n",
       " (513, 521),\n",
       " (522, 529),\n",
       " (530, 538),\n",
       " (539, 546),\n",
       " (547, 550),\n",
       " (551, 556),\n",
       " (557, 564),\n",
       " (565, 574),\n",
       " (575, 585),\n",
       " (586, 588),\n",
       " (589, 595),\n",
       " (596, 605),\n",
       " (606, 607),\n",
       " (608, 611),\n",
       " (612, 618),\n",
       " (619, 627),\n",
       " (628, 634),\n",
       " (635, 638),\n",
       " (639, 649),\n",
       " (650, 652),\n",
       " (653, 661),\n",
       " (662, 669),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks[\"offset_mapping\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " None,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sequence id\n",
    "\n",
    "# Nones are the special tokens, and 0s are the question, and 1s are the context\n",
    "\n",
    "toks.sequence_ids(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Simple truncate\n",
    "\n",
    "As mentioned in the presentation section, for long context, we have 2 strategies: truncate and using sliding window. We implement the truncate first in section 4.1, and using sliding window in the following section 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` Ring of Fire '' ( 13.24 ) 296 324 11 126 78 89\n",
      "to counter Soviet geopolitical expansion during the Cold War 76 136 11 126 23 33\n",
      "Cara Black 37 47 19 82 25 26\n",
      "vice president 530 544 12 126 106 107\n",
      "Shemar Moore ( 1994 -- 2005 , 2014 ) Darius McCrary 98 149 12 126 28 42\n"
     ]
    }
   ],
   "source": [
    "# To convert the answer position to token positions\n",
    "# the tokens is the concatenation of question + context\n",
    "# the outputs here is the starting and ending positions of the context and answer in the tokens\n",
    "\n",
    "for ind, offset in enumerate(toks[\"offset_mapping\"]):\n",
    "\n",
    "    answer = samples[ind][\"answers\"][0]\n",
    "    text = answer[\"span_text\"]\n",
    "\n",
    "    # the answer position in the context\n",
    "    start = int(answer[\"span_start\"])\n",
    "    end = int(answer[\"span_end\"])\n",
    "\n",
    "    # find context pos in tokens\n",
    "    context_start = toks.sequence_ids(ind).index(1) # get the starting index of context in the tokens\n",
    "    context_end = toks.sequence_ids(ind).index(None, context_start) - 1 # get the ending index of context in the tokens\n",
    "\n",
    "    # convert answer position to token pos\n",
    "    if offset[context_start][1] > end or offset[context_end][0] < start:\n",
    "        answer_start = 0\n",
    "        answer_end = 0\n",
    "\n",
    "    else:\n",
    "        tok_id = context_start\n",
    "        while tok_id <= context_end and offset[tok_id][0] < start:\n",
    "            tok_id += 1\n",
    "        answer_start = tok_id\n",
    "\n",
    "        tok_id = context_end\n",
    "        while tok_id >= context_start and offset[tok_id][1] > end:\n",
    "            tok_id -= 1\n",
    "\n",
    "        answer_end = tok_id\n",
    "        \n",
    "    print(text, start, end, context_start, context_end, answer_start, answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the data process\n",
    "\n",
    "# put all previous operations together and define a function\n",
    "\n",
    "def process_truncate(samples):\n",
    "\n",
    "    # redefine column of question\n",
    "    # this is slightly different from previous tests since the map function will rearrange\n",
    "    # the dataset according to their feature names, so we should put the feature names before the\n",
    "    # indexing\n",
    "\n",
    "    new_column = []\n",
    "    \n",
    "    for sample in samples[\"questions\"]:\n",
    "\n",
    "        new_column.append(sample[0][\"input_text\"])\n",
    "    \n",
    "    # create new column of question\n",
    "\n",
    "    samples[\"question\"] = new_column\n",
    "\n",
    "    # tokenization\n",
    "    toks = tokenizer(text=samples[\"question\"], \n",
    "                     text_pair=samples[\"contexts\"],\n",
    "                     max_length=128, truncation=\"only_second\", padding=\"max_length\", return_offsets_mapping=True)\n",
    "    \n",
    "\n",
    "    # convert the answer position to token positions\n",
    "    start_pos = []\n",
    "    end_pos = []\n",
    "\n",
    "    for ind, offset in enumerate(toks[\"offset_mapping\"]):\n",
    "\n",
    "        answer = samples[\"answers\"][ind][0]\n",
    "        text = answer[\"span_text\"]\n",
    "\n",
    "        # text pos for answer\n",
    "        start = int(answer[\"span_start\"])\n",
    "        end = int(answer[\"span_end\"])\n",
    "\n",
    "        # find context pos in tokens\n",
    "        context_start = toks.sequence_ids(ind).index(1)\n",
    "        context_end = toks.sequence_ids(ind).index(None, context_start) - 1\n",
    "\n",
    "        # convert pos to token pos\n",
    "        if offset[context_start][1] > end or offset[context_end][0] < start:\n",
    "\n",
    "            answer_start = 0\n",
    "            answer_end = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            tok_id = context_start\n",
    "            while tok_id <= context_end and offset[tok_id][0] < start:\n",
    "                tok_id += 1\n",
    "            answer_start = tok_id\n",
    "\n",
    "            tok_id = context_end\n",
    "            while tok_id >= context_start and offset[tok_id][1] > end:\n",
    "                tok_id -= 1\n",
    "\n",
    "            answer_end = tok_id\n",
    "\n",
    "        start_pos.append(answer_start)\n",
    "        end_pos.append(answer_end)\n",
    "\n",
    "    # the trainer relies on the feature names of the datadict\n",
    "    # in this context (mrc), the start and end positions of the answers should be named\n",
    "    # - start_positions\n",
    "    # - end_positions\n",
    "    # Otherwise, we get error message when train: ValueError: The model did not return a loss from the inputs, \n",
    "    # only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.**\n",
    "\n",
    "    toks[\"start_positions\"] = start_pos # be careful with feature names\n",
    "    toks[\"end_positions\"] = end_pos\n",
    "\n",
    "    return toks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45d1f052fe14c0197cfe5d00266f256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13933 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 13933\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 871\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "\n",
    "tokenized_data_truncate = data.map(process_truncate, batched=True, remove_columns=data[\"train\"].column_names)\n",
    "tokenized_data_truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Sliding window\n",
    "\n",
    "In previous sections, we truncate data within maximal length and discard the rest. This leads to the waste of data.\n",
    "There is another way to treate data, which is to cut original sentences into smaller chunk with some overlapping to perserve integrity of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parameters allow overflowing\n",
    "\n",
    "samples = data[\"train\"].select(range(5))\n",
    "new_column = []\n",
    "for i in range(5):\n",
    "    new_column.append(samples[i][\"questions\"][0][\"input_text\"])\n",
    "\n",
    "samples = samples.add_column(\"question\", new_column)\n",
    "toks = tokenizer(text=samples[\"question\"], \n",
    "                    text_pair=samples[\"contexts\"],\n",
    "                    return_overflowing_tokens=True,     # to activate sliding truncation\n",
    "                    stride=64,                          # define the overlapping length, default value is 0 (no overlap)\n",
    "                    max_length=128, \n",
    "                    truncation=\"only_second\", \n",
    "                    padding=\"max_length\", \n",
    "                    return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is an extra key named \"overflow_to_sample_mapping\"\n",
    "\n",
    "toks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 2, 3, 3, 3, 4, 4, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the extra field contains the indices of the original data\n",
    "# We selected 5 data from the original dataset and after the overflowing process,\n",
    "# it returns 11 data by cutting the contexts into chunks of size 128 and \n",
    "# stride of 64\n",
    "\n",
    "toks[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] when does stephanie die in grey's anatomy [SEP] dr. stephanie edwards grey's anatomy character the season 12 promotional photo of jerrika hinton as stephanie edwards first appearance going, going, gone ( 9. 01 ) september 27, 2012 ( as recurring cast ) ` ` seal our fate'' ( 10. 01 ) september 26, 2013 ( as series regular ) last appearance ` ` ring of fire'' ( 13. 24 ) may 18, 2017 created by shonda rhimes portrayed by jerrika hinton information full name stephanie edwards nickname ( s ) grumpy steph dr. lavender title m [SEP]\n",
      "[CLS] when does stephanie die in grey's anatomy [SEP] 10. 01 ) september 26, 2013 ( as series regular ) last appearance ` ` ring of fire'' ( 13. 24 ) may 18, 2017 created by shonda rhimes portrayed by jerrika hinton information full name stephanie edwards nickname ( s ) grumpy steph dr. lavender title m. d. significant other ( s ) jackson avery kyle diaz ( deceased ) [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] what was the key goal of the truman doctrine [SEP] the truman doctrine was an american foreign policy whose stated purpose was to counter soviet geopolitical expansion during the cold war. it was first announced to congress by president harry s. truman on march 12, 1947, and further developed on july 12, 1948, when he pledged to contain threats to greece and turkey. direct american military force was usually not involved, but congress appropriated financial aid to support the economies and militaries of greece and turkey. more generally, the truman doctrine implied american support for other nations allegedly threatened by soviet communism. the truman doctrine became the foundation of american foreign [SEP]\n"
     ]
    }
   ],
   "source": [
    "# we can show the texts of the chunk to make sure that the cuts were correct\n",
    "\n",
    "for text in tokenizer.batch_decode(toks[\"input_ids\"][:3]):\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` Ring of Fire '' ( 13.24 ) 296 324 11 126 78 89\n",
      "`` Ring of Fire '' ( 13.24 ) 296 324 11 89 26 37\n",
      "to counter Soviet geopolitical expansion during the Cold War 76 136 11 126 23 33\n",
      "to counter Soviet geopolitical expansion during the Cold War 76 136 11 113 0 0\n",
      "Cara Black 37 47 19 82 25 26\n",
      "vice president 530 544 12 126 106 107\n",
      "vice president 530 544 12 126 55 56\n",
      "vice president 530 544 12 105 0 0\n",
      "Shemar Moore ( 1994 -- 2005 , 2014 ) Darius McCrary 98 149 12 126 28 42\n",
      "Shemar Moore ( 1994 -- 2005 , 2014 ) Darius McCrary 98 149 12 126 0 0\n",
      "Shemar Moore ( 1994 -- 2005 , 2014 ) Darius McCrary 98 149 12 82 0 0\n"
     ]
    }
   ],
   "source": [
    "# We do the same process as before by replacing the offset_mapping by \n",
    "# overflow_to_sample_mapping\n",
    "\n",
    "for ind, _ in enumerate(toks[\"overflow_to_sample_mapping\"]):\n",
    "\n",
    "    answer = samples[toks[\"overflow_to_sample_mapping\"][ind]][\"answers\"][0]\n",
    "    text = answer[\"span_text\"]\n",
    "\n",
    "    # text pos for answer\n",
    "    start = int(answer[\"span_start\"])\n",
    "    end = int(answer[\"span_end\"])\n",
    "\n",
    "    # find context pos in tokens\n",
    "    context_start = toks.sequence_ids(ind).index(1)\n",
    "    context_end = toks.sequence_ids(ind).index(None, context_start) - 1\n",
    "\n",
    "    offset = toks.get(\"offset_mapping\")[ind]\n",
    "\n",
    "    # convert pos to token pos\n",
    "    if offset[context_start][1] > end or offset[context_end][0] < start:\n",
    "        answer_start = 0\n",
    "        answer_end = 0\n",
    "\n",
    "    else:\n",
    "        tok_id = context_start\n",
    "        while tok_id <= context_end and offset[tok_id][0] < start:\n",
    "            tok_id += 1\n",
    "        answer_start = tok_id\n",
    "        tok_id = context_end\n",
    "        while tok_id >= context_start and offset[tok_id][1] > end:\n",
    "            tok_id -= 1\n",
    "\n",
    "        answer_end = tok_id\n",
    "        \n",
    "    print(text, start, end, context_start, context_end, answer_start, answer_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process by taking consideration of the sliding truncation\n",
    "\n",
    "def process_overflow(samples):\n",
    "\n",
    "    # redefine column of question\n",
    "\n",
    "    question = []\n",
    "    \n",
    "    for sample in samples[\"questions\"]:\n",
    "\n",
    "        question.append(sample[0][\"input_text\"])\n",
    "\n",
    "    # tokenization\n",
    "    toks = tokenizer(text=question, \n",
    "                    text_pair=samples[\"contexts\"],\n",
    "                    return_overflowing_tokens=True, # to activate sliding truncation\n",
    "                    stride=64, # define the overlapping length, default value is 0 (no overlap)\n",
    "                    max_length=128, \n",
    "                    truncation=\"only_second\", \n",
    "                    padding=\"max_length\", \n",
    "                    return_offsets_mapping=True)\n",
    "    \n",
    "    start_pos = []\n",
    "    end_pos = []\n",
    "    ids = []\n",
    "\n",
    "    sample_map = toks.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    for ind, _ in enumerate(sample_map):\n",
    "\n",
    "        answer = samples[\"answers\"][sample_map[ind]][0]\n",
    "        \n",
    "        text = answer[\"span_text\"]\n",
    "\n",
    "        # text pos for answer\n",
    "        start = int(answer[\"span_start\"])\n",
    "        end = int(answer[\"span_end\"])\n",
    "\n",
    "        # find context pos in tokens\n",
    "        context_start = toks.sequence_ids(ind).index(1)\n",
    "        context_end = toks.sequence_ids(ind).index(None, context_start) - 1\n",
    "\n",
    "        offset = toks.get(\"offset_mapping\")[ind]\n",
    "        \n",
    "\n",
    "        # convert pos to token pos\n",
    "        if offset[context_start][1] > end or offset[context_end][0] < start:\n",
    "\n",
    "            answer_start = 0\n",
    "            answer_end = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            tok_id = context_start\n",
    "            while tok_id <= context_end and offset[tok_id][0] < start:\n",
    "                tok_id += 1\n",
    "            answer_start = tok_id\n",
    "\n",
    "            tok_id = context_end\n",
    "            while tok_id >= context_start and offset[tok_id][1] > end:\n",
    "                tok_id -= 1\n",
    "\n",
    "            answer_end = tok_id\n",
    "\n",
    "        ids.append(samples[\"id\"][sample_map[ind]]) # we added info aout the id to identify the origin of the data\n",
    "        start_pos.append(answer_start)\n",
    "        end_pos.append(answer_end)\n",
    "\n",
    "        toks[\"offset_mapping\"][ind] = [\n",
    "            v if toks.sequence_ids(ind)[k] == 1 else None\n",
    "            for k, v in enumerate(toks[\"offset_mapping\"][ind])\n",
    "        ]\n",
    "\n",
    "    toks[\"start_positions\"] = start_pos # be careful with feature names\n",
    "    toks[\"end_positions\"] = end_pos\n",
    "    toks[\"id\"] = ids\n",
    "\n",
    "    return toks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 32010\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions'],\n",
       "        num_rows: 2022\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "\n",
    "tokenized_data_overflow = data.map(process_overflow, batched=True, remove_columns=data[\"train\"].column_names)\n",
    "tokenized_data_overflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# we use the same model as in NER\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(ckp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract predictions and the corresponding labels\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def result(start_logits, end_logits, samples, examples) :\n",
    "\n",
    "    preds = {}\n",
    "    refs = {}\n",
    "\n",
    "    example_by_sample = defaultdict(list)\n",
    "\n",
    "    for i, sample_id in enumerate(samples[\"id\"]):\n",
    "\n",
    "        example_by_sample[sample_id].append(i)\n",
    "\n",
    "\n",
    "    n_best = 20\n",
    "    max_answer_len = 30\n",
    "\n",
    "    for sample in samples:\n",
    "\n",
    "        sample_id = sample[\"id\"]\n",
    "        context = sample[\"contexts\"]\n",
    "        answers =[]\n",
    "\n",
    "        for example_id in example_by_sample[sample_id]:\n",
    "\n",
    "            start_logit = start_logits[example_id]\n",
    "            end_logit = end_logits[example_id]\n",
    "\n",
    "            offset = examples[example_id][\"offset_mapping\"]\n",
    "\n",
    "            start_indices = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
    "            end_indices = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
    "\n",
    "            for s_ind in start_indices:\n",
    "                for e_ind in end_indices:\n",
    "\n",
    "                    if offset[s_ind] is None or offset[e_ind] is None:\n",
    "                        continue\n",
    "                    if e_ind < s_ind or e_ind - s_ind + 1 > max_answer_len:\n",
    "                        continue\n",
    "\n",
    "                    answers.append({\n",
    "                        \"text\": context[offset[s_ind][0]: offset[e_ind][1]],\n",
    "                        \"score\": start_logit[s_ind] + end_logit[e_ind]\n",
    "                    })\n",
    "\n",
    "        if len(answers) > 0:\n",
    "            \n",
    "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "\n",
    "            preds[sample_id] = best_answer[\"text\"]\n",
    "            \n",
    "        else:\n",
    "            preds[sample_id] = \"\"\n",
    "\n",
    "        refs[sample_id] = sample[\"answers\"][0][\"span_text\"]\n",
    "\n",
    "    return preds, refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_cmrc import evaluate_cmrc\n",
    "\n",
    "def metric(pred):\n",
    "\n",
    "    s_logits, e_logits = pred[0]\n",
    "    if s_logits.shape[0] == len(tokenized_data[\"validation\"]):\n",
    "        p, r = result(s_logits, e_logits, data[\"validation\"], tokenized_data[\"validation\"])\n",
    "        \n",
    "    return evaluate_cmrc(p, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. train args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir=\"../tmp/checkpoints\",\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=128,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenized_data_overflow\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. train + eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 03:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.901897430419922,\n",
       " 'eval_avg': 0.2343407000742645,\n",
       " 'eval_f1': 0.08980654366173216,\n",
       " 'eval_em': 0.3788748564867968,\n",
       " 'eval_total': 871,\n",
       " 'eval_skip': 0,\n",
       " 'eval_runtime': 24.4983,\n",
       " 'eval_samples_per_second': 82.536,\n",
       " 'eval_steps_per_second': 0.653}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1503' max='1503' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1503/1503 08:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Avg</th>\n",
       "      <th>F1</th>\n",
       "      <th>Em</th>\n",
       "      <th>Total</th>\n",
       "      <th>Skip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.980300</td>\n",
       "      <td>1.030865</td>\n",
       "      <td>0.154136</td>\n",
       "      <td>0.059134</td>\n",
       "      <td>0.249139</td>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.702300</td>\n",
       "      <td>1.029207</td>\n",
       "      <td>0.151903</td>\n",
       "      <td>0.062705</td>\n",
       "      <td>0.241102</td>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.387400</td>\n",
       "      <td>1.110168</td>\n",
       "      <td>0.152483</td>\n",
       "      <td>0.063864</td>\n",
       "      <td>0.241102</td>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1503, training_loss=0.8424471972865893, metrics={'train_runtime': 536.643, 'train_samples_per_second': 178.946, 'train_steps_per_second': 2.801, 'total_flos': 6273081887339520.0, 'train_loss': 0.8424471972865893, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1101675033569336,\n",
       " 'eval_avg': 0.15248326279167984,\n",
       " 'eval_f1': 0.06386434418267078,\n",
       " 'eval_em': 0.24110218140068887,\n",
       " 'eval_total': 871,\n",
       " 'eval_skip': 0,\n",
       " 'eval_runtime': 21.62,\n",
       " 'eval_samples_per_second': 93.525,\n",
       " 'eval_steps_per_second': 0.74,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x7fd400bfffa0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.2047954797744751,\n",
       " 'start': 8,\n",
       " 'end': 34,\n",
       " 'answer': 'a boxer who works in europ'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question=\"who is john\", context=\"John is a boxer who works in europ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
