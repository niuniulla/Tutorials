{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "construct dataset\n",
    "\"\"\"\n",
    "\n",
    "# we don't really need this dataset class, here is only for demonstration\n",
    "# in case the data are not from hugging face but othere sources\n",
    "\n",
    "# For HF data sets, we can just load the data using load_dataset which will\n",
    "# return the structure data.\n",
    "# The loaded data can then be filtered, mapped using dataset methods\n",
    "# For details, see \"hg_transformers_datasets.ipynb\"\n",
    "\n",
    "class TranslationDataset(Dataset) :\n",
    "\n",
    "    def __init__(self) :\n",
    "\n",
    "        super().__init__()\n",
    "        self.data = load_dataset(\"opus_books\", 'en-fr', split='train')\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "\n",
    "        return self.data[index][\"translation\"]['en'], self.data[index][\"translation\"]['fr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Wanderer', 'Le grand Meaulnes')\n",
      "('Alain-Fournier', 'Alain-Fournier')\n",
      "('First Part', 'PREMIÈRE PARTIE')\n",
      "('I', 'CHAPITRE PREMIER')\n"
     ]
    }
   ],
   "source": [
    "# show some data in dataset\n",
    "\n",
    "dataset = TranslationDataset()\n",
    "for i in range(4) :\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "split dataset into train and valid sets\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_set, valid_set = random_split(dataset, lengths=[0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101668, 25417)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "len(train_set), len(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use the dataset function to split the dataset\n",
    "\n",
    "# dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "construct data loader\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"Nevertheless, Lord Glenarvan kept the promise which he had given.',\n",
       "  \"'I will work harder, then,' says I, 'and you shall have it all.'\",\n",
       "  'Mr. Bingley was unaffectedly civil in his answer, and forced his younger sister to be civil also, and say what the occasion required.',\n",
       "  'After my broken and imperfect prayer was over, I drank the rum in which I had steeped the tobacco, which was so strong and rank of the tobacco that I could scarcely get it down; immediately upon this I went to bed.',\n",
       "  'Depuis que la Fausta avait témoigné le désir d’un rendez-vous, toute cette chasse semblait bien longue à Fabrice.',\n",
       "  'All the fires must be extinguished, so that nothing may betray the presence of men on the island.\"',\n",
       "  'Bouteloup didn\\'t even wait until the husband had gone!\"',\n",
       "  'To prevent my being known, I pulled off my blue apron, and wrapped the bundle in it, which before was made up in a piece of painted calico, and very remarkable; I also wrapped up my straw hat in it, and so put the bundle upon my head; and it was very well that I did thus, for coming through the Bluecoat Hospital, who should I meet but the wench that had given me the bundle to hold.',\n",
       "  '\"Then, gentlemen, you will not oppose our executing the orders we have received?\" asked one who appeared to be the leader of the party.',\n",
       "  '\"It is very handsome!\"',\n",
       "  'The youngest of the brothers Roy passed along the muddy street; he was swinging at the end of a string, then flinging into the air three horse-chestnuts which fell into the playground.',\n",
       "  'Their powers of conversation were considerable.',\n",
       "  '\"By God!\" exclaimed Maheu, furious at being dragged out of his dejection, \"what is all this clatter again?',\n",
       "  '« Ne disant mot de mon martyre, on ne se cache point de moi et je vois tout ce qui peut se passer…',\n",
       "  'They bound and gagged him; then he was led to a dark cavern, at the foot of Mount Franklin, where the convicts had taken refuge.',\n",
       "  'We were a fashionable and highly cultured party.'),\n",
       " (\"«Néanmoins, lord Glenarvan tint la parole qu'il avait donnée.\",\n",
       "  \"--Alors je travaillerai plus dur, dis-je, et je vous donnerai tout l'argent.\",\n",
       "  'Bennet avait recommencé la litanie de ses remerciements pour l’hospitalité offerte a ses deux filles. Mr.',\n",
       "  \"Après cette prière brusque et incomplète je bus le _rum_ dans lequel j'avais fait tremper le tabac; mais il en était si chargé et si fort que ce ne fut qu'avec beaucoup de peine que je l'avalai.\",\n",
       "  'Now that Fausta had shewn a desire to meet him, all this pursuit seemed to Fabrizio very tedious.',\n",
       "  \"Que tous les feux soient éteints. Que rien enfin ne trahisse la présence de l'homme sur cette île!\",\n",
       "  \"Bouteloup n'attendait meme plus que le mari fut parti!\",\n",
       "  \"Pour empêcher que je fusse reconnue, je détachai mon tablier bleu, et je le roulai autour du paquet qui était enveloppé dans un morceau d'indienne; j'y roulai aussi mon chapeau de paille et je mis le paquet sur ma tête; et je fis très bien, car, passant à travers Bluecoat-Hospital, qui rencontrai-je sinon la fille qui m'avait donné à tenir son paquet?\",\n",
       "  \"-- Alors, messieurs, vous ne vous opposerez pas à ce que nous exécutions les ordres que nous avons reçus? demanda celui qui paraissait le chef de l'escouade.\",\n",
       "  '– C’est cela qui est beau ! »',\n",
       "  'Le cadet des Roy passa dans la rue boueuse, faisant tourner au bout d’une ficelle, puis lâchant en l’air trois marrons attachés qui retombèrent dans la cour.',\n",
       "  'Jamais Elizabeth ne les avait vues aussi aimables que pendant l’heure qui suivit.',\n",
       "  \"—Mais, nom de Dieu! s'écria Maheu, furieux d'etre tiré de son accablement, qu'est-ce que c'est encore que tous ces potins?\",\n",
       "  '\"If I make no mention of my suffering, nothing will be kept back from me, and I shall see all that goes on....',\n",
       "  \"Ceux-ci le lièrent et le bâillonnèrent; puis, il fut emmené dans une caverne obscure, au pied du mont Franklin, là où les convicts s'étaient réfugiés.\",\n",
       "  'Nous étions a une soirée.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to show batch data in data loader\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocess data\n",
    "\"\"\"\n",
    "\n",
    "# to be more efficient, it is better to process the data in batch instead of one by one\n",
    "# So, instead of adding the processing in the dataset construction, we use pass a processing function\n",
    "# to the dataloader, which allows to process the data in batch\n",
    "\n",
    "# return_tensor allows to transform the tokenized data into the format we'd like to use to train or do inference\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "def preprocess(batch) :\n",
    "\n",
    "    prefix = \"translate English to French: \"\n",
    "    inputs = [prefix + example[0] for example in batch]\n",
    "    targets = [example[1] for example in batch]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, padding=\"max_length\", max_length=500, truncation=True, return_tensors=\"pt\")\n",
    "    return model_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader with preprocessing function\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=preprocess)\n",
    "valid_loader = DataLoader(valid_set, batch_size=1, shuffle=False, collate_fn=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0],\n",
       "        [13959,  1566,    12,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[14976,    30,  1903,  ...,     0,     0,     0],\n",
       "        [  283,     5,  4004,  ...,     0,     0,     0],\n",
       "        [    3,   104,  3307,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    3,   104, 27889,  ...,     0,     0,     0],\n",
       "        [24470,    15, 26182,  ...,     0,     0,     0],\n",
       "        [  695,  4154,     6,  ...,     0,     0,     0]])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show some data\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "# use gpu is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda(1) # 1 is optional, I used 1 since my cuda0 is busy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "def train(epochs=2, log_steps=100) :\n",
    "\n",
    "    gSteps = 0\n",
    "\n",
    "    for e in range(epochs) :\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader :\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                \n",
    "                batch = {k: v.cuda(1) for k, v in batch.items()}\n",
    "\n",
    "            output = model(**batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output.loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if gSteps % log_steps == 0:\n",
    "\n",
    "                print(f\"epoch: {e}, steps: {gSteps}, loss: {output.loss.item()}\")\n",
    "            \n",
    "            gSteps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, steps: 0, loss: 15.056625366210938\n",
      "epoch: 0, steps: 100, loss: 0.3115119934082031\n",
      "epoch: 0, steps: 200, loss: 0.2316114604473114\n",
      "epoch: 0, steps: 300, loss: 0.1704372614622116\n",
      "epoch: 0, steps: 400, loss: 0.15944263339042664\n",
      "epoch: 0, steps: 500, loss: 0.17312923073768616\n",
      "epoch: 0, steps: 600, loss: 0.12698674201965332\n",
      "epoch: 0, steps: 700, loss: 0.23214466869831085\n",
      "epoch: 0, steps: 800, loss: 0.15984416007995605\n",
      "epoch: 0, steps: 900, loss: 0.18747538328170776\n",
      "epoch: 0, steps: 1000, loss: 0.18290716409683228\n",
      "epoch: 0, steps: 1100, loss: 0.16001208126544952\n",
      "epoch: 0, steps: 1200, loss: 0.19968031346797943\n",
      "epoch: 0, steps: 1300, loss: 0.19356799125671387\n",
      "epoch: 0, steps: 1400, loss: 0.17190241813659668\n",
      "epoch: 0, steps: 1500, loss: 0.17707660794258118\n",
      "epoch: 0, steps: 1600, loss: 0.11719249933958054\n",
      "epoch: 0, steps: 1700, loss: 0.17830918729305267\n",
      "epoch: 0, steps: 1800, loss: 0.11448365449905396\n",
      "epoch: 0, steps: 1900, loss: 0.20359036326408386\n",
      "epoch: 0, steps: 2000, loss: 0.19085530936717987\n",
      "epoch: 0, steps: 2100, loss: 0.20983673632144928\n",
      "epoch: 0, steps: 2200, loss: 0.12596048414707184\n",
      "epoch: 0, steps: 2300, loss: 0.2185661643743515\n",
      "epoch: 0, steps: 2400, loss: 0.19306564331054688\n",
      "epoch: 0, steps: 2500, loss: 0.19131416082382202\n",
      "epoch: 0, steps: 2600, loss: 0.16417066752910614\n",
      "epoch: 0, steps: 2700, loss: 0.16654251515865326\n",
      "epoch: 0, steps: 2800, loss: 0.10986488312482834\n",
      "epoch: 0, steps: 2900, loss: 0.18064673244953156\n",
      "epoch: 0, steps: 3000, loss: 0.12329953908920288\n",
      "epoch: 0, steps: 3100, loss: 0.14954984188079834\n",
      "epoch: 0, steps: 3200, loss: 0.20227904617786407\n",
      "epoch: 0, steps: 3300, loss: 0.19553065299987793\n",
      "epoch: 0, steps: 3400, loss: 0.3301245868206024\n",
      "epoch: 0, steps: 3500, loss: 0.11276502907276154\n",
      "epoch: 0, steps: 3600, loss: 0.1607208400964737\n",
      "epoch: 0, steps: 3700, loss: 0.11968963593244553\n",
      "epoch: 0, steps: 3800, loss: 0.18392989039421082\n",
      "epoch: 0, steps: 3900, loss: 0.1073264554142952\n",
      "epoch: 0, steps: 4000, loss: 0.1416817158460617\n",
      "epoch: 0, steps: 4100, loss: 0.16037189960479736\n",
      "epoch: 0, steps: 4200, loss: 0.1347273290157318\n",
      "epoch: 0, steps: 4300, loss: 0.12305265665054321\n",
      "epoch: 0, steps: 4400, loss: 0.1571420133113861\n",
      "epoch: 0, steps: 4500, loss: 0.2041166126728058\n",
      "epoch: 0, steps: 4600, loss: 0.13001997768878937\n",
      "epoch: 0, steps: 4700, loss: 0.21066376566886902\n",
      "epoch: 0, steps: 4800, loss: 0.10799110680818558\n",
      "epoch: 0, steps: 4900, loss: 0.09994655847549438\n",
      "epoch: 0, steps: 5000, loss: 0.12382234632968903\n",
      "epoch: 0, steps: 5100, loss: 0.12766630947589874\n",
      "epoch: 0, steps: 5200, loss: 0.12191759049892426\n",
      "epoch: 0, steps: 5300, loss: 0.18171747028827667\n",
      "epoch: 0, steps: 5400, loss: 0.12776672840118408\n",
      "epoch: 0, steps: 5500, loss: 0.13708357512950897\n",
      "epoch: 0, steps: 5600, loss: 0.10973647981882095\n",
      "epoch: 0, steps: 5700, loss: 0.18435196578502655\n",
      "epoch: 0, steps: 5800, loss: 0.1378263235092163\n",
      "epoch: 0, steps: 5900, loss: 0.0902026817202568\n",
      "epoch: 0, steps: 6000, loss: 0.0868813693523407\n",
      "epoch: 0, steps: 6100, loss: 0.2179802805185318\n",
      "epoch: 0, steps: 6200, loss: 0.19015903770923615\n",
      "epoch: 0, steps: 6300, loss: 0.16413724422454834\n",
      "epoch: 1, steps: 6400, loss: 0.13425546884536743\n",
      "epoch: 1, steps: 6500, loss: 0.1477603316307068\n",
      "epoch: 1, steps: 6600, loss: 0.18494734168052673\n",
      "epoch: 1, steps: 6700, loss: 0.11838052421808243\n",
      "epoch: 1, steps: 6800, loss: 0.09533967077732086\n",
      "epoch: 1, steps: 6900, loss: 0.33465832471847534\n",
      "epoch: 1, steps: 7000, loss: 0.11599772423505783\n",
      "epoch: 1, steps: 7100, loss: 0.12024958431720734\n",
      "epoch: 1, steps: 7200, loss: 0.10001733154058456\n",
      "epoch: 1, steps: 7300, loss: 0.13038378953933716\n",
      "epoch: 1, steps: 7400, loss: 0.17142726480960846\n",
      "epoch: 1, steps: 7500, loss: 0.08570662140846252\n",
      "epoch: 1, steps: 7600, loss: 0.11435491591691971\n",
      "epoch: 1, steps: 7700, loss: 0.13200758397579193\n",
      "epoch: 1, steps: 7800, loss: 0.2528182566165924\n",
      "epoch: 1, steps: 7900, loss: 0.18080715835094452\n",
      "epoch: 1, steps: 8000, loss: 0.2272501140832901\n",
      "epoch: 1, steps: 8100, loss: 0.21792982518672943\n",
      "epoch: 1, steps: 8200, loss: 0.07652575522661209\n",
      "epoch: 1, steps: 8300, loss: 0.10861935466527939\n",
      "epoch: 1, steps: 8400, loss: 0.11040203273296356\n",
      "epoch: 1, steps: 8500, loss: 0.10581206530332565\n",
      "epoch: 1, steps: 8600, loss: 0.09981750696897507\n",
      "epoch: 1, steps: 8700, loss: 0.13979260623455048\n",
      "epoch: 1, steps: 8800, loss: 0.1489470899105072\n",
      "epoch: 1, steps: 8900, loss: 0.12706510722637177\n",
      "epoch: 1, steps: 9000, loss: 0.1009773388504982\n",
      "epoch: 1, steps: 9100, loss: 0.15035976469516754\n",
      "epoch: 1, steps: 9200, loss: 0.14292004704475403\n",
      "epoch: 1, steps: 9300, loss: 0.13285769522190094\n",
      "epoch: 1, steps: 9400, loss: 0.19180957973003387\n",
      "epoch: 1, steps: 9500, loss: 0.15445250272750854\n",
      "epoch: 1, steps: 9600, loss: 0.16048328578472137\n",
      "epoch: 1, steps: 9700, loss: 0.18065325915813446\n",
      "epoch: 1, steps: 9800, loss: 0.16528254747390747\n",
      "epoch: 1, steps: 9900, loss: 0.10976278781890869\n",
      "epoch: 1, steps: 10000, loss: 0.09947219491004944\n",
      "epoch: 1, steps: 10100, loss: 0.18335005640983582\n",
      "epoch: 1, steps: 10200, loss: 0.13404494524002075\n",
      "epoch: 1, steps: 10300, loss: 0.1139843612909317\n",
      "epoch: 1, steps: 10400, loss: 0.11594647169113159\n",
      "epoch: 1, steps: 10500, loss: 0.21896564960479736\n",
      "epoch: 1, steps: 10600, loss: 0.13397593796253204\n",
      "epoch: 1, steps: 10700, loss: 0.10917039215564728\n",
      "epoch: 1, steps: 10800, loss: 0.10894767940044403\n",
      "epoch: 1, steps: 10900, loss: 0.08374787122011185\n",
      "epoch: 1, steps: 11000, loss: 0.13527534902095795\n",
      "epoch: 1, steps: 11100, loss: 0.15084882080554962\n",
      "epoch: 1, steps: 11200, loss: 0.15644577145576477\n",
      "epoch: 1, steps: 11300, loss: 0.15165022015571594\n",
      "epoch: 1, steps: 11400, loss: 0.15510748326778412\n",
      "epoch: 1, steps: 11500, loss: 0.15684007108211517\n",
      "epoch: 1, steps: 11600, loss: 0.08310621231794357\n",
      "epoch: 1, steps: 11700, loss: 0.11330181360244751\n",
      "epoch: 1, steps: 11800, loss: 0.14019763469696045\n",
      "epoch: 1, steps: 11900, loss: 0.2066773623228073\n",
      "epoch: 1, steps: 12000, loss: 0.11064121127128601\n",
      "epoch: 1, steps: 12100, loss: 0.1022665724158287\n",
      "epoch: 1, steps: 12200, loss: 0.10489119589328766\n",
      "epoch: 1, steps: 12300, loss: 0.12727928161621094\n",
      "epoch: 1, steps: 12400, loss: 0.20846013724803925\n",
      "epoch: 1, steps: 12500, loss: 0.12308810651302338\n",
      "epoch: 1, steps: 12600, loss: 0.22098088264465332\n",
      "epoch: 1, steps: 12700, loss: 0.0766916424036026\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inference, we should do a iteration for search, which is out of scope here.\n",
    "# The different techniques of search will be explored later.\n",
    "\n",
    "# So for simplicity, we use pipeline for inference\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"translation_xx_to_yy\", model=model, tokenizer=tokenizer, device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your input_length: 25 is bigger than 0.9 * max_length: 20. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Qingyi/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Il est plus facile de construire un modèle de réseau neural à zéro dans PyT'}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"translate English to French: Building a neural network model from scratch in PyTorch is easier than it sounds.\"\n",
    "pipe(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
