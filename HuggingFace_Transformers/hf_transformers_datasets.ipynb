{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load online data\n",
    "\n",
    "This can be used to download all huggingface datasets : https://huggingface.co/datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load the data, you have to pass the location of the dataset which \n",
    "# is composed of the name of the repository and the dataset name. \n",
    "# There is no use to provide the url.\n",
    "\n",
    "# by default, data is a DatasetDict\n",
    "# Depending ont he data structure to be downloaded, the content of this dict \n",
    "# will change.\n",
    "# the following example has only on element in the dict since there is only one\n",
    "# set is available.\n",
    "# Otherwise, the dict may contain several elements such as train, valid, test. \n",
    "\n",
    "\n",
    "data =  load_dataset(\"yezhengli9/opus_books_demo\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 127085\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we are only interested in one part of the dataset (eg. train), one can use\n",
    "# the 'split' argument.\n",
    "# And the data becomes Dataset but not a dict anymore.\n",
    "\n",
    "data =  load_dataset(\"yezhengli9/opus_books_demo\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One can also slicing the dataset to be loaded\n",
    "# the Slicing can be done by providing :\n",
    "#  * the start and the end indices.\n",
    "#  * the percentage of the data size\n",
    "\n",
    "data =  load_dataset(\"yezhengli9/opus_books_demo\", split=\"train[:10]\")\n",
    "# data =  load_dataset(\"yezhengli9/opus_books_demo\", split=\"train[:10%]\")\n",
    "# data =  load_dataset(\"yezhengli9/opus_books_demo\", split=[\"train[:10%]\", \"train[90%:]\"]) # return a list\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 127085\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sometimes, there are several subsets in a dataset.\n",
    "# We can download a subset by providing the name of this subset\n",
    "\n",
    "data_sub = load_dataset(\"Helsinki-NLP/opus_books\", \"en-fr\")\n",
    "data_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect dataset\n",
    "\n",
    "The inspection is to see and check data.\n",
    "The returned results are lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'The Wanderer', 'fr': 'Le grand Meaulnes'},\n",
       " {'en': 'Alain-Fournier', 'fr': 'Alain-Fournier'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the data is a dict, by specifying the key (eg. train), we can get its content\n",
    "# otherwise, the data can be accessible directly.\n",
    "data_sub[\"train\"][\"translation\"][:2]\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'translation': Translation(languages=['en', 'fr'], id=None)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get other attributes \n",
    "\n",
    "# all collumns\n",
    "data.column_names\n",
    "data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 101668\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 25417\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spliting data into different groups (eg. train, valid)\n",
    "# this returns a DatasetDict\n",
    "\n",
    "data.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter & Manipulation\n",
    "\n",
    "Those functions are to process the data for later usage, which is different from the inspection in the above section.\n",
    "The results of those functions are still dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the data, this return a Dataset, which is different from previous section\n",
    "# for inspection, which return a list of dict\n",
    "\n",
    "data.select([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6bc235d27a4eb98b990a2a8e5551bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'en': 'All day Millie had waited for the station omnibus to bring her a hat for the bad weather.',\n",
       "  'fr': 'Toute la journée, Millie avait attendu une voiture de La Gare qui devait lui apporter un chapeau pour la mauvaise saison.'},\n",
       " {'en': 'And thus they had it out together, without the least bad feeling.',\n",
       "  'fr': 'Et elles continuaient ainsi à se tenir tête sans la moindre humeur.'},\n",
       " {'en': 'At first Augustin, in a bad temper, watched from the classroom step as this play started.',\n",
       "  'fr': 'Augustin, debout sur le seuil de la classe, regardait d’abord avec mauvaise humeur s’organiser ces jeux.'},\n",
       " {'en': \"The bad lads of the countryside thought it a lark to smoke cigarettes, to put sugar and water on their hair to make it curl, to kiss girls from the Continuation School in the street, and to call out from behind a hedge, 'Pokebonnet,' to rag a passing nun.\",\n",
       "  'fr': 'Fumer la cigarette, se mettre de l’eau sucrée sur les cheveux pour qu’ils frisent, embrasser les filles du Cours Complémentaire dans les chemins et crier « à la cornette ! » derrière la haie pour narguer la religieuse qui passe, c’était la joie de tous les mauvais drôles du pays.'},\n",
       " {'en': 'At twenty, however, bad lads of that kind can very well improve and become often most sensible fellows.',\n",
       "  'fr': 'À vingt ans, d’ailleurs, les mauvais drôles de cette espèce peuvent très bien s’amender et deviennent parfois des jeunes gens fort sensibles.'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For more complex filter, one can use filter method with a flter function representing the filter \n",
    "# criteria, this returns a Dataset\n",
    "# The opration is not in place\n",
    "\n",
    "data_bad = data.filter(lambda translates : \"bad \" in translates[\"translation\"][\"en\"])\n",
    "data_bad[\"translation\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fd024dc87a405baa17b4ee0b11e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To apply an operation to data, one can use map method.\n",
    "\n",
    "def replace(translate) :\n",
    "\n",
    "    translate[\"translation\"]['en'] = translate[\"translation\"]['en'].replace(\"bad \", \"good \")\n",
    "    return translate\n",
    "\n",
    "data_good = data.map(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7b5fc1c6e9489a84b5a84e03efaf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here show there is no more sentences containing \"bad \"\n",
    "\n",
    "data_good_b = data_good.filter(lambda translates : \"bad \" in translates[\"translation\"][\"en\"])\n",
    "data_good_b[\"translation\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more complex example using tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to to the preprocessing\n",
    "# https://huggingface.co/docs/transformers/tasks/translation\n",
    "\n",
    "# Prefix the input with a prompt so T5 knows this is a translation task. \n",
    "# Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "def preprocess(example) :\n",
    "\n",
    "    inputs = prefix + example[\"translation\"][source_lang]\n",
    "    targets = example[\"translation\"][target_lang]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c87c50e26b41d5a8029d84853d57f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_data = data.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269d877296854f84a310b0ea6887557b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To accelerate the process, we can specify the number of process.\n",
    "\n",
    "preprocessed_data = data.map(preprocess, num_proc=4)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf32c2485e24747b37da8e78a53d2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# batched processing \n",
    "\n",
    "# if we use the function above for batched process, we will get an error of indice.\n",
    "# The error is caused by the fact that the function above doesn't support batched data,\n",
    "# eg: without batch, each element is data[i][\"translation\"]['en'], which returns the \n",
    "# ieme element in english.\n",
    "# however, data[i:i+batch_size][\"translation\"]['en'] triggers error.\n",
    "\n",
    "def preprocess_batched(examples):\n",
    "    \n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "preprocessed_data = data.map(preprocess_batched, batched=True)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11193b2193e34b0796f62e2e12315584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed_data = data.map(preprocess_batched, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 127085\n",
       "})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can remove the information we don't want\n",
    "\n",
    "preprocessed_data = data.map(preprocess_batched, batched=True, remove_columns=data.column_names)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 127085\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save \n",
    "preprocessed_data.save_to_disk(\"./processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load \n",
    "preprocessed_data = load_from_disk(\"./processed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load one or several files\n",
    "# If there are several files, put all file names into a list and pass the list to data_files arg.\n",
    "\n",
    "data = load_dataset(\"csv\", data_files=\"./mydata.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is dedicated function to load csv file\n",
    "\n",
    "data = Dataset.from_csv(\"./mydata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all files in a folder\n",
    "\n",
    "data = load_dataset(\"csv\", data_dir=\"./myfilespath\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from panda data\n",
    "\n",
    "data = Dataset.from_pandas(data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'sentence1'}, {'text': 'sentence2'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if the data is a list, we can't load them directly as dataset\n",
    "# if so, we get an error: AttributeError: 'str' object has no attribute 'get'\n",
    "\n",
    "data_list = [\"sentence1\", \"sentence2\"]\n",
    "\n",
    "# we should decorate the list as\n",
    "\n",
    "data_list = [{\"text\": line} for line in data_list]\n",
    "print(data_list)\n",
    "\n",
    "Dataset.from_list(data_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "diffuser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
