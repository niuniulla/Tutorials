{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Stable diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All pipelines are based on DiffusionPipeline. A pipeline can fulfill one or some tasks. There is an explaination of the pipelines and their tasks: https://huggingface.co/docs/diffusers/v0.29.2/en/api/pipelines/overview#diffusers. \n",
    "But for some tasks, we can use AutoPipeline to load the model without knowing the specific pipeline to use.\n",
    "To choose which pipeline to use, we have either look at the doc or the code source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614df30c0dfc477c9d697fab3ea314d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.29.2\",\n",
       "  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DiffusionPipeline\n",
    "\n",
    "# it loads a StableDiffusionPipeline\n",
    "# it contains \n",
    "#   - feature_extractor: to translate encoded prompt to image features\n",
    "#   - scheduler: scheduler\n",
    "#   - text_encoder: to encode tokenized prompt\n",
    "#   - tokenizer: to tokenize prompt\n",
    "#   - unet: to estimate noise for each time step\n",
    "#   - vae: to encode image space to latent space\n",
    "\n",
    "# The corresponding models for each components can be seen in the pipe object.\n",
    "# note: image_encoder is not used in this pipe.\n",
    "\n",
    "# Each component was loaded from a subfolder of the repository.\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_safetensors=True)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28d90782d8747a787e55eb575aea4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionPipeline {\n",
       "  \"_class_name\": \"StableDiffusionPipeline\",\n",
       "  \"_diffusers_version\": \"0.29.2\",\n",
       "  \"_name_or_path\": \"CompVis/stable-diffusion-v1-4\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"image_encoder\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"requires_safety_checker\": true,\n",
       "  \"safety_checker\": [\n",
       "    \"stable_diffusion\",\n",
       "    \"StableDiffusionSafetyChecker\"\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"PNDMScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The same pipeline can be loaded using autopipeline for tasks\n",
    "# We get the same pipeline as before.\n",
    "\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "pipe_auto = AutoPipelineForText2Image.from_pretrained(\"CompVis/stable-diffusion-v1-4\", use_safetensors=True)\n",
    "pipe_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTokenizer(name_or_path='/home/niuniu/.cache/huggingface/hub/models--CompVis--stable-diffusion-v1-4/snapshots/133a221b8aa7292a167afc5127cb63fb5005638b/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access to a component\n",
    "\n",
    "pipe_auto.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189a840028c24690adee231da71f0dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1-5-pruned.ckpt:   0%|          | 0.00/7.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/6b/20/6b201da5f0f5c60524535ebb7deac2eef68605655d3bbacfee9cce0087f3b3f5/e1441589a6f3c5a53f5f54d0975a18a7feb7cdf0b0dee276dfc3331ae376a053?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27v1-5-pruned.ckpt%3B+filename%3D%22v1-5-pruned.ckpt%22%3B&Expires=1720798478&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDc5ODQ3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy82Yi8yMC82YjIwMWRhNWYwZjVjNjA1MjQ1MzVlYmI3ZGVhYzJlZWY2ODYwNTY1NWQzYmJhY2ZlZTljY2UwMDg3ZjNiM2Y1L2UxNDQxNTg5YTZmM2M1YTUzZjVmNTRkMDk3NWExOGE3ZmViN2NkZjBiMGRlZTI3NmRmYzMzMzFhZTM3NmEwNTM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=fVbqd-cpb9hVzCAJvOG%7EsInp8V0AWGS52QSkrpdTwlEQO4O5via1k2L544IkgcPFK8F78UQo3-hM63vXYGYcQhhwVuZvZjhPz47PM-2sjmmhy5lKs4uHPZq4yF7-CLrbXqWAtNNN6C8sy-E2HgqUoVhbqAIPbJCAraeSydbakSPvJHDRi3GLRLwcFx08N7hKtfwLm7OaX5mkbDKooMXBbaGRTdCpk3P5gBLEnpit-PXzGcIniW-NzrJJH7vOTELiAYUdUGzUjy-avTfjYTl9OuuXPdrmG065w4ZMkxwtICO4StXSusWG9%7EUdm34kTvqaeHITPVKNFXzHwo6ni8U9sw__&Key-Pair-Id=K3ESJI6DHPFC7: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b27fd9c96054bfcb91ada5a6f587519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1-5-pruned.ckpt:  82%|########2 | 6.33G/7.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from checkpoint file for '/home/niuniu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9/v1-5-pruned.ckpt' at '/home/niuniu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9/v1-5-pruned.ckpt'. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/DL_pytorch/diffuser/lib64/python3.12/site-packages/diffusers/models/model_loading_utils.py:108\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, variant)\u001b[0m\n\u001b[1;32m    107\u001b[0m         weights_only_kwarg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m} \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.13\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 108\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mweights_only_kwarg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/DL_pytorch/diffuser/lib64/python3.12/site-packages/torch/serialization.py:1024\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1024\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1026\u001b[0m              map_location,\n\u001b[1;32m   1027\u001b[0m              pickle_module,\n\u001b[1;32m   1028\u001b[0m              overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[1;32m   1029\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution.Do it only if you get the file from a trusted source. WeightsUnpickler error: Unsupported class pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/DL_pytorch/diffuser/lib64/python3.12/site-packages/diffusers/models/model_loading_utils.py:116\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, variant)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou cloned.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for single file models\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[0;32m----> 5\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_single_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pipe\n",
      "File \u001b[0;32m~/Documents/DL_pytorch/diffuser/lib64/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DL_pytorch/diffuser/lib64/python3.12/site-packages/diffusers/loaders/single_file.py:383\u001b[0m, in \u001b[0;36mFromSingleFileMixin.from_single_file\u001b[0;34m(cls, pretrained_model_link_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_pipeline_class\n\u001b[1;32m    381\u001b[0m pipeline_class \u001b[38;5;241m=\u001b[39m _get_pipeline_class(\u001b[38;5;28mcls\u001b[39m, config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 383\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_single_file_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_link_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     config \u001b[38;5;241m=\u001b[39m fetch_diffusers_config(checkpoint)\n",
      "File \u001b[0;32m~/Documents/DL_pytorch/diffuser/lib64/python3.12/site-packages/diffusers/loaders/single_file_utils.py:334\u001b[0m, in \u001b[0;36mload_single_file_checkpoint\u001b[0;34m(pretrained_model_link_or_path, resume_download, force_download, proxies, token, cache_dir, local_files_only, revision)\u001b[0m\n\u001b[1;32m    321\u001b[0m     repo_id, weights_name \u001b[38;5;241m=\u001b[39m _extract_repo_id_and_weights_name(pretrained_model_link_or_path)\n\u001b[1;32m    322\u001b[0m     pretrained_model_link_or_path \u001b[38;5;241m=\u001b[39m _get_model_file(\n\u001b[1;32m    323\u001b[0m         repo_id,\n\u001b[1;32m    324\u001b[0m         weights_name\u001b[38;5;241m=\u001b[39mweights_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 334\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_link_or_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# some checkpoints contain the model state dict under a \"state_dict\" key\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n",
      "File \u001b[0;32m~/Documents/DL_pytorch/diffuser/lib64/python3.12/site-packages/diffusers/models/model_loading_utils.py:128\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, variant)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is necessary to load this pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load weights from checkpoint file for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to load weights from checkpoint file for '/home/niuniu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9/v1-5-pruned.ckpt' at '/home/niuniu/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/1d0c4ebf6ff58a5caecab40fa1406526bca4b5b9/v1-5-pruned.ckpt'. "
     ]
    }
   ],
   "source": [
    "# for single file models\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_single_file(\n",
    "    \"https://huggingface.co/runwayml/stable-diffusion-v1-5/blob/main/v1-5-pruned.ckpt\"\n",
    ")\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variants are:\n",
    "    - precision: f32 by default, or half (f16), this can't by used for training or on cpu. However, we could finetune it using half precision but with some modifications (see half precision training for tranformers).\n",
    "    - no-exponential mean averagbed (EMA) weights, should not be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_in.weight torch.float32\n",
      "conv_in.bias torch.float32\n",
      "time_embedding.linear_1.weight torch.float32\n",
      "time_embedding.linear_1.bias torch.float32\n",
      "time_embedding.linear_2.weight torch.float32\n",
      "time_embedding.linear_2.bias torch.float32\n",
      "down_blocks.0.attentions.0.norm.weight torch.float32\n",
      "down_blocks.0.attentions.0.norm.bias torch.float32\n",
      "down_blocks.0.attentions.0.proj_in.weight torch.float32\n",
      "down_blocks.0.attentions.0.proj_in.bias torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "down_blocks.0.attentions.0.proj_out.weight torch.float32\n",
      "down_blocks.0.attentions.0.proj_out.bias torch.float32\n",
      "down_blocks.0.attentions.1.norm.weight torch.float32\n",
      "down_blocks.0.attentions.1.norm.bias torch.float32\n",
      "down_blocks.0.attentions.1.proj_in.weight torch.float32\n",
      "down_blocks.0.attentions.1.proj_in.bias torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "down_blocks.0.attentions.1.proj_out.weight torch.float32\n",
      "down_blocks.0.attentions.1.proj_out.bias torch.float32\n",
      "down_blocks.0.resnets.0.norm1.weight torch.float32\n",
      "down_blocks.0.resnets.0.norm1.bias torch.float32\n",
      "down_blocks.0.resnets.0.conv1.weight torch.float32\n",
      "down_blocks.0.resnets.0.conv1.bias torch.float32\n",
      "down_blocks.0.resnets.0.time_emb_proj.weight torch.float32\n",
      "down_blocks.0.resnets.0.time_emb_proj.bias torch.float32\n",
      "down_blocks.0.resnets.0.norm2.weight torch.float32\n",
      "down_blocks.0.resnets.0.norm2.bias torch.float32\n",
      "down_blocks.0.resnets.0.conv2.weight torch.float32\n",
      "down_blocks.0.resnets.0.conv2.bias torch.float32\n",
      "down_blocks.0.resnets.1.norm1.weight torch.float32\n",
      "down_blocks.0.resnets.1.norm1.bias torch.float32\n",
      "down_blocks.0.resnets.1.conv1.weight torch.float32\n",
      "down_blocks.0.resnets.1.conv1.bias torch.float32\n",
      "down_blocks.0.resnets.1.time_emb_proj.weight torch.float32\n",
      "down_blocks.0.resnets.1.time_emb_proj.bias torch.float32\n",
      "down_blocks.0.resnets.1.norm2.weight torch.float32\n",
      "down_blocks.0.resnets.1.norm2.bias torch.float32\n",
      "down_blocks.0.resnets.1.conv2.weight torch.float32\n",
      "down_blocks.0.resnets.1.conv2.bias torch.float32\n",
      "down_blocks.0.downsamplers.0.conv.weight torch.float32\n",
      "down_blocks.0.downsamplers.0.conv.bias torch.float32\n",
      "down_blocks.1.attentions.0.norm.weight torch.float32\n",
      "down_blocks.1.attentions.0.norm.bias torch.float32\n",
      "down_blocks.1.attentions.0.proj_in.weight torch.float32\n",
      "down_blocks.1.attentions.0.proj_in.bias torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "down_blocks.1.attentions.0.proj_out.weight torch.float32\n",
      "down_blocks.1.attentions.0.proj_out.bias torch.float32\n",
      "down_blocks.1.attentions.1.norm.weight torch.float32\n",
      "down_blocks.1.attentions.1.norm.bias torch.float32\n",
      "down_blocks.1.attentions.1.proj_in.weight torch.float32\n",
      "down_blocks.1.attentions.1.proj_in.bias torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "down_blocks.1.attentions.1.proj_out.weight torch.float32\n",
      "down_blocks.1.attentions.1.proj_out.bias torch.float32\n",
      "down_blocks.1.resnets.0.norm1.weight torch.float32\n",
      "down_blocks.1.resnets.0.norm1.bias torch.float32\n",
      "down_blocks.1.resnets.0.conv1.weight torch.float32\n",
      "down_blocks.1.resnets.0.conv1.bias torch.float32\n",
      "down_blocks.1.resnets.0.time_emb_proj.weight torch.float32\n",
      "down_blocks.1.resnets.0.time_emb_proj.bias torch.float32\n",
      "down_blocks.1.resnets.0.norm2.weight torch.float32\n",
      "down_blocks.1.resnets.0.norm2.bias torch.float32\n",
      "down_blocks.1.resnets.0.conv2.weight torch.float32\n",
      "down_blocks.1.resnets.0.conv2.bias torch.float32\n",
      "down_blocks.1.resnets.0.conv_shortcut.weight torch.float32\n",
      "down_blocks.1.resnets.0.conv_shortcut.bias torch.float32\n",
      "down_blocks.1.resnets.1.norm1.weight torch.float32\n",
      "down_blocks.1.resnets.1.norm1.bias torch.float32\n",
      "down_blocks.1.resnets.1.conv1.weight torch.float32\n",
      "down_blocks.1.resnets.1.conv1.bias torch.float32\n",
      "down_blocks.1.resnets.1.time_emb_proj.weight torch.float32\n",
      "down_blocks.1.resnets.1.time_emb_proj.bias torch.float32\n",
      "down_blocks.1.resnets.1.norm2.weight torch.float32\n",
      "down_blocks.1.resnets.1.norm2.bias torch.float32\n",
      "down_blocks.1.resnets.1.conv2.weight torch.float32\n",
      "down_blocks.1.resnets.1.conv2.bias torch.float32\n",
      "down_blocks.1.downsamplers.0.conv.weight torch.float32\n",
      "down_blocks.1.downsamplers.0.conv.bias torch.float32\n",
      "down_blocks.2.attentions.0.norm.weight torch.float32\n",
      "down_blocks.2.attentions.0.norm.bias torch.float32\n",
      "down_blocks.2.attentions.0.proj_in.weight torch.float32\n",
      "down_blocks.2.attentions.0.proj_in.bias torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "down_blocks.2.attentions.0.proj_out.weight torch.float32\n",
      "down_blocks.2.attentions.0.proj_out.bias torch.float32\n",
      "down_blocks.2.attentions.1.norm.weight torch.float32\n",
      "down_blocks.2.attentions.1.norm.bias torch.float32\n",
      "down_blocks.2.attentions.1.proj_in.weight torch.float32\n",
      "down_blocks.2.attentions.1.proj_in.bias torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "down_blocks.2.attentions.1.proj_out.weight torch.float32\n",
      "down_blocks.2.attentions.1.proj_out.bias torch.float32\n",
      "down_blocks.2.resnets.0.norm1.weight torch.float32\n",
      "down_blocks.2.resnets.0.norm1.bias torch.float32\n",
      "down_blocks.2.resnets.0.conv1.weight torch.float32\n",
      "down_blocks.2.resnets.0.conv1.bias torch.float32\n",
      "down_blocks.2.resnets.0.time_emb_proj.weight torch.float32\n",
      "down_blocks.2.resnets.0.time_emb_proj.bias torch.float32\n",
      "down_blocks.2.resnets.0.norm2.weight torch.float32\n",
      "down_blocks.2.resnets.0.norm2.bias torch.float32\n",
      "down_blocks.2.resnets.0.conv2.weight torch.float32\n",
      "down_blocks.2.resnets.0.conv2.bias torch.float32\n",
      "down_blocks.2.resnets.0.conv_shortcut.weight torch.float32\n",
      "down_blocks.2.resnets.0.conv_shortcut.bias torch.float32\n",
      "down_blocks.2.resnets.1.norm1.weight torch.float32\n",
      "down_blocks.2.resnets.1.norm1.bias torch.float32\n",
      "down_blocks.2.resnets.1.conv1.weight torch.float32\n",
      "down_blocks.2.resnets.1.conv1.bias torch.float32\n",
      "down_blocks.2.resnets.1.time_emb_proj.weight torch.float32\n",
      "down_blocks.2.resnets.1.time_emb_proj.bias torch.float32\n",
      "down_blocks.2.resnets.1.norm2.weight torch.float32\n",
      "down_blocks.2.resnets.1.norm2.bias torch.float32\n",
      "down_blocks.2.resnets.1.conv2.weight torch.float32\n",
      "down_blocks.2.resnets.1.conv2.bias torch.float32\n",
      "down_blocks.2.downsamplers.0.conv.weight torch.float32\n",
      "down_blocks.2.downsamplers.0.conv.bias torch.float32\n",
      "down_blocks.3.resnets.0.norm1.weight torch.float32\n",
      "down_blocks.3.resnets.0.norm1.bias torch.float32\n",
      "down_blocks.3.resnets.0.conv1.weight torch.float32\n",
      "down_blocks.3.resnets.0.conv1.bias torch.float32\n",
      "down_blocks.3.resnets.0.time_emb_proj.weight torch.float32\n",
      "down_blocks.3.resnets.0.time_emb_proj.bias torch.float32\n",
      "down_blocks.3.resnets.0.norm2.weight torch.float32\n",
      "down_blocks.3.resnets.0.norm2.bias torch.float32\n",
      "down_blocks.3.resnets.0.conv2.weight torch.float32\n",
      "down_blocks.3.resnets.0.conv2.bias torch.float32\n",
      "down_blocks.3.resnets.1.norm1.weight torch.float32\n",
      "down_blocks.3.resnets.1.norm1.bias torch.float32\n",
      "down_blocks.3.resnets.1.conv1.weight torch.float32\n",
      "down_blocks.3.resnets.1.conv1.bias torch.float32\n",
      "down_blocks.3.resnets.1.time_emb_proj.weight torch.float32\n",
      "down_blocks.3.resnets.1.time_emb_proj.bias torch.float32\n",
      "down_blocks.3.resnets.1.norm2.weight torch.float32\n",
      "down_blocks.3.resnets.1.norm2.bias torch.float32\n",
      "down_blocks.3.resnets.1.conv2.weight torch.float32\n",
      "down_blocks.3.resnets.1.conv2.bias torch.float32\n",
      "up_blocks.0.resnets.0.norm1.weight torch.float32\n",
      "up_blocks.0.resnets.0.norm1.bias torch.float32\n",
      "up_blocks.0.resnets.0.conv1.weight torch.float32\n",
      "up_blocks.0.resnets.0.conv1.bias torch.float32\n",
      "up_blocks.0.resnets.0.time_emb_proj.weight torch.float32\n",
      "up_blocks.0.resnets.0.time_emb_proj.bias torch.float32\n",
      "up_blocks.0.resnets.0.norm2.weight torch.float32\n",
      "up_blocks.0.resnets.0.norm2.bias torch.float32\n",
      "up_blocks.0.resnets.0.conv2.weight torch.float32\n",
      "up_blocks.0.resnets.0.conv2.bias torch.float32\n",
      "up_blocks.0.resnets.0.conv_shortcut.weight torch.float32\n",
      "up_blocks.0.resnets.0.conv_shortcut.bias torch.float32\n",
      "up_blocks.0.resnets.1.norm1.weight torch.float32\n",
      "up_blocks.0.resnets.1.norm1.bias torch.float32\n",
      "up_blocks.0.resnets.1.conv1.weight torch.float32\n",
      "up_blocks.0.resnets.1.conv1.bias torch.float32\n",
      "up_blocks.0.resnets.1.time_emb_proj.weight torch.float32\n",
      "up_blocks.0.resnets.1.time_emb_proj.bias torch.float32\n",
      "up_blocks.0.resnets.1.norm2.weight torch.float32\n",
      "up_blocks.0.resnets.1.norm2.bias torch.float32\n",
      "up_blocks.0.resnets.1.conv2.weight torch.float32\n",
      "up_blocks.0.resnets.1.conv2.bias torch.float32\n",
      "up_blocks.0.resnets.1.conv_shortcut.weight torch.float32\n",
      "up_blocks.0.resnets.1.conv_shortcut.bias torch.float32\n",
      "up_blocks.0.resnets.2.norm1.weight torch.float32\n",
      "up_blocks.0.resnets.2.norm1.bias torch.float32\n",
      "up_blocks.0.resnets.2.conv1.weight torch.float32\n",
      "up_blocks.0.resnets.2.conv1.bias torch.float32\n",
      "up_blocks.0.resnets.2.time_emb_proj.weight torch.float32\n",
      "up_blocks.0.resnets.2.time_emb_proj.bias torch.float32\n",
      "up_blocks.0.resnets.2.norm2.weight torch.float32\n",
      "up_blocks.0.resnets.2.norm2.bias torch.float32\n",
      "up_blocks.0.resnets.2.conv2.weight torch.float32\n",
      "up_blocks.0.resnets.2.conv2.bias torch.float32\n",
      "up_blocks.0.resnets.2.conv_shortcut.weight torch.float32\n",
      "up_blocks.0.resnets.2.conv_shortcut.bias torch.float32\n",
      "up_blocks.0.upsamplers.0.conv.weight torch.float32\n",
      "up_blocks.0.upsamplers.0.conv.bias torch.float32\n",
      "up_blocks.1.attentions.0.norm.weight torch.float32\n",
      "up_blocks.1.attentions.0.norm.bias torch.float32\n",
      "up_blocks.1.attentions.0.proj_in.weight torch.float32\n",
      "up_blocks.1.attentions.0.proj_in.bias torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.1.attentions.0.proj_out.weight torch.float32\n",
      "up_blocks.1.attentions.0.proj_out.bias torch.float32\n",
      "up_blocks.1.attentions.1.norm.weight torch.float32\n",
      "up_blocks.1.attentions.1.norm.bias torch.float32\n",
      "up_blocks.1.attentions.1.proj_in.weight torch.float32\n",
      "up_blocks.1.attentions.1.proj_in.bias torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.1.attentions.1.proj_out.weight torch.float32\n",
      "up_blocks.1.attentions.1.proj_out.bias torch.float32\n",
      "up_blocks.1.attentions.2.norm.weight torch.float32\n",
      "up_blocks.1.attentions.2.norm.bias torch.float32\n",
      "up_blocks.1.attentions.2.proj_in.weight torch.float32\n",
      "up_blocks.1.attentions.2.proj_in.bias torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.1.attentions.2.proj_out.weight torch.float32\n",
      "up_blocks.1.attentions.2.proj_out.bias torch.float32\n",
      "up_blocks.1.resnets.0.norm1.weight torch.float32\n",
      "up_blocks.1.resnets.0.norm1.bias torch.float32\n",
      "up_blocks.1.resnets.0.conv1.weight torch.float32\n",
      "up_blocks.1.resnets.0.conv1.bias torch.float32\n",
      "up_blocks.1.resnets.0.time_emb_proj.weight torch.float32\n",
      "up_blocks.1.resnets.0.time_emb_proj.bias torch.float32\n",
      "up_blocks.1.resnets.0.norm2.weight torch.float32\n",
      "up_blocks.1.resnets.0.norm2.bias torch.float32\n",
      "up_blocks.1.resnets.0.conv2.weight torch.float32\n",
      "up_blocks.1.resnets.0.conv2.bias torch.float32\n",
      "up_blocks.1.resnets.0.conv_shortcut.weight torch.float32\n",
      "up_blocks.1.resnets.0.conv_shortcut.bias torch.float32\n",
      "up_blocks.1.resnets.1.norm1.weight torch.float32\n",
      "up_blocks.1.resnets.1.norm1.bias torch.float32\n",
      "up_blocks.1.resnets.1.conv1.weight torch.float32\n",
      "up_blocks.1.resnets.1.conv1.bias torch.float32\n",
      "up_blocks.1.resnets.1.time_emb_proj.weight torch.float32\n",
      "up_blocks.1.resnets.1.time_emb_proj.bias torch.float32\n",
      "up_blocks.1.resnets.1.norm2.weight torch.float32\n",
      "up_blocks.1.resnets.1.norm2.bias torch.float32\n",
      "up_blocks.1.resnets.1.conv2.weight torch.float32\n",
      "up_blocks.1.resnets.1.conv2.bias torch.float32\n",
      "up_blocks.1.resnets.1.conv_shortcut.weight torch.float32\n",
      "up_blocks.1.resnets.1.conv_shortcut.bias torch.float32\n",
      "up_blocks.1.resnets.2.norm1.weight torch.float32\n",
      "up_blocks.1.resnets.2.norm1.bias torch.float32\n",
      "up_blocks.1.resnets.2.conv1.weight torch.float32\n",
      "up_blocks.1.resnets.2.conv1.bias torch.float32\n",
      "up_blocks.1.resnets.2.time_emb_proj.weight torch.float32\n",
      "up_blocks.1.resnets.2.time_emb_proj.bias torch.float32\n",
      "up_blocks.1.resnets.2.norm2.weight torch.float32\n",
      "up_blocks.1.resnets.2.norm2.bias torch.float32\n",
      "up_blocks.1.resnets.2.conv2.weight torch.float32\n",
      "up_blocks.1.resnets.2.conv2.bias torch.float32\n",
      "up_blocks.1.resnets.2.conv_shortcut.weight torch.float32\n",
      "up_blocks.1.resnets.2.conv_shortcut.bias torch.float32\n",
      "up_blocks.1.upsamplers.0.conv.weight torch.float32\n",
      "up_blocks.1.upsamplers.0.conv.bias torch.float32\n",
      "up_blocks.2.attentions.0.norm.weight torch.float32\n",
      "up_blocks.2.attentions.0.norm.bias torch.float32\n",
      "up_blocks.2.attentions.0.proj_in.weight torch.float32\n",
      "up_blocks.2.attentions.0.proj_in.bias torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.2.attentions.0.proj_out.weight torch.float32\n",
      "up_blocks.2.attentions.0.proj_out.bias torch.float32\n",
      "up_blocks.2.attentions.1.norm.weight torch.float32\n",
      "up_blocks.2.attentions.1.norm.bias torch.float32\n",
      "up_blocks.2.attentions.1.proj_in.weight torch.float32\n",
      "up_blocks.2.attentions.1.proj_in.bias torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.2.attentions.1.proj_out.weight torch.float32\n",
      "up_blocks.2.attentions.1.proj_out.bias torch.float32\n",
      "up_blocks.2.attentions.2.norm.weight torch.float32\n",
      "up_blocks.2.attentions.2.norm.bias torch.float32\n",
      "up_blocks.2.attentions.2.proj_in.weight torch.float32\n",
      "up_blocks.2.attentions.2.proj_in.bias torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.2.attentions.2.proj_out.weight torch.float32\n",
      "up_blocks.2.attentions.2.proj_out.bias torch.float32\n",
      "up_blocks.2.resnets.0.norm1.weight torch.float32\n",
      "up_blocks.2.resnets.0.norm1.bias torch.float32\n",
      "up_blocks.2.resnets.0.conv1.weight torch.float32\n",
      "up_blocks.2.resnets.0.conv1.bias torch.float32\n",
      "up_blocks.2.resnets.0.time_emb_proj.weight torch.float32\n",
      "up_blocks.2.resnets.0.time_emb_proj.bias torch.float32\n",
      "up_blocks.2.resnets.0.norm2.weight torch.float32\n",
      "up_blocks.2.resnets.0.norm2.bias torch.float32\n",
      "up_blocks.2.resnets.0.conv2.weight torch.float32\n",
      "up_blocks.2.resnets.0.conv2.bias torch.float32\n",
      "up_blocks.2.resnets.0.conv_shortcut.weight torch.float32\n",
      "up_blocks.2.resnets.0.conv_shortcut.bias torch.float32\n",
      "up_blocks.2.resnets.1.norm1.weight torch.float32\n",
      "up_blocks.2.resnets.1.norm1.bias torch.float32\n",
      "up_blocks.2.resnets.1.conv1.weight torch.float32\n",
      "up_blocks.2.resnets.1.conv1.bias torch.float32\n",
      "up_blocks.2.resnets.1.time_emb_proj.weight torch.float32\n",
      "up_blocks.2.resnets.1.time_emb_proj.bias torch.float32\n",
      "up_blocks.2.resnets.1.norm2.weight torch.float32\n",
      "up_blocks.2.resnets.1.norm2.bias torch.float32\n",
      "up_blocks.2.resnets.1.conv2.weight torch.float32\n",
      "up_blocks.2.resnets.1.conv2.bias torch.float32\n",
      "up_blocks.2.resnets.1.conv_shortcut.weight torch.float32\n",
      "up_blocks.2.resnets.1.conv_shortcut.bias torch.float32\n",
      "up_blocks.2.resnets.2.norm1.weight torch.float32\n",
      "up_blocks.2.resnets.2.norm1.bias torch.float32\n",
      "up_blocks.2.resnets.2.conv1.weight torch.float32\n",
      "up_blocks.2.resnets.2.conv1.bias torch.float32\n",
      "up_blocks.2.resnets.2.time_emb_proj.weight torch.float32\n",
      "up_blocks.2.resnets.2.time_emb_proj.bias torch.float32\n",
      "up_blocks.2.resnets.2.norm2.weight torch.float32\n",
      "up_blocks.2.resnets.2.norm2.bias torch.float32\n",
      "up_blocks.2.resnets.2.conv2.weight torch.float32\n",
      "up_blocks.2.resnets.2.conv2.bias torch.float32\n",
      "up_blocks.2.resnets.2.conv_shortcut.weight torch.float32\n",
      "up_blocks.2.resnets.2.conv_shortcut.bias torch.float32\n",
      "up_blocks.2.upsamplers.0.conv.weight torch.float32\n",
      "up_blocks.2.upsamplers.0.conv.bias torch.float32\n",
      "up_blocks.3.attentions.0.norm.weight torch.float32\n",
      "up_blocks.3.attentions.0.norm.bias torch.float32\n",
      "up_blocks.3.attentions.0.proj_in.weight torch.float32\n",
      "up_blocks.3.attentions.0.proj_in.bias torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.3.attentions.0.proj_out.weight torch.float32\n",
      "up_blocks.3.attentions.0.proj_out.bias torch.float32\n",
      "up_blocks.3.attentions.1.norm.weight torch.float32\n",
      "up_blocks.3.attentions.1.norm.bias torch.float32\n",
      "up_blocks.3.attentions.1.proj_in.weight torch.float32\n",
      "up_blocks.3.attentions.1.proj_in.bias torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.3.attentions.1.proj_out.weight torch.float32\n",
      "up_blocks.3.attentions.1.proj_out.bias torch.float32\n",
      "up_blocks.3.attentions.2.norm.weight torch.float32\n",
      "up_blocks.3.attentions.2.norm.bias torch.float32\n",
      "up_blocks.3.attentions.2.proj_in.weight torch.float32\n",
      "up_blocks.3.attentions.2.proj_in.bias torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "up_blocks.3.attentions.2.proj_out.weight torch.float32\n",
      "up_blocks.3.attentions.2.proj_out.bias torch.float32\n",
      "up_blocks.3.resnets.0.norm1.weight torch.float32\n",
      "up_blocks.3.resnets.0.norm1.bias torch.float32\n",
      "up_blocks.3.resnets.0.conv1.weight torch.float32\n",
      "up_blocks.3.resnets.0.conv1.bias torch.float32\n",
      "up_blocks.3.resnets.0.time_emb_proj.weight torch.float32\n",
      "up_blocks.3.resnets.0.time_emb_proj.bias torch.float32\n",
      "up_blocks.3.resnets.0.norm2.weight torch.float32\n",
      "up_blocks.3.resnets.0.norm2.bias torch.float32\n",
      "up_blocks.3.resnets.0.conv2.weight torch.float32\n",
      "up_blocks.3.resnets.0.conv2.bias torch.float32\n",
      "up_blocks.3.resnets.0.conv_shortcut.weight torch.float32\n",
      "up_blocks.3.resnets.0.conv_shortcut.bias torch.float32\n",
      "up_blocks.3.resnets.1.norm1.weight torch.float32\n",
      "up_blocks.3.resnets.1.norm1.bias torch.float32\n",
      "up_blocks.3.resnets.1.conv1.weight torch.float32\n",
      "up_blocks.3.resnets.1.conv1.bias torch.float32\n",
      "up_blocks.3.resnets.1.time_emb_proj.weight torch.float32\n",
      "up_blocks.3.resnets.1.time_emb_proj.bias torch.float32\n",
      "up_blocks.3.resnets.1.norm2.weight torch.float32\n",
      "up_blocks.3.resnets.1.norm2.bias torch.float32\n",
      "up_blocks.3.resnets.1.conv2.weight torch.float32\n",
      "up_blocks.3.resnets.1.conv2.bias torch.float32\n",
      "up_blocks.3.resnets.1.conv_shortcut.weight torch.float32\n",
      "up_blocks.3.resnets.1.conv_shortcut.bias torch.float32\n",
      "up_blocks.3.resnets.2.norm1.weight torch.float32\n",
      "up_blocks.3.resnets.2.norm1.bias torch.float32\n",
      "up_blocks.3.resnets.2.conv1.weight torch.float32\n",
      "up_blocks.3.resnets.2.conv1.bias torch.float32\n",
      "up_blocks.3.resnets.2.time_emb_proj.weight torch.float32\n",
      "up_blocks.3.resnets.2.time_emb_proj.bias torch.float32\n",
      "up_blocks.3.resnets.2.norm2.weight torch.float32\n",
      "up_blocks.3.resnets.2.norm2.bias torch.float32\n",
      "up_blocks.3.resnets.2.conv2.weight torch.float32\n",
      "up_blocks.3.resnets.2.conv2.bias torch.float32\n",
      "up_blocks.3.resnets.2.conv_shortcut.weight torch.float32\n",
      "up_blocks.3.resnets.2.conv_shortcut.bias torch.float32\n",
      "mid_block.attentions.0.norm.weight torch.float32\n",
      "mid_block.attentions.0.norm.bias torch.float32\n",
      "mid_block.attentions.0.proj_in.weight torch.float32\n",
      "mid_block.attentions.0.proj_in.bias torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.bias torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.bias torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.bias torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float32\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float32\n",
      "mid_block.attentions.0.proj_out.weight torch.float32\n",
      "mid_block.attentions.0.proj_out.bias torch.float32\n",
      "mid_block.resnets.0.norm1.weight torch.float32\n",
      "mid_block.resnets.0.norm1.bias torch.float32\n",
      "mid_block.resnets.0.conv1.weight torch.float32\n",
      "mid_block.resnets.0.conv1.bias torch.float32\n",
      "mid_block.resnets.0.time_emb_proj.weight torch.float32\n",
      "mid_block.resnets.0.time_emb_proj.bias torch.float32\n",
      "mid_block.resnets.0.norm2.weight torch.float32\n",
      "mid_block.resnets.0.norm2.bias torch.float32\n",
      "mid_block.resnets.0.conv2.weight torch.float32\n",
      "mid_block.resnets.0.conv2.bias torch.float32\n",
      "mid_block.resnets.1.norm1.weight torch.float32\n",
      "mid_block.resnets.1.norm1.bias torch.float32\n",
      "mid_block.resnets.1.conv1.weight torch.float32\n",
      "mid_block.resnets.1.conv1.bias torch.float32\n",
      "mid_block.resnets.1.time_emb_proj.weight torch.float32\n",
      "mid_block.resnets.1.time_emb_proj.bias torch.float32\n",
      "mid_block.resnets.1.norm2.weight torch.float32\n",
      "mid_block.resnets.1.norm2.bias torch.float32\n",
      "mid_block.resnets.1.conv2.weight torch.float32\n",
      "mid_block.resnets.1.conv2.bias torch.float32\n",
      "conv_norm_out.weight torch.float32\n",
      "conv_norm_out.bias torch.float32\n",
      "conv_out.weight torch.float32\n",
      "conv_out.bias torch.float32\n"
     ]
    }
   ],
   "source": [
    "# show the precision of the default model\n",
    "\n",
    "for name, param in pipe_auto.unet.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0735a05658490e9f5ff49f3d9eaa17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f423d2fa6a440c7b7adc73c1ef19968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.fp16.safetensors:   0%|          | 0.00/608M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83755ce191754520bd219aed20da5b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d052260cc11948ce9453329a0c45520f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.fp16.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59a3872a84342dc84aab004ee51b421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca77d1a38fb4d02927b1f11948b50da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_in.weight torch.float16\n",
      "conv_in.bias torch.float16\n",
      "time_embedding.linear_1.weight torch.float16\n",
      "time_embedding.linear_1.bias torch.float16\n",
      "time_embedding.linear_2.weight torch.float16\n",
      "time_embedding.linear_2.bias torch.float16\n",
      "down_blocks.0.attentions.0.norm.weight torch.float16\n",
      "down_blocks.0.attentions.0.norm.bias torch.float16\n",
      "down_blocks.0.attentions.0.proj_in.weight torch.float16\n",
      "down_blocks.0.attentions.0.proj_in.bias torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "down_blocks.0.attentions.0.proj_out.weight torch.float16\n",
      "down_blocks.0.attentions.0.proj_out.bias torch.float16\n",
      "down_blocks.0.attentions.1.norm.weight torch.float16\n",
      "down_blocks.0.attentions.1.norm.bias torch.float16\n",
      "down_blocks.0.attentions.1.proj_in.weight torch.float16\n",
      "down_blocks.0.attentions.1.proj_in.bias torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "down_blocks.0.attentions.1.proj_out.weight torch.float16\n",
      "down_blocks.0.attentions.1.proj_out.bias torch.float16\n",
      "down_blocks.0.resnets.0.norm1.weight torch.float16\n",
      "down_blocks.0.resnets.0.norm1.bias torch.float16\n",
      "down_blocks.0.resnets.0.conv1.weight torch.float16\n",
      "down_blocks.0.resnets.0.conv1.bias torch.float16\n",
      "down_blocks.0.resnets.0.time_emb_proj.weight torch.float16\n",
      "down_blocks.0.resnets.0.time_emb_proj.bias torch.float16\n",
      "down_blocks.0.resnets.0.norm2.weight torch.float16\n",
      "down_blocks.0.resnets.0.norm2.bias torch.float16\n",
      "down_blocks.0.resnets.0.conv2.weight torch.float16\n",
      "down_blocks.0.resnets.0.conv2.bias torch.float16\n",
      "down_blocks.0.resnets.1.norm1.weight torch.float16\n",
      "down_blocks.0.resnets.1.norm1.bias torch.float16\n",
      "down_blocks.0.resnets.1.conv1.weight torch.float16\n",
      "down_blocks.0.resnets.1.conv1.bias torch.float16\n",
      "down_blocks.0.resnets.1.time_emb_proj.weight torch.float16\n",
      "down_blocks.0.resnets.1.time_emb_proj.bias torch.float16\n",
      "down_blocks.0.resnets.1.norm2.weight torch.float16\n",
      "down_blocks.0.resnets.1.norm2.bias torch.float16\n",
      "down_blocks.0.resnets.1.conv2.weight torch.float16\n",
      "down_blocks.0.resnets.1.conv2.bias torch.float16\n",
      "down_blocks.0.downsamplers.0.conv.weight torch.float16\n",
      "down_blocks.0.downsamplers.0.conv.bias torch.float16\n",
      "down_blocks.1.attentions.0.norm.weight torch.float16\n",
      "down_blocks.1.attentions.0.norm.bias torch.float16\n",
      "down_blocks.1.attentions.0.proj_in.weight torch.float16\n",
      "down_blocks.1.attentions.0.proj_in.bias torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "down_blocks.1.attentions.0.proj_out.weight torch.float16\n",
      "down_blocks.1.attentions.0.proj_out.bias torch.float16\n",
      "down_blocks.1.attentions.1.norm.weight torch.float16\n",
      "down_blocks.1.attentions.1.norm.bias torch.float16\n",
      "down_blocks.1.attentions.1.proj_in.weight torch.float16\n",
      "down_blocks.1.attentions.1.proj_in.bias torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "down_blocks.1.attentions.1.proj_out.weight torch.float16\n",
      "down_blocks.1.attentions.1.proj_out.bias torch.float16\n",
      "down_blocks.1.resnets.0.norm1.weight torch.float16\n",
      "down_blocks.1.resnets.0.norm1.bias torch.float16\n",
      "down_blocks.1.resnets.0.conv1.weight torch.float16\n",
      "down_blocks.1.resnets.0.conv1.bias torch.float16\n",
      "down_blocks.1.resnets.0.time_emb_proj.weight torch.float16\n",
      "down_blocks.1.resnets.0.time_emb_proj.bias torch.float16\n",
      "down_blocks.1.resnets.0.norm2.weight torch.float16\n",
      "down_blocks.1.resnets.0.norm2.bias torch.float16\n",
      "down_blocks.1.resnets.0.conv2.weight torch.float16\n",
      "down_blocks.1.resnets.0.conv2.bias torch.float16\n",
      "down_blocks.1.resnets.0.conv_shortcut.weight torch.float16\n",
      "down_blocks.1.resnets.0.conv_shortcut.bias torch.float16\n",
      "down_blocks.1.resnets.1.norm1.weight torch.float16\n",
      "down_blocks.1.resnets.1.norm1.bias torch.float16\n",
      "down_blocks.1.resnets.1.conv1.weight torch.float16\n",
      "down_blocks.1.resnets.1.conv1.bias torch.float16\n",
      "down_blocks.1.resnets.1.time_emb_proj.weight torch.float16\n",
      "down_blocks.1.resnets.1.time_emb_proj.bias torch.float16\n",
      "down_blocks.1.resnets.1.norm2.weight torch.float16\n",
      "down_blocks.1.resnets.1.norm2.bias torch.float16\n",
      "down_blocks.1.resnets.1.conv2.weight torch.float16\n",
      "down_blocks.1.resnets.1.conv2.bias torch.float16\n",
      "down_blocks.1.downsamplers.0.conv.weight torch.float16\n",
      "down_blocks.1.downsamplers.0.conv.bias torch.float16\n",
      "down_blocks.2.attentions.0.norm.weight torch.float16\n",
      "down_blocks.2.attentions.0.norm.bias torch.float16\n",
      "down_blocks.2.attentions.0.proj_in.weight torch.float16\n",
      "down_blocks.2.attentions.0.proj_in.bias torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "down_blocks.2.attentions.0.proj_out.weight torch.float16\n",
      "down_blocks.2.attentions.0.proj_out.bias torch.float16\n",
      "down_blocks.2.attentions.1.norm.weight torch.float16\n",
      "down_blocks.2.attentions.1.norm.bias torch.float16\n",
      "down_blocks.2.attentions.1.proj_in.weight torch.float16\n",
      "down_blocks.2.attentions.1.proj_in.bias torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "down_blocks.2.attentions.1.proj_out.weight torch.float16\n",
      "down_blocks.2.attentions.1.proj_out.bias torch.float16\n",
      "down_blocks.2.resnets.0.norm1.weight torch.float16\n",
      "down_blocks.2.resnets.0.norm1.bias torch.float16\n",
      "down_blocks.2.resnets.0.conv1.weight torch.float16\n",
      "down_blocks.2.resnets.0.conv1.bias torch.float16\n",
      "down_blocks.2.resnets.0.time_emb_proj.weight torch.float16\n",
      "down_blocks.2.resnets.0.time_emb_proj.bias torch.float16\n",
      "down_blocks.2.resnets.0.norm2.weight torch.float16\n",
      "down_blocks.2.resnets.0.norm2.bias torch.float16\n",
      "down_blocks.2.resnets.0.conv2.weight torch.float16\n",
      "down_blocks.2.resnets.0.conv2.bias torch.float16\n",
      "down_blocks.2.resnets.0.conv_shortcut.weight torch.float16\n",
      "down_blocks.2.resnets.0.conv_shortcut.bias torch.float16\n",
      "down_blocks.2.resnets.1.norm1.weight torch.float16\n",
      "down_blocks.2.resnets.1.norm1.bias torch.float16\n",
      "down_blocks.2.resnets.1.conv1.weight torch.float16\n",
      "down_blocks.2.resnets.1.conv1.bias torch.float16\n",
      "down_blocks.2.resnets.1.time_emb_proj.weight torch.float16\n",
      "down_blocks.2.resnets.1.time_emb_proj.bias torch.float16\n",
      "down_blocks.2.resnets.1.norm2.weight torch.float16\n",
      "down_blocks.2.resnets.1.norm2.bias torch.float16\n",
      "down_blocks.2.resnets.1.conv2.weight torch.float16\n",
      "down_blocks.2.resnets.1.conv2.bias torch.float16\n",
      "down_blocks.2.downsamplers.0.conv.weight torch.float16\n",
      "down_blocks.2.downsamplers.0.conv.bias torch.float16\n",
      "down_blocks.3.resnets.0.norm1.weight torch.float16\n",
      "down_blocks.3.resnets.0.norm1.bias torch.float16\n",
      "down_blocks.3.resnets.0.conv1.weight torch.float16\n",
      "down_blocks.3.resnets.0.conv1.bias torch.float16\n",
      "down_blocks.3.resnets.0.time_emb_proj.weight torch.float16\n",
      "down_blocks.3.resnets.0.time_emb_proj.bias torch.float16\n",
      "down_blocks.3.resnets.0.norm2.weight torch.float16\n",
      "down_blocks.3.resnets.0.norm2.bias torch.float16\n",
      "down_blocks.3.resnets.0.conv2.weight torch.float16\n",
      "down_blocks.3.resnets.0.conv2.bias torch.float16\n",
      "down_blocks.3.resnets.1.norm1.weight torch.float16\n",
      "down_blocks.3.resnets.1.norm1.bias torch.float16\n",
      "down_blocks.3.resnets.1.conv1.weight torch.float16\n",
      "down_blocks.3.resnets.1.conv1.bias torch.float16\n",
      "down_blocks.3.resnets.1.time_emb_proj.weight torch.float16\n",
      "down_blocks.3.resnets.1.time_emb_proj.bias torch.float16\n",
      "down_blocks.3.resnets.1.norm2.weight torch.float16\n",
      "down_blocks.3.resnets.1.norm2.bias torch.float16\n",
      "down_blocks.3.resnets.1.conv2.weight torch.float16\n",
      "down_blocks.3.resnets.1.conv2.bias torch.float16\n",
      "up_blocks.0.resnets.0.norm1.weight torch.float16\n",
      "up_blocks.0.resnets.0.norm1.bias torch.float16\n",
      "up_blocks.0.resnets.0.conv1.weight torch.float16\n",
      "up_blocks.0.resnets.0.conv1.bias torch.float16\n",
      "up_blocks.0.resnets.0.time_emb_proj.weight torch.float16\n",
      "up_blocks.0.resnets.0.time_emb_proj.bias torch.float16\n",
      "up_blocks.0.resnets.0.norm2.weight torch.float16\n",
      "up_blocks.0.resnets.0.norm2.bias torch.float16\n",
      "up_blocks.0.resnets.0.conv2.weight torch.float16\n",
      "up_blocks.0.resnets.0.conv2.bias torch.float16\n",
      "up_blocks.0.resnets.0.conv_shortcut.weight torch.float16\n",
      "up_blocks.0.resnets.0.conv_shortcut.bias torch.float16\n",
      "up_blocks.0.resnets.1.norm1.weight torch.float16\n",
      "up_blocks.0.resnets.1.norm1.bias torch.float16\n",
      "up_blocks.0.resnets.1.conv1.weight torch.float16\n",
      "up_blocks.0.resnets.1.conv1.bias torch.float16\n",
      "up_blocks.0.resnets.1.time_emb_proj.weight torch.float16\n",
      "up_blocks.0.resnets.1.time_emb_proj.bias torch.float16\n",
      "up_blocks.0.resnets.1.norm2.weight torch.float16\n",
      "up_blocks.0.resnets.1.norm2.bias torch.float16\n",
      "up_blocks.0.resnets.1.conv2.weight torch.float16\n",
      "up_blocks.0.resnets.1.conv2.bias torch.float16\n",
      "up_blocks.0.resnets.1.conv_shortcut.weight torch.float16\n",
      "up_blocks.0.resnets.1.conv_shortcut.bias torch.float16\n",
      "up_blocks.0.resnets.2.norm1.weight torch.float16\n",
      "up_blocks.0.resnets.2.norm1.bias torch.float16\n",
      "up_blocks.0.resnets.2.conv1.weight torch.float16\n",
      "up_blocks.0.resnets.2.conv1.bias torch.float16\n",
      "up_blocks.0.resnets.2.time_emb_proj.weight torch.float16\n",
      "up_blocks.0.resnets.2.time_emb_proj.bias torch.float16\n",
      "up_blocks.0.resnets.2.norm2.weight torch.float16\n",
      "up_blocks.0.resnets.2.norm2.bias torch.float16\n",
      "up_blocks.0.resnets.2.conv2.weight torch.float16\n",
      "up_blocks.0.resnets.2.conv2.bias torch.float16\n",
      "up_blocks.0.resnets.2.conv_shortcut.weight torch.float16\n",
      "up_blocks.0.resnets.2.conv_shortcut.bias torch.float16\n",
      "up_blocks.0.upsamplers.0.conv.weight torch.float16\n",
      "up_blocks.0.upsamplers.0.conv.bias torch.float16\n",
      "up_blocks.1.attentions.0.norm.weight torch.float16\n",
      "up_blocks.1.attentions.0.norm.bias torch.float16\n",
      "up_blocks.1.attentions.0.proj_in.weight torch.float16\n",
      "up_blocks.1.attentions.0.proj_in.bias torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.1.attentions.0.proj_out.weight torch.float16\n",
      "up_blocks.1.attentions.0.proj_out.bias torch.float16\n",
      "up_blocks.1.attentions.1.norm.weight torch.float16\n",
      "up_blocks.1.attentions.1.norm.bias torch.float16\n",
      "up_blocks.1.attentions.1.proj_in.weight torch.float16\n",
      "up_blocks.1.attentions.1.proj_in.bias torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.1.attentions.1.proj_out.weight torch.float16\n",
      "up_blocks.1.attentions.1.proj_out.bias torch.float16\n",
      "up_blocks.1.attentions.2.norm.weight torch.float16\n",
      "up_blocks.1.attentions.2.norm.bias torch.float16\n",
      "up_blocks.1.attentions.2.proj_in.weight torch.float16\n",
      "up_blocks.1.attentions.2.proj_in.bias torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.1.attentions.2.proj_out.weight torch.float16\n",
      "up_blocks.1.attentions.2.proj_out.bias torch.float16\n",
      "up_blocks.1.resnets.0.norm1.weight torch.float16\n",
      "up_blocks.1.resnets.0.norm1.bias torch.float16\n",
      "up_blocks.1.resnets.0.conv1.weight torch.float16\n",
      "up_blocks.1.resnets.0.conv1.bias torch.float16\n",
      "up_blocks.1.resnets.0.time_emb_proj.weight torch.float16\n",
      "up_blocks.1.resnets.0.time_emb_proj.bias torch.float16\n",
      "up_blocks.1.resnets.0.norm2.weight torch.float16\n",
      "up_blocks.1.resnets.0.norm2.bias torch.float16\n",
      "up_blocks.1.resnets.0.conv2.weight torch.float16\n",
      "up_blocks.1.resnets.0.conv2.bias torch.float16\n",
      "up_blocks.1.resnets.0.conv_shortcut.weight torch.float16\n",
      "up_blocks.1.resnets.0.conv_shortcut.bias torch.float16\n",
      "up_blocks.1.resnets.1.norm1.weight torch.float16\n",
      "up_blocks.1.resnets.1.norm1.bias torch.float16\n",
      "up_blocks.1.resnets.1.conv1.weight torch.float16\n",
      "up_blocks.1.resnets.1.conv1.bias torch.float16\n",
      "up_blocks.1.resnets.1.time_emb_proj.weight torch.float16\n",
      "up_blocks.1.resnets.1.time_emb_proj.bias torch.float16\n",
      "up_blocks.1.resnets.1.norm2.weight torch.float16\n",
      "up_blocks.1.resnets.1.norm2.bias torch.float16\n",
      "up_blocks.1.resnets.1.conv2.weight torch.float16\n",
      "up_blocks.1.resnets.1.conv2.bias torch.float16\n",
      "up_blocks.1.resnets.1.conv_shortcut.weight torch.float16\n",
      "up_blocks.1.resnets.1.conv_shortcut.bias torch.float16\n",
      "up_blocks.1.resnets.2.norm1.weight torch.float16\n",
      "up_blocks.1.resnets.2.norm1.bias torch.float16\n",
      "up_blocks.1.resnets.2.conv1.weight torch.float16\n",
      "up_blocks.1.resnets.2.conv1.bias torch.float16\n",
      "up_blocks.1.resnets.2.time_emb_proj.weight torch.float16\n",
      "up_blocks.1.resnets.2.time_emb_proj.bias torch.float16\n",
      "up_blocks.1.resnets.2.norm2.weight torch.float16\n",
      "up_blocks.1.resnets.2.norm2.bias torch.float16\n",
      "up_blocks.1.resnets.2.conv2.weight torch.float16\n",
      "up_blocks.1.resnets.2.conv2.bias torch.float16\n",
      "up_blocks.1.resnets.2.conv_shortcut.weight torch.float16\n",
      "up_blocks.1.resnets.2.conv_shortcut.bias torch.float16\n",
      "up_blocks.1.upsamplers.0.conv.weight torch.float16\n",
      "up_blocks.1.upsamplers.0.conv.bias torch.float16\n",
      "up_blocks.2.attentions.0.norm.weight torch.float16\n",
      "up_blocks.2.attentions.0.norm.bias torch.float16\n",
      "up_blocks.2.attentions.0.proj_in.weight torch.float16\n",
      "up_blocks.2.attentions.0.proj_in.bias torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.2.attentions.0.proj_out.weight torch.float16\n",
      "up_blocks.2.attentions.0.proj_out.bias torch.float16\n",
      "up_blocks.2.attentions.1.norm.weight torch.float16\n",
      "up_blocks.2.attentions.1.norm.bias torch.float16\n",
      "up_blocks.2.attentions.1.proj_in.weight torch.float16\n",
      "up_blocks.2.attentions.1.proj_in.bias torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.2.attentions.1.proj_out.weight torch.float16\n",
      "up_blocks.2.attentions.1.proj_out.bias torch.float16\n",
      "up_blocks.2.attentions.2.norm.weight torch.float16\n",
      "up_blocks.2.attentions.2.norm.bias torch.float16\n",
      "up_blocks.2.attentions.2.proj_in.weight torch.float16\n",
      "up_blocks.2.attentions.2.proj_in.bias torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.2.attentions.2.proj_out.weight torch.float16\n",
      "up_blocks.2.attentions.2.proj_out.bias torch.float16\n",
      "up_blocks.2.resnets.0.norm1.weight torch.float16\n",
      "up_blocks.2.resnets.0.norm1.bias torch.float16\n",
      "up_blocks.2.resnets.0.conv1.weight torch.float16\n",
      "up_blocks.2.resnets.0.conv1.bias torch.float16\n",
      "up_blocks.2.resnets.0.time_emb_proj.weight torch.float16\n",
      "up_blocks.2.resnets.0.time_emb_proj.bias torch.float16\n",
      "up_blocks.2.resnets.0.norm2.weight torch.float16\n",
      "up_blocks.2.resnets.0.norm2.bias torch.float16\n",
      "up_blocks.2.resnets.0.conv2.weight torch.float16\n",
      "up_blocks.2.resnets.0.conv2.bias torch.float16\n",
      "up_blocks.2.resnets.0.conv_shortcut.weight torch.float16\n",
      "up_blocks.2.resnets.0.conv_shortcut.bias torch.float16\n",
      "up_blocks.2.resnets.1.norm1.weight torch.float16\n",
      "up_blocks.2.resnets.1.norm1.bias torch.float16\n",
      "up_blocks.2.resnets.1.conv1.weight torch.float16\n",
      "up_blocks.2.resnets.1.conv1.bias torch.float16\n",
      "up_blocks.2.resnets.1.time_emb_proj.weight torch.float16\n",
      "up_blocks.2.resnets.1.time_emb_proj.bias torch.float16\n",
      "up_blocks.2.resnets.1.norm2.weight torch.float16\n",
      "up_blocks.2.resnets.1.norm2.bias torch.float16\n",
      "up_blocks.2.resnets.1.conv2.weight torch.float16\n",
      "up_blocks.2.resnets.1.conv2.bias torch.float16\n",
      "up_blocks.2.resnets.1.conv_shortcut.weight torch.float16\n",
      "up_blocks.2.resnets.1.conv_shortcut.bias torch.float16\n",
      "up_blocks.2.resnets.2.norm1.weight torch.float16\n",
      "up_blocks.2.resnets.2.norm1.bias torch.float16\n",
      "up_blocks.2.resnets.2.conv1.weight torch.float16\n",
      "up_blocks.2.resnets.2.conv1.bias torch.float16\n",
      "up_blocks.2.resnets.2.time_emb_proj.weight torch.float16\n",
      "up_blocks.2.resnets.2.time_emb_proj.bias torch.float16\n",
      "up_blocks.2.resnets.2.norm2.weight torch.float16\n",
      "up_blocks.2.resnets.2.norm2.bias torch.float16\n",
      "up_blocks.2.resnets.2.conv2.weight torch.float16\n",
      "up_blocks.2.resnets.2.conv2.bias torch.float16\n",
      "up_blocks.2.resnets.2.conv_shortcut.weight torch.float16\n",
      "up_blocks.2.resnets.2.conv_shortcut.bias torch.float16\n",
      "up_blocks.2.upsamplers.0.conv.weight torch.float16\n",
      "up_blocks.2.upsamplers.0.conv.bias torch.float16\n",
      "up_blocks.3.attentions.0.norm.weight torch.float16\n",
      "up_blocks.3.attentions.0.norm.bias torch.float16\n",
      "up_blocks.3.attentions.0.proj_in.weight torch.float16\n",
      "up_blocks.3.attentions.0.proj_in.bias torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.3.attentions.0.proj_out.weight torch.float16\n",
      "up_blocks.3.attentions.0.proj_out.bias torch.float16\n",
      "up_blocks.3.attentions.1.norm.weight torch.float16\n",
      "up_blocks.3.attentions.1.norm.bias torch.float16\n",
      "up_blocks.3.attentions.1.proj_in.weight torch.float16\n",
      "up_blocks.3.attentions.1.proj_in.bias torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.3.attentions.1.proj_out.weight torch.float16\n",
      "up_blocks.3.attentions.1.proj_out.bias torch.float16\n",
      "up_blocks.3.attentions.2.norm.weight torch.float16\n",
      "up_blocks.3.attentions.2.norm.bias torch.float16\n",
      "up_blocks.3.attentions.2.proj_in.weight torch.float16\n",
      "up_blocks.3.attentions.2.proj_in.bias torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "up_blocks.3.attentions.2.proj_out.weight torch.float16\n",
      "up_blocks.3.attentions.2.proj_out.bias torch.float16\n",
      "up_blocks.3.resnets.0.norm1.weight torch.float16\n",
      "up_blocks.3.resnets.0.norm1.bias torch.float16\n",
      "up_blocks.3.resnets.0.conv1.weight torch.float16\n",
      "up_blocks.3.resnets.0.conv1.bias torch.float16\n",
      "up_blocks.3.resnets.0.time_emb_proj.weight torch.float16\n",
      "up_blocks.3.resnets.0.time_emb_proj.bias torch.float16\n",
      "up_blocks.3.resnets.0.norm2.weight torch.float16\n",
      "up_blocks.3.resnets.0.norm2.bias torch.float16\n",
      "up_blocks.3.resnets.0.conv2.weight torch.float16\n",
      "up_blocks.3.resnets.0.conv2.bias torch.float16\n",
      "up_blocks.3.resnets.0.conv_shortcut.weight torch.float16\n",
      "up_blocks.3.resnets.0.conv_shortcut.bias torch.float16\n",
      "up_blocks.3.resnets.1.norm1.weight torch.float16\n",
      "up_blocks.3.resnets.1.norm1.bias torch.float16\n",
      "up_blocks.3.resnets.1.conv1.weight torch.float16\n",
      "up_blocks.3.resnets.1.conv1.bias torch.float16\n",
      "up_blocks.3.resnets.1.time_emb_proj.weight torch.float16\n",
      "up_blocks.3.resnets.1.time_emb_proj.bias torch.float16\n",
      "up_blocks.3.resnets.1.norm2.weight torch.float16\n",
      "up_blocks.3.resnets.1.norm2.bias torch.float16\n",
      "up_blocks.3.resnets.1.conv2.weight torch.float16\n",
      "up_blocks.3.resnets.1.conv2.bias torch.float16\n",
      "up_blocks.3.resnets.1.conv_shortcut.weight torch.float16\n",
      "up_blocks.3.resnets.1.conv_shortcut.bias torch.float16\n",
      "up_blocks.3.resnets.2.norm1.weight torch.float16\n",
      "up_blocks.3.resnets.2.norm1.bias torch.float16\n",
      "up_blocks.3.resnets.2.conv1.weight torch.float16\n",
      "up_blocks.3.resnets.2.conv1.bias torch.float16\n",
      "up_blocks.3.resnets.2.time_emb_proj.weight torch.float16\n",
      "up_blocks.3.resnets.2.time_emb_proj.bias torch.float16\n",
      "up_blocks.3.resnets.2.norm2.weight torch.float16\n",
      "up_blocks.3.resnets.2.norm2.bias torch.float16\n",
      "up_blocks.3.resnets.2.conv2.weight torch.float16\n",
      "up_blocks.3.resnets.2.conv2.bias torch.float16\n",
      "up_blocks.3.resnets.2.conv_shortcut.weight torch.float16\n",
      "up_blocks.3.resnets.2.conv_shortcut.bias torch.float16\n",
      "mid_block.attentions.0.norm.weight torch.float16\n",
      "mid_block.attentions.0.norm.bias torch.float16\n",
      "mid_block.attentions.0.proj_in.weight torch.float16\n",
      "mid_block.attentions.0.proj_in.bias torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.norm1.bias torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.norm2.bias torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.norm3.bias torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight torch.float16\n",
      "mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias torch.float16\n",
      "mid_block.attentions.0.proj_out.weight torch.float16\n",
      "mid_block.attentions.0.proj_out.bias torch.float16\n",
      "mid_block.resnets.0.norm1.weight torch.float16\n",
      "mid_block.resnets.0.norm1.bias torch.float16\n",
      "mid_block.resnets.0.conv1.weight torch.float16\n",
      "mid_block.resnets.0.conv1.bias torch.float16\n",
      "mid_block.resnets.0.time_emb_proj.weight torch.float16\n",
      "mid_block.resnets.0.time_emb_proj.bias torch.float16\n",
      "mid_block.resnets.0.norm2.weight torch.float16\n",
      "mid_block.resnets.0.norm2.bias torch.float16\n",
      "mid_block.resnets.0.conv2.weight torch.float16\n",
      "mid_block.resnets.0.conv2.bias torch.float16\n",
      "mid_block.resnets.1.norm1.weight torch.float16\n",
      "mid_block.resnets.1.norm1.bias torch.float16\n",
      "mid_block.resnets.1.conv1.weight torch.float16\n",
      "mid_block.resnets.1.conv1.bias torch.float16\n",
      "mid_block.resnets.1.time_emb_proj.weight torch.float16\n",
      "mid_block.resnets.1.time_emb_proj.bias torch.float16\n",
      "mid_block.resnets.1.norm2.weight torch.float16\n",
      "mid_block.resnets.1.norm2.bias torch.float16\n",
      "mid_block.resnets.1.conv2.weight torch.float16\n",
      "mid_block.resnets.1.conv2.bias torch.float16\n",
      "conv_norm_out.weight torch.float16\n",
      "conv_norm_out.bias torch.float16\n",
      "conv_out.weight torch.float16\n",
      "conv_out.bias torch.float16\n"
     ]
    }
   ],
   "source": [
    "# There are 2 parameters for the precision:\n",
    "# - \"variant\" designates which variant we use to load the model\n",
    "#   in this case, if the torch_dtype is not set, the variant's precision\n",
    "#   will be converted to the default precision - f32.\n",
    "# - \"torch_dtype\" is the type of the conversion after loading.\n",
    "#   if the variant is f32 and dtype is f16, the loaded model will be converted \n",
    "#   to f16.\n",
    "\n",
    "import torch\n",
    "\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\"CompVis/stable-diffusion-v1-4\", variant=\"fp16\", torch_dtype=torch.half, use_safetensors=True)\n",
    "for name, param in pipe.unet.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Community pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community pipelines some variant implementations of the original papers. \n",
    "All community pipelines are listed in: https://github.com/huggingface/diffusers/tree/main/examples/community.\n",
    "The pipelines can be either in HF hub or in the Git repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffuser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
